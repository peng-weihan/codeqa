import os
import openai
import json
import concurrent.futures
from typing import Dict, Any, Optional

from dotenv import load_dotenv
load_dotenv()
api_key = os.getenv("AIHUBMIX_API_KEY")

from openai import OpenAI

client = OpenAI(api_key=api_key, base_url="https://aihubmix.com/v1")

def score_answer(question, reference, candidate):
    # ... existing code ...
    prompt = f"""You are a professional evaluator. Please rate the candidate answer against the reference answer based on the following five criteria:

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ“‹  YOUR TASK
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Evaluation Criteria and Scoring Guidelines (each scored 0 to 10):

1. Accuracy:
  9-10 â€” Completely accurate; core points and details are correct with no ambiguity.
  7-8  â€” Mostly accurate; only minor details are slightly incorrect or loosely expressed.
  5-6  â€” Partially accurate; some errors or omissions, but main points are generally correct.
  3-4  â€” Several errors or ambiguities that affect the understanding of core information.
  0-2  â€” Serious errors; misleading or fails to convey key information.

2. Completeness:
  9-10 â€” Covers all key points from the reference answer without omission.
  7-8  â€” Covers most key points; only minor non-critical information missing.
  5-6  â€” Missing several key points; content is somewhat incomplete.
  3-4  â€” Important information largely missing; content is one-sided.
  0-2  â€” Covers very little or irrelevant information; seriously incomplete.

3. Clarity of Expression:
  9-10 â€” Fluent language; clear and precise expression; easy to understand.
  7-8  â€” Mostly fluent; some expressions slightly unclear or not concise.
  5-6  â€” Expression somewhat awkward; some ambiguity or lack of fluency.
  3-4  â€” Language obscure; sentences are not smooth; hinders understanding.
  0-2  â€” Expression confusing; very difficult to understand.

4. Relevance:
  9-10 â€” Content fully focused on the question topic; no irrelevant information.
  7-8  â€” Mostly focused; only minor irrelevant or peripheral information.
  5-6  â€” Topic not sufficiently focused; contains considerable off-topic content.
  3-4  â€” Content deviates from topic; includes excessive irrelevant information.
  0-2  â€” Majority of content irrelevant to the question.

5. Logical Structure:
  9-10 â€” Clear and reasonable structure; well-organized and coherent.
  7-8  â€” Generally reasonable structure; mostly clear organization; minor misarrangements.
  5-6  â€” Loose structure; logical jumps; organization is average.
  3-4  â€” Disorganized structure; lacks logical order; difficult to follow.
  0-2  â€” No clear structure; logic is chaotic.


â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ“¥  INPUT STRUCTURE
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Question:
{question}

Reference Answer:
{reference}

Candidate Answer:
{candidate}

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ“¤  OUTPUT REQUIREMENT
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Please output only one integer score between 0 and 50 representing the sum score of the 5 angles. No explanation or other content is needed.
"""

    try:
        response = client.chat.completions.create(
            model="DeepSeek-V3",
            messages=[
            {"role": "system", "content": "You are a helpful assistant"},
            {"role": "user", "content": prompt},
            ],
            stream=False
        )
        score_str = response.choices[0].message.content.strip()
        print(f"è¯„åˆ†ç»“æœï¼š{score_str}")
        try:
            score = int(score_str)
            if 0 <= score <= 50:
                return score
            else:
                return None
        except:
            return None
    except Exception as e:
        print(f"è¯„åˆ†å‡ºé”™: {e}")
        return None

def compare_answers_pairwise(question, reference, rag, mcts):
    # ... existing code ...
    prompt = f"""
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šè¯„å®¡ä¸“å®¶ï¼Œéœ€è¦æ ¹æ®é—®é¢˜å’Œå‚è€ƒç­”æ¡ˆï¼Œåˆ¤æ–­ä¸¤ä¸ªå€™é€‰ç­”æ¡ˆä¸­å“ªä¸ªæ›´å¥½ã€‚

è¯·ä¸¥æ ¼ä¾æ®ä»¥ä¸‹è¯„ä¼°ç»´åº¦è¿›è¡Œåˆ¤æ–­ï¼š

1. å‡†ç¡®æ€§ï¼šæ˜¯å¦å‡†ç¡®ä¼ è¾¾å‚è€ƒç­”æ¡ˆçš„æ ¸å¿ƒä¿¡æ¯
2. å®Œæ•´æ€§ï¼šæ˜¯å¦è¦†ç›–æ‰€æœ‰å…³é”®ä¿¡æ¯ç‚¹
3. è¡¨è¾¾æ¸…æ™°åº¦ï¼šè¯­è¨€æ˜¯å¦æ¸…æ™°ã€é€šé¡º
4. ç›¸å…³æ€§ï¼šæ˜¯å¦èšç„¦é—®é¢˜ä¸»é¢˜
5. é€»è¾‘ç»“æ„ï¼šæ˜¯å¦ç»“æ„æ¸…æ™°ã€æ¡ç†æ¸…æ¥š

---
é—®é¢˜ï¼š
{question}

å‚è€ƒç­”æ¡ˆï¼š
{reference}

å€™é€‰ç­”æ¡ˆ Aï¼ˆæ¥è‡ª RAGï¼‰ï¼š
{rag}

å€™é€‰ç­”æ¡ˆ Bï¼ˆæ¥è‡ª MCTSï¼‰ï¼š
{mcts}

è¯·ä» A å’Œ B ä¸­é€‰æ‹©ä¸€ä¸ª**æ›´ä¼˜**çš„ç­”æ¡ˆï¼ˆå³æ›´ç¬¦åˆä¸Šè¿°æ ‡å‡†ï¼‰ï¼Œæˆ–è€…åœ¨ä¸¤è€…è´¨é‡éå¸¸æ¥è¿‘ã€éš¾ä»¥åˆ†å‡ºä¼˜åŠ£æ—¶ï¼Œé€‰æ‹© "Same"ã€‚

åªè¾“å‡ºä¸€ä¸ªå•è¯ï¼š"A"ã€"B" æˆ– "Same"ï¼Œä¸è¦è¾“å‡ºå…¶ä»–å†…å®¹æˆ–è§£é‡Šã€‚
"""

    try:
        response = client.chat.completions.create(
            model="DeepSeek-V3",
            messages=[
                {"role": "system", "content": "You are a helpful evaluator."},
                {"role": "user", "content": prompt},
            ],
            stream=False
        )

        choice = response.choices[0].message.content.strip().upper()
        print(f"æ¯”è¾ƒç»“æœï¼š{choice}")
        return choice
    except Exception as e:
        print(f"æ¯”è¾ƒå‡ºé”™: {e}")
        return None

def process_single_record(record: Dict[str, Any]) -> Optional[Dict[str, Any]]:
    """å¤„ç†å•ä¸ªè®°å½•çš„å‡½æ•°ï¼Œç”¨äºå¹¶è¡Œæ‰§è¡Œ"""
    try:
        question = record.get("question", "")
        reference = record.get("ground_truth", "")
        
        # è·å–å„ç§ç­”æ¡ˆ
        direct_answer = record.get("direct_answer", "")
        rag_answer = record.get("rag_answer", "")
        mcts_answer = record.get("mcts_answer", "")
        
        if not reference:
            return None

        # ç›´æ¥åœ¨åŸè®°å½•ä¸­æ·»åŠ è¯„åˆ†
        if direct_answer:
            direct_score = score_answer(question, reference, direct_answer)
            record["score"] = direct_score
            print(f"Directç­”æ¡ˆè¯„åˆ†: {direct_score}")

        if rag_answer:
            rag_score = score_answer(question, reference, rag_answer)
            record["score"] = rag_score
            print(f"RAGç­”æ¡ˆè¯„åˆ†: {rag_score}")

        if mcts_answer:
            mcts_score = score_answer(question, reference, mcts_answer)
            record["score"] = mcts_score
            print(f"MCTSç­”æ¡ˆè¯„åˆ†: {mcts_score}")

        print(f"å·²è¯„åˆ†é—®é¢˜: {question}")
        return record
        
    except Exception as e:
        print(f"å¤„ç†è®°å½•æ—¶å‡ºé”™: {e}")
        return None

def evaluate_jsonl_parallel(input_jsonl_path, output_jsonl_path, max_workers=16):
    """å¹¶è¡Œå¤„ç† JSONL æ–‡ä»¶"""
    # è¯»å–æ‰€æœ‰è®°å½•
    records = []
    with open(input_jsonl_path, 'r', encoding='utf-8') as fin:
        for line in fin:
            try:
                record = json.loads(line)
                records.append(record)
            except Exception as e:
                print(f"[è·³è¿‡] æ— æ•ˆJSONè¡Œ: {e}")
                continue
    
    print(f"æ€»å…±è¯»å–åˆ° {len(records)} æ¡è®°å½•ï¼Œå¼€å§‹å¹¶è¡Œå¤„ç†...")
    
    # ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œå¤„ç†
    results = []
    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
        # æäº¤æ‰€æœ‰ä»»åŠ¡
        future_to_record = {executor.submit(process_single_record, record): record for record in records}
        
        # æ”¶é›†ç»“æœ
        for future in concurrent.futures.as_completed(future_to_record):
            record = future_to_record[future]
            try:
                result = future.result()
                if result is not None:
                    results.append(result)
            except Exception as e:
                print(f"å¤„ç†è®°å½•æ—¶å‡ºé”™: {e}")
    
    print("prepare to write results...")
    # å†™å…¥ç»“æœ
    with open(output_jsonl_path, 'w', encoding='utf-8') as fout:
        for result in results:
            fout.write(json.dumps(result, ensure_ascii=False) + "\n")
    

if __name__ == "__main__":
    repos = [
        'astropy',
        # 'flask',
        # 'matplotlib',
        # 'pylint',
        # 'pytest',
        # 'requests',
        # 'scikit-learn',
        # 'sphinx',
        # 'sqlfluff',
        # 'xarray',
        # 'django',
        # 'sympy',
    ]
    subtype = "rag_doc"
    type = "rag"
    # type = "mcts"
    for repo in repos:
        # input_path = f"/data3/pwh/answers/direct/{repo}_direct.jsonl"
        # output_path = f"/data3/pwh/answers/score/direct/{repo}_score.jsonl"
        input_path = f"/data3/pwh/answers/{subtype}/{repo}_{type}.jsonl"
        output_path = f"/data3/pwh/answers/score/{subtype}/{repo}_score.jsonl"
        # ä½¿ç”¨å¹¶è¡Œå¤„ç†
        evaluate_jsonl_parallel(input_path, output_path, max_workers=16)