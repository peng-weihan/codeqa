import os
import openai
import json

from dotenv import load_dotenv
load_dotenv()
api_key = os.getenv("DEEPSEEK_API_KEY")

from openai import OpenAI

client = OpenAI(api_key=api_key, base_url="https://api.deepseek.com")

def score_answer(question, reference, candidate, model="deepseek/deepseek-reasoner"):

    prompt = f"""You are a professional evaluator. Please rate the candidate answer against the reference answer based on the following five criteria:

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ“‹  YOUR TASK
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Evaluation Criteria and Scoring Guidelines (each scored 1 to 5):

1. Accuracy:
  5 â€” Completely accurate; core points and details are correct with no ambiguity.
  4 â€” Mostly accurate; only minor details are slightly incorrect or loosely expressed.
  3 â€” Partially accurate; some errors or omissions, but main points are generally correct.
  2 â€” Several errors or ambiguities that affect the understanding of core information.
  1 â€” Serious errors; misleading or fails to convey key information.

2. Completeness:
  5 â€” Covers all key points from the reference answer without omission.
  4 â€” Covers most key points; only minor non-critical information missing.
  3 â€” Missing several key points; content is somewhat incomplete.
  2 â€” Important information largely missing; content is one-sided.
  1 â€” Covers very little or irrelevant information; seriously incomplete.

3. Clarity of Expression:
  5 â€” Fluent language; clear and precise expression; easy to understand.
  4 â€” Mostly fluent; some expressions slightly unclear or not concise.
  3 â€” Expression somewhat awkward; some ambiguity or lack of fluency.
  2 â€” Language obscure; sentences are not smooth; hinders understanding.
  1 â€” Expression confusing; very difficult to understand.

4. Relevance:
  5 â€” Content fully focused on the question topic; no irrelevant information.
  4 â€” Mostly focused; only minor irrelevant or peripheral information.
  3 â€” Topic not sufficiently focused; contains considerable off-topic content.
  2 â€” Content deviates from topic; includes excessive irrelevant information.
  1 â€” Majority of content irrelevant to the question.

5. Logical Structure:
  5 â€” Clear and reasonable structure; well-organized and coherent.
  4 â€” Generally reasonable structure; mostly clear organization; minor misarrangements.
  3 â€” Loose structure; logical jumps; organization is average.
  2 â€” Disorganized structure; lacks logical order; difficult to follow.
  1 â€” No clear structure; logic is chaotic.

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ“¥  INPUT STRUCTURE
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Question:
{question}

Reference Answer:
{reference}

Candidate Answer:
{candidate}

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ“¤  OUTPUT REQUIREMENT
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Please output only one integer score between 0 and 25 representing the overall quality of the candidate answer. No explanation or other content is needed.
"""

    response = client.chat.completions.create(
    model="deepseek-chat",
    messages=[
        {"role": "system", "content": "You are a helpful assistant"},
        {"role": "user", "content": prompt},
    ],
    stream=False
    )
    score_str = response.choices[0].message.content.strip()
    print(f"è¯„åˆ†ç»“æœï¼š{score_str}")
    try:
        score = int(score_str)
        if 0 <= score <= 100:
            return score
        else:
            return None
    except:
        return None

def compare_answers_pairwise(question, reference, rag, mcts):
    prompt = f"""
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šè¯„å®¡ä¸“å®¶ï¼Œéœ€è¦æ ¹æ®é—®é¢˜å’Œå‚è€ƒç­”æ¡ˆï¼Œåˆ¤æ–­ä¸¤ä¸ªå€™é€‰ç­”æ¡ˆä¸­å“ªä¸ªæ›´å¥½ã€‚

è¯·ä¸¥æ ¼ä¾æ®ä»¥ä¸‹è¯„ä¼°ç»´åº¦è¿›è¡Œåˆ¤æ–­ï¼š

1. å‡†ç¡®æ€§ï¼šæ˜¯å¦å‡†ç¡®ä¼ è¾¾å‚è€ƒç­”æ¡ˆçš„æ ¸å¿ƒä¿¡æ¯
2. å®Œæ•´æ€§ï¼šæ˜¯å¦è¦†ç›–æ‰€æœ‰å…³é”®ä¿¡æ¯ç‚¹
3. è¡¨è¾¾æ¸…æ™°åº¦ï¼šè¯­è¨€æ˜¯å¦æ¸…æ™°ã€é€šé¡º
4. ç›¸å…³æ€§ï¼šæ˜¯å¦èšç„¦é—®é¢˜ä¸»é¢˜
5. é€»è¾‘ç»“æ„ï¼šæ˜¯å¦ç»“æ„æ¸…æ™°ã€æ¡ç†æ¸…æ¥š

---
é—®é¢˜ï¼š
{question}

å‚è€ƒç­”æ¡ˆï¼š
{reference}

å€™é€‰ç­”æ¡ˆ Aï¼ˆæ¥è‡ª RAGï¼‰ï¼š
{rag}

å€™é€‰ç­”æ¡ˆ Bï¼ˆæ¥è‡ª MCTSï¼‰ï¼š
{mcts}

è¯·ä» A å’Œ B ä¸­é€‰æ‹©ä¸€ä¸ª**æ›´ä¼˜**çš„ç­”æ¡ˆï¼ˆå³æ›´ç¬¦åˆä¸Šè¿°æ ‡å‡†ï¼‰ï¼Œæˆ–è€…åœ¨ä¸¤è€…è´¨é‡éå¸¸æ¥è¿‘ã€éš¾ä»¥åˆ†å‡ºä¼˜åŠ£æ—¶ï¼Œé€‰æ‹© "Same"ã€‚

åªè¾“å‡ºä¸€ä¸ªå•è¯ï¼š"A"ã€"B" æˆ– "Same"ï¼Œä¸è¦è¾“å‡ºå…¶ä»–å†…å®¹æˆ–è§£é‡Šã€‚
"""

    response = client.chat.completions.create(
        model="deepseek-chat",
        messages=[
            {"role": "system", "content": "You are a helpful evaluator."},
            {"role": "user", "content": prompt},
        ],
        stream=False
    )

    choice = response.choices[0].message.content.strip().upper()
    print(f"æ¯”è¾ƒç»“æœï¼š{choice}")
    return choice

def evaluate_jsonl(input_jsonl_path, output_jsonl_path):
    with open(input_jsonl_path, 'r', encoding='utf-8') as fin, \
        open(output_jsonl_path, 'w', encoding='utf-8') as fout:
        for line in fin:
            try:
                record = json.loads(line)
            except Exception as e:
                print(f"[è·³è¿‡] æ— æ•ˆJSONè¡Œ: {e}")
                continue

            reference = record.get("wiki_answer", "")
            rag = record.get("rag_answer", "")
            mcts = record.get("mcts_answer", "")
            question = record.get("question", "")
            if not reference or not rag or not mcts:
                continue


            # winner = compare_answers_pairwise(question, reference, rag, mcts)
            # result = {
            #     "question": question,
            #     "winner": winner,  # 'A' for RAG, 'B' for MCTS
            #     "rag_answer": rag,
            #     "mcts_answer": mcts,
            #     "reference": reference
            # }
            # fout.write(json.dumps(result, ensure_ascii=False) + "\n")


            rag_score = score_answer(question, reference, rag)
            mcts_score = score_answer(question, reference, mcts)

            result = {
                "question": record.get("question", ""),
                "rag_score": rag_score,
                "mcts_score": mcts_score
            }

            fout.write(json.dumps(result, ensure_ascii=False) + "\n")


            print(f"å·²è¯„åˆ†é—®é¢˜: {result['question']}, RAGåˆ†æ•°: {rag_score}, MCTSåˆ†æ•°: {mcts_score}")

if __name__ == "__main__":
    input_path = "/data3/pwh/codeqa/dataset/generated_answers/generated_answers_rag_agent_flask_new.jsonl"    # ä½ çš„è¾“å…¥jsonlæ–‡ä»¶è·¯å¾„
    output_path = "/data3/pwh/codeqa/dataset/score/result.jsonl"  # è¾“å‡ºè¯„åˆ†ç»“æœæ–‡ä»¶è·¯å¾„
    evaluate_jsonl(input_path, output_path)
