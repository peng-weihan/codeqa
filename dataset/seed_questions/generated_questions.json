[
{"question":"Where can I find the implementation of the `DatabaseSchemaEditor` and its `_is_changing_type_of_indexed_text_column` method in the codebase?","answer":"","relative_code_list":[{"start_line":7,"end_line":380,"belongs_to":{"file_name":"schema.py","upper_path":"../django/django/db/backends/postgresql","module":"postgresql","define_class":["DatabaseSchemaEditor"],"imports":["django.db.backends.base.schema.BaseDatabaseSchemaEditor","django.db.backends.ddl_references.IndexColumns","django.db.backends.postgresql.psycopg_any.sql","django.db.backends.utils.strip_quotes"]},"relative_function":[],"code":"class DatabaseSchemaEditor(BaseDatabaseSchemaEditor):\n    # Setting all constraints to IMMEDIATE to allow changing data in the same\n    # transaction.\n    sql_update_with_default = (\n        \"UPDATE %(table)s SET %(column)s = %(default)s WHERE %(column)s IS NULL\"\n        \"; SET CONSTRAINTS ALL IMMEDIATE\"\n    )\n    sql_alter_sequence_type = \"ALTER SEQUENCE IF EXISTS %(sequence)s AS %(type)s\"\n    sql_delete_sequence = \"DROP SEQUENCE IF EXISTS %(sequence)s CASCADE\"\n\n    sql_create_index = (\n        \"CREATE INDEX %(name)s ON %(table)s%(using)s \"\n        \"(%(columns)s)%(include)s%(extra)s%(condition)s\"\n    )\n    sql_create_index_concurrently = (\n        \"CREATE INDEX CONCURRENTLY %(name)s ON %(table)s%(using)s \"\n        \"(%(columns)s)%(include)s%(extra)s%(condition)s\"\n    )\n    sql_delete_index = \"DROP INDEX IF EXISTS %(name)s\"\n    sql_delete_index_concurrently = \"DROP INDEX CONCURRENTLY IF EXISTS %(name)s\"\n\n    # Setting the constraint to IMMEDIATE to allow changing data in the same\n    # transaction.\n    sql_create_column_inline_fk = (\n        \"CONSTRAINT %(name)s REFERENCES %(to_table)s(%(to_column)s)%(deferrable)s\"\n        \"; SET CONSTRAINTS %(namespace)s%(name)s IMMEDIATE\"\n    )\n    # Setting the constraint to IMMEDIATE runs any deferred checks to allow\n    # dropping it in the same transaction.\n    sql_delete_fk = (\n        \"SET CONSTRAINTS %(name)s IMMEDIATE; \"\n        \"ALTER TABLE %(table)s DROP CONSTRAINT %(name)s\"\n    )\n    sql_delete_procedure = \"DROP FUNCTION %(procedure)s(%(param_types)s)\"\n\n    def execute(self, sql, params=()):\n        # Merge the query client-side, as PostgreSQL won't do it server-side.\n        if params is None:\n            return super().execute(sql, params)\n        sql = self.connection.ops.compose_sql(str(sql), params)\n        # Don't let the superclass touch anything.\n        return super().execute(sql, None)\n\n    sql_add_identity = (\n        \"ALTER TABLE %(table)s ALTER COLUMN %(column)s ADD \"\n        \"GENERATED BY DEFAULT AS IDENTITY\"\n    )\n    sql_drop_indentity = (\n        \"ALTER TABLE %(table)s ALTER COLUMN %(column)s DROP IDENTITY IF EXISTS\"\n    )\n\n    def quote_value(self, value):\n        return sql.quote(value, self.connection.connection)\n\n    def _field_indexes_sql(self, model, field):\n        output = super()._field_indexes_sql(model, field)\n        like_index_statement = self._create_like_index_sql(model, field)\n        if like_index_statement is not None:\n            output.append(like_index_statement)\n        return output\n\n    def _field_data_type(self, field):\n        if field.is_relation:\n            return field.rel_db_type(self.connection)\n        return self.connection.data_types.get(\n            field.get_internal_type(),\n            field.db_type(self.connection),\n        )\n\n    def _field_base_data_types(self, field):\n        # Yield base data types for array fields.\n        if field.base_field.get_internal_type() == \"ArrayField\":\n            yield from self._field_base_data_types(field.base_field)\n        else:\n            yield self._field_data_type(field.base_field)\n\n    def _create_like_index_sql(self, model, field):\n        \"\"\"\n        Return the statement to create an index with varchar operator pattern\n        when the column type is 'varchar' or 'text', otherwise return None.\n        \"\"\"\n        db_type = field.db_type(connection=self.connection)\n        if db_type is not None and (field.db_index or field.unique):\n            # Fields with database column types of `varchar` and `text` need\n            # a second index that specifies their operator class, which is\n            # needed when performing correct LIKE queries outside the\n            # C locale. See #12234.\n            #\n            # The same doesn't apply to array fields such as varchar[size]\n            # and text[size], so skip them.\n            if \"[\" in db_type:\n                return None\n            # Non-deterministic collations on Postgresql don't support indexes\n            # for operator classes varchar_pattern_ops/text_pattern_ops.\n            collation_name = getattr(field, \"db_collation\", None)\n            if not collation_name and field.is_relation:\n                collation_name = getattr(field.target_field, \"db_collation\", None)\n            if collation_name and not self._is_collation_deterministic(collation_name):\n                return None\n            if db_type.startswith(\"varchar\"):\n                return self._create_index_sql(\n                    model,\n                    fields=[field],\n                    suffix=\"_like\",\n                    opclasses=[\"varchar_pattern_ops\"],\n                )\n            elif db_type.startswith(\"text\"):\n                return self._create_index_sql(\n                    model,\n                    fields=[field],\n                    suffix=\"_like\",\n                    opclasses=[\"text_pattern_ops\"],\n                )\n        return None\n\n    def _using_sql(self, new_field, old_field):\n        if new_field.generated:\n            return \"\"\n        using_sql = \" USING %(column)s::%(type)s\"\n        new_internal_type = new_field.get_internal_type()\n        old_internal_type = old_field.get_internal_type()\n        if new_internal_type == \"ArrayField\" and new_internal_type == old_internal_type:\n            # Compare base data types for array fields.\n            if list(self._field_base_data_types(old_field)) != list(\n                self._field_base_data_types(new_field)\n            ):\n                return using_sql\n        elif self._field_data_type(old_field) != self._field_data_type(new_field):\n            return using_sql\n        return \"\"\n\n    def _get_sequence_name(self, table, column):\n        with self.connection.cursor() as cursor:\n            for sequence in self.connection.introspection.get_sequences(cursor, table):\n                if sequence[\"column\"] == column:\n                    return sequence[\"name\"]\n        return None\n\n    def _is_changing_type_of_indexed_text_column(self, old_field, old_type, new_type):\n        return (old_field.db_index or old_field.unique) and (\n            (old_type.startswith(\"varchar\") and not new_type.startswith(\"varchar\"))\n            or (old_type.startswith(\"text\") and not new_type.startswith(\"text\"))\n            or (old_type.startswith(\"citext\") and not new_type.startswith(\"citext\"))\n        )\n\n    def _alter_column_type_sql(\n        self, model, old_field, new_field, new_type, old_collation, new_collation\n    ):\n        # Drop indexes on varchar/text/citext columns that are changing to a\n        # different type.\n        old_db_params = old_field.db_parameters(connection=self.connection)\n        old_type = old_db_params[\"type\"]\n        if self._is_changing_type_of_indexed_text_column(old_field, old_type, new_type):\n            index_name = self._create_index_name(\n                model._meta.db_table, [old_field.column], suffix=\"_like\"\n            )\n            self.execute(self._delete_index_sql(model, index_name))\n\n        self.sql_alter_column_type = (\n            \"ALTER COLUMN %(column)s TYPE %(type)s%(collation)s\"\n        )\n        # Cast when data type changed.\n        if using_sql := self._using_sql(new_field, old_field):\n            self.sql_alter_column_type += using_sql\n        new_internal_type = new_field.get_internal_type()\n        old_internal_type = old_field.get_internal_type()\n        # Make ALTER TYPE with IDENTITY make sense.\n        table = strip_quotes(model._meta.db_table)\n        auto_field_types = {\n            \"AutoField\",\n            \"BigAutoField\",\n            \"SmallAutoField\",\n        }\n        old_is_auto = old_internal_type in auto_field_types\n        new_is_auto = new_internal_type in auto_field_types\n        if new_is_auto and not old_is_auto:\n            column = strip_quotes(new_field.column)\n            return (\n                (\n                    self.sql_alter_column_type\n                    % {\n                        \"column\": self.quote_name(column),\n                        \"type\": new_type,\n                        \"collation\": \"\",\n                    },\n                    [],\n                ),\n                [\n                    (\n                        self.sql_add_identity\n                        % {\n                            \"table\": self.quote_name(table),\n                            \"column\": self.quote_name(column),\n                        },\n                        [],\n                    ),\n                ],\n            )\n        elif old_is_auto and not new_is_auto:\n            # Drop IDENTITY if exists (pre-Django 4.1 serial columns don't have\n            # it).\n            self.execute(\n                self.sql_drop_indentity\n                % {\n                    \"table\": self.quote_name(table),\n                    \"column\": self.quote_name(strip_quotes(new_field.column)),\n                }\n            )\n            column = strip_quotes(new_field.column)\n            fragment, _ = super()._alter_column_type_sql(\n                model, old_field, new_field, new_type, old_collation, new_collation\n            )\n            # Drop the sequence if exists (Django 4.1+ identity columns don't\n            # have it).\n            other_actions = []\n            if sequence_name := self._get_sequence_name(table, column):\n                other_actions = [\n                    (\n                        self.sql_delete_sequence\n                        % {\n                            \"sequence\": self.quote_name(sequence_name),\n                        },\n                        [],\n                    )\n                ]\n            return fragment, other_actions\n        elif new_is_auto and old_is_auto and old_internal_type != new_internal_type:\n            fragment, _ = super()._alter_column_type_sql(\n                model, old_field, new_field, new_type, old_collation, new_collation\n            )\n            column = strip_quotes(new_field.column)\n            db_types = {\n                \"AutoField\": \"integer\",\n                \"BigAutoField\": \"bigint\",\n                \"SmallAutoField\": \"smallint\",\n            }\n            # Alter the sequence type if exists (Django 4.1+ identity columns\n            # don't have it).\n            other_actions = []\n            if sequence_name := self._get_sequence_name(table, column):\n                other_actions = [\n                    (\n                        self.sql_alter_sequence_type\n                        % {\n                            \"sequence\": self.quote_name(sequence_name),\n                            \"type\": db_types[new_internal_type],\n                        },\n                        [],\n                    ),\n                ]\n            return fragment, other_actions\n        else:\n            return super()._alter_column_type_sql(\n                model, old_field, new_field, new_type, old_collation, new_collation\n            )\n\n    def _alter_field(\n        self,\n        model,\n        old_field,\n        new_field,\n        old_type,\n        new_type,\n        old_db_params,\n        new_db_params,\n        strict=False,\n    ):\n        super()._alter_field(\n            model,\n            old_field,\n            new_field,\n            old_type,\n            new_type,\n            old_db_params,\n            new_db_params,\n            strict,\n        )\n        # Added an index? Create any PostgreSQL-specific indexes.\n        if (\n            (not (old_field.db_index or old_field.unique) and new_field.db_index)\n            or (not old_field.unique and new_field.unique)\n            or (\n                self._is_changing_type_of_indexed_text_column(\n                    old_field, old_type, new_type\n                )\n            )\n        ):\n            like_index_statement = self._create_like_index_sql(model, new_field)\n            if like_index_statement is not None:\n                self.execute(like_index_statement)\n\n        # Removed an index? Drop any PostgreSQL-specific indexes.\n        if old_field.unique and not (new_field.db_index or new_field.unique):\n            index_to_remove = self._create_index_name(\n                model._meta.db_table, [old_field.column], suffix=\"_like\"\n            )\n            self.execute(self._delete_index_sql(model, index_to_remove))\n\n    def _index_columns(self, table, columns, col_suffixes, opclasses):\n        if opclasses:\n            return IndexColumns(\n                table,\n                columns,\n                self.quote_name,\n                col_suffixes=col_suffixes,\n                opclasses=opclasses,\n            )\n        return super()._index_columns(table, columns, col_suffixes, opclasses)\n\n    def add_index(self, model, index, concurrently=False):\n        self.execute(\n            index.create_sql(model, self, concurrently=concurrently), params=None\n        )\n\n    def remove_index(self, model, index, concurrently=False):\n        self.execute(index.remove_sql(model, self, concurrently=concurrently))\n\n    def _delete_index_sql(self, model, name, sql=None, concurrently=False):\n        sql = sql or (\n            self.sql_delete_index_concurrently\n            if concurrently\n            else self.sql_delete_index\n        )\n        return super()._delete_index_sql(model, name, sql)\n\n    def _create_index_sql(\n        self,\n        model,\n        *,\n        fields=None,\n        name=None,\n        suffix=\"\",\n        using=\"\",\n        db_tablespace=None,\n        col_suffixes=(),\n        sql=None,\n        opclasses=(),\n        condition=None,\n        concurrently=False,\n        include=None,\n        expressions=None,\n    ):\n        sql = sql or (\n            self.sql_create_index\n            if not concurrently\n            else self.sql_create_index_concurrently\n        )\n        return super()._create_index_sql(\n            model,\n            fields=fields,\n            name=name,\n            suffix=suffix,\n            using=using,\n            db_tablespace=db_tablespace,\n            col_suffixes=col_suffixes,\n            sql=sql,\n            opclasses=opclasses,\n            condition=condition,\n            include=include,\n            expressions=expressions,\n        )\n\n    def _is_collation_deterministic(self, collation_name):\n        with self.connection.cursor() as cursor:\n            cursor.execute(\n                \"\"\"\n                SELECT collisdeterministic\n                FROM pg_collation\n                WHERE collname = %s\n                \"\"\",\n                [collation_name],\n            )\n            row = cursor.fetchone()\n            return row[0] if row else None"},{"start_line":145,"end_line":150,"belongs_to":{"file_name":"schema.py","upper_path":"../django/django/db/backends/postgresql","module":"postgresql","define_class":["DatabaseSchemaEditor"],"imports":["django.db.backends.base.schema.BaseDatabaseSchemaEditor","django.db.backends.ddl_references.IndexColumns","django.db.backends.postgresql.psycopg_any.sql","django.db.backends.utils.strip_quotes"]},"relative_function":[],"code":"def _is_changing_type_of_indexed_text_column(self, old_field, old_type, new_type):\n        return (old_field.db_index or old_field.unique) and (\n            (old_type.startswith(\"varchar\") and not new_type.startswith(\"varchar\"))\n            or (old_type.startswith(\"text\") and not new_type.startswith(\"text\"))\n            or (old_type.startswith(\"citext\") and not new_type.startswith(\"citext\"))\n        )"}],"ground_truth":null,"score":null},    
{"question":"What are the parameters and expected behavior of the `_destroy_test_user` method in the `DatabaseCreation`?","answer":"","relative_code_list":[{"start_line":9,"end_line":91,"belongs_to":{"file_name":"creation.py","upper_path":"../django/django/db/backends/postgresql","module":"postgresql","define_class":["DatabaseCreation"],"imports":["sys","django.core.exceptions.ImproperlyConfigured","django.db.backends.base.creation.BaseDatabaseCreation","django.db.backends.postgresql.psycopg_any.errors","django.db.backends.utils.strip_quotes"]},"relative_function":[],"code":"class DatabaseCreation(BaseDatabaseCreation):\n    def _quote_name(self, name):\n        return self.connection.ops.quote_name(name)\n\n    def _get_database_create_suffix(self, encoding=None, template=None):\n        suffix = \"\"\n        if encoding:\n            suffix += \" ENCODING '{}'\".format(encoding)\n        if template:\n            suffix += \" TEMPLATE {}\".format(self._quote_name(template))\n        return suffix and \"WITH\" + suffix\n\n    def sql_table_creation_suffix(self):\n        test_settings = self.connection.settings_dict[\"TEST\"]\n        if test_settings.get(\"COLLATION\") is not None:\n            raise ImproperlyConfigured(\n                \"PostgreSQL does not support collation setting at database \"\n                \"creation time.\"\n            )\n        return self._get_database_create_suffix(\n            encoding=test_settings[\"CHARSET\"],\n            template=test_settings.get(\"TEMPLATE\"),\n        )\n\n    def _database_exists(self, cursor, database_name):\n        cursor.execute(\n            \"SELECT 1 FROM pg_catalog.pg_database WHERE datname = %s\",\n            [strip_quotes(database_name)],\n        )\n        return cursor.fetchone() is not None\n\n    def _execute_create_test_db(self, cursor, parameters, keepdb=False):\n        try:\n            if keepdb and self._database_exists(cursor, parameters[\"dbname\"]):\n                # If the database should be kept and it already exists, don't\n                # try to create a new one.\n                return\n            super()._execute_create_test_db(cursor, parameters, keepdb)\n        except Exception as e:\n            if not isinstance(e.__cause__, errors.DuplicateDatabase):\n                # All errors except \"database already exists\" cancel tests.\n                self.log(\"Got an error creating the test database: %s\" % e)\n                sys.exit(2)\n            elif not keepdb:\n                # If the database should be kept, ignore \"database already\n                # exists\".\n                raise\n\n    def _clone_test_db(self, suffix, verbosity, keepdb=False):\n        # CREATE DATABASE ... WITH TEMPLATE ... requires closing connections\n        # to the template database.\n        self.connection.close()\n        self.connection.close_pool()\n\n        source_database_name = self.connection.settings_dict[\"NAME\"]\n        target_database_name = self.get_test_db_clone_settings(suffix)[\"NAME\"]\n        test_db_params = {\n            \"dbname\": self._quote_name(target_database_name),\n            \"suffix\": self._get_database_create_suffix(template=source_database_name),\n        }\n        with self._nodb_cursor() as cursor:\n            try:\n                self._execute_create_test_db(cursor, test_db_params, keepdb)\n            except Exception:\n                try:\n                    if verbosity >= 1:\n                        self.log(\n                            \"Destroying old test database for alias %s...\"\n                            % (\n                                self._get_database_display_str(\n                                    verbosity, target_database_name\n                                ),\n                            )\n                        )\n                    cursor.execute(\"DROP DATABASE %(dbname)s\" % test_db_params)\n                    self._execute_create_test_db(cursor, test_db_params, keepdb)\n                except Exception as e:\n                    self.log(\"Got an error cloning the test database: %s\" % e)\n                    sys.exit(2)\n\n    def _destroy_test_db(self, test_database_name, verbosity):\n        self.connection.close_pool()\n        return super()._destroy_test_db(test_database_name, verbosity)"},{"start_line":316,"end_line":323,"belongs_to":{"file_name":"creation.py","upper_path":"../django/django/db/backends/oracle","module":"oracle","define_class":["DatabaseCreation"],"imports":["sys","django.conf.settings","django.db.DatabaseError","django.db.backends.base.creation.BaseDatabaseCreation","django.utils.crypto.get_random_string","django.utils.functional.cached_property"]},"relative_function":[],"code":"def _destroy_test_user(self, cursor, parameters, verbosity):\n        if verbosity >= 2:\n            self.log(\"_destroy_test_user(): user=%s\" % parameters[\"user\"])\n            self.log(\"Be patient. This can take some time...\")\n        statements = [\n            \"DROP USER %(user)s CASCADE\",\n        ]\n        self._execute_statements(cursor, statements, parameters, verbosity)"}],"ground_truth":null,"score":null},
{"question":"Where can I find the definition of the `FormatStylePlaceholderCursor`.`__getattr__` function in the codebase?","answer":"","relative_code_list":[{"start_line":482,"end_line":662,"belongs_to":{"file_name":"base.py","upper_path":"../django/django/db/backends/oracle","module":"oracle","define_class":["_UninitializedOperatorsDescriptor","DatabaseWrapper","OracleParam","VariableWrapper","FormatStylePlaceholderCursor"],"imports":["datetime","decimal","os","platform","contextlib.contextmanager","django.conf.settings","django.core.exceptions.ImproperlyConfigured","django.db.IntegrityError","django.db.backends.base.base.BaseDatabaseWrapper","django.db.backends.utils.debug_transaction","django.utils.asyncio.async_unsafe","django.utils.encoding.force_bytes","django.utils.encoding.force_str","django.utils.functional.cached_property","django.utils.version.get_version_tuple","client.DatabaseClient","creation.DatabaseCreation","features.DatabaseFeatures","introspection.DatabaseIntrospection","operations.DatabaseOperations","schema.DatabaseSchemaEditor","utils.Oracle_datetime","utils.dsn","validation.DatabaseValidation","oracledb","ctypes"]},"relative_function":[],"code":"class FormatStylePlaceholderCursor:\n    \"\"\"\n    Django uses \"format\" (e.g. '%s') style placeholders, but Oracle uses \":var\"\n    style. This fixes it -- but note that if you want to use a literal \"%s\" in\n    a query, you'll need to use \"%%s\".\n    \"\"\"\n\n    charset = \"utf-8\"\n\n    def __init__(self, connection, database):\n        self.cursor = connection.cursor()\n        self.cursor.outputtypehandler = self._output_type_handler\n        self.database = database\n\n    @staticmethod\n    def _output_number_converter(value):\n        return decimal.Decimal(value) if \".\" in value else int(value)\n\n    @staticmethod\n    def _get_decimal_converter(precision, scale):\n        if scale == 0:\n            return int\n        context = decimal.Context(prec=precision)\n        quantize_value = decimal.Decimal(1).scaleb(-scale)\n        return lambda v: decimal.Decimal(v).quantize(quantize_value, context=context)\n\n    @staticmethod\n    def _output_type_handler(cursor, name, defaultType, length, precision, scale):\n        \"\"\"\n        Called for each db column fetched from cursors. Return numbers as the\n        appropriate Python type, and NCLOB with JSON as strings.\n        \"\"\"\n        if defaultType == Database.NUMBER:\n            if scale == -127:\n                if precision == 0:\n                    # NUMBER column: decimal-precision floating point.\n                    # This will normally be an integer from a sequence,\n                    # but it could be a decimal value.\n                    outconverter = FormatStylePlaceholderCursor._output_number_converter\n                else:\n                    # FLOAT column: binary-precision floating point.\n                    # This comes from FloatField columns.\n                    outconverter = float\n            elif precision > 0:\n                # NUMBER(p,s) column: decimal-precision fixed point.\n                # This comes from IntegerField and DecimalField columns.\n                outconverter = FormatStylePlaceholderCursor._get_decimal_converter(\n                    precision, scale\n                )\n            else:\n                # No type information. This normally comes from a\n                # mathematical expression in the SELECT list. Guess int\n                # or Decimal based on whether it has a decimal point.\n                outconverter = FormatStylePlaceholderCursor._output_number_converter\n            return cursor.var(\n                Database.STRING,\n                size=255,\n                arraysize=cursor.arraysize,\n                outconverter=outconverter,\n            )\n        # oracledb 2.0.0+ returns NLOB columns with IS JSON constraints as\n        # dicts. Use a no-op converter to avoid this.\n        elif defaultType == Database.DB_TYPE_NCLOB:\n            return cursor.var(Database.DB_TYPE_NCLOB, arraysize=cursor.arraysize)\n\n    def _format_params(self, params):\n        try:\n            return {k: OracleParam(v, self, True) for k, v in params.items()}\n        except AttributeError:\n            return tuple(OracleParam(p, self, True) for p in params)\n\n    def _guess_input_sizes(self, params_list):\n        # Try dict handling; if that fails, treat as sequence\n        if hasattr(params_list[0], \"keys\"):\n            sizes = {}\n            for params in params_list:\n                for k, value in params.items():\n                    if value.input_size:\n                        sizes[k] = value.input_size\n            if sizes:\n                self.setinputsizes(**sizes)\n        else:\n            # It's not a list of dicts; it's a list of sequences\n            sizes = [None] * len(params_list[0])\n            for params in params_list:\n                for i, value in enumerate(params):\n                    if value.input_size:\n                        sizes[i] = value.input_size\n            if sizes:\n                self.setinputsizes(*sizes)\n\n    def _param_generator(self, params):\n        # Try dict handling; if that fails, treat as sequence\n        if hasattr(params, \"items\"):\n            return {k: v.force_bytes for k, v in params.items()}\n        else:\n            return [p.force_bytes for p in params]\n\n    def _fix_for_params(self, query, params, unify_by_values=False):\n        # oracledb wants no trailing ';' for SQL statements.  For PL/SQL, it\n        # it does want a trailing ';' but not a trailing '/'.  However, these\n        # characters must be included in the original query in case the query\n        # is being passed to SQL*Plus.\n        if query.endswith(\";\") or query.endswith(\"/\"):\n            query = query[:-1]\n        if params is None:\n            params = []\n        elif hasattr(params, \"keys\"):\n            # Handle params as dict\n            args = {k: \":%s\" % k for k in params}\n            query %= args\n        elif unify_by_values and params:\n            # Handle params as a dict with unified query parameters by their\n            # values. It can be used only in single query execute() because\n            # executemany() shares the formatted query with each of the params\n            # list. e.g. for input params = [0.75, 2, 0.75, 'sth', 0.75]\n            # params_dict = {\n            #     (float, 0.75): ':arg0',\n            #     (int, 2): ':arg1',\n            #     (str, 'sth'): ':arg2',\n            # }\n            # args = [':arg0', ':arg1', ':arg0', ':arg2', ':arg0']\n            # params = {':arg0': 0.75, ':arg1': 2, ':arg2': 'sth'}\n            # The type of parameters in param_types keys is necessary to avoid\n            # unifying 0/1 with False/True.\n            param_types = [(type(param), param) for param in params]\n            params_dict = {\n                param_type: \":arg%d\" % i\n                for i, param_type in enumerate(dict.fromkeys(param_types))\n            }\n            args = [params_dict[param_type] for param_type in param_types]\n            params = {\n                placeholder: param for (_, param), placeholder in params_dict.items()\n            }\n            query %= tuple(args)\n        else:\n            # Handle params as sequence\n            args = [(\":arg%d\" % i) for i in range(len(params))]\n            query %= tuple(args)\n        return query, self._format_params(params)\n\n    def execute(self, query, params=None):\n        query, params = self._fix_for_params(query, params, unify_by_values=True)\n        self._guess_input_sizes([params])\n        with wrap_oracle_errors():\n            return self.cursor.execute(query, self._param_generator(params))\n\n    def executemany(self, query, params=None):\n        if not params:\n            # No params given, nothing to do\n            return None\n        # uniform treatment for sequences and iterables\n        params_iter = iter(params)\n        query, firstparams = self._fix_for_params(query, next(params_iter))\n        # we build a list of formatted params; as we're going to traverse it\n        # more than once, we can't make it lazy by using a generator\n        formatted = [firstparams] + [self._format_params(p) for p in params_iter]\n        self._guess_input_sizes(formatted)\n        with wrap_oracle_errors():\n            return self.cursor.executemany(\n                query, [self._param_generator(p) for p in formatted]\n            )\n\n    def close(self):\n        try:\n            self.cursor.close()\n        except Database.InterfaceError:\n            # already closed\n            pass\n\n    def var(self, *args):\n        return VariableWrapper(self.cursor.var(*args))\n\n    def arrayvar(self, *args):\n        return VariableWrapper(self.cursor.arrayvar(*args))\n\n    def __getattr__(self, attr):\n        return getattr(self.cursor, attr)\n\n    def __iter__(self):\n        return iter(self.cursor)"},{"start_line":658,"end_line":659,"belongs_to":{"file_name":"base.py","upper_path":"../django/django/db/backends/oracle","module":"oracle","define_class":["_UninitializedOperatorsDescriptor","DatabaseWrapper","OracleParam","VariableWrapper","FormatStylePlaceholderCursor"],"imports":["datetime","decimal","os","platform","contextlib.contextmanager","django.conf.settings","django.core.exceptions.ImproperlyConfigured","django.db.IntegrityError","django.db.backends.base.base.BaseDatabaseWrapper","django.db.backends.utils.debug_transaction","django.utils.asyncio.async_unsafe","django.utils.encoding.force_bytes","django.utils.encoding.force_str","django.utils.functional.cached_property","django.utils.version.get_version_tuple","client.DatabaseClient","creation.DatabaseCreation","features.DatabaseFeatures","introspection.DatabaseIntrospection","operations.DatabaseOperations","schema.DatabaseSchemaEditor","utils.Oracle_datetime","utils.dsn","validation.DatabaseValidation","oracledb","ctypes"]},"relative_function":[],"code":"def __getattr__(self, attr):\n        return getattr(self.cursor, attr)"}],"ground_truth":null,"score":null},
{"question":"What are the expected input parameters and return values for the `test_collations` methods in the `DatabaseFeatures`?","answer":"","relative_code_list":[{"start_line":9,"end_line":171,"belongs_to":{"file_name":"features.py","upper_path":"../django/django/db/backends/postgresql","module":"postgresql","define_class":["DatabaseFeatures"],"imports":["operator","django.db.DataError","django.db.InterfaceError","django.db.backends.base.features.BaseDatabaseFeatures","django.db.backends.postgresql.psycopg_any.is_psycopg3","django.utils.functional.cached_property"]},"relative_function":[],"code":"class DatabaseFeatures(BaseDatabaseFeatures):\n    minimum_database_version = (14,)\n    allows_group_by_selected_pks = True\n    can_return_columns_from_insert = True\n    can_return_rows_from_bulk_insert = True\n    has_real_datatype = True\n    has_native_uuid_field = True\n    has_native_duration_field = True\n    has_native_json_field = True\n    can_defer_constraint_checks = True\n    has_select_for_update = True\n    has_select_for_update_nowait = True\n    has_select_for_update_of = True\n    has_select_for_update_skip_locked = True\n    has_select_for_no_key_update = True\n    can_release_savepoints = True\n    supports_comments = True\n    supports_tablespaces = True\n    supports_transactions = True\n    can_introspect_materialized_views = True\n    can_distinct_on_fields = True\n    can_rollback_ddl = True\n    schema_editor_uses_clientside_param_binding = True\n    supports_combined_alters = True\n    nulls_order_largest = True\n    closed_cursor_error_class = InterfaceError\n    greatest_least_ignores_nulls = True\n    can_clone_databases = True\n    supports_temporal_subtraction = True\n    supports_slicing_ordering_in_compound = True\n    create_test_procedure_without_params_sql = \"\"\"\n        CREATE FUNCTION test_procedure () RETURNS void AS $$\n        DECLARE\n            V_I INTEGER;\n        BEGIN\n            V_I := 1;\n        END;\n    $$ LANGUAGE plpgsql;\"\"\"\n    create_test_procedure_with_int_param_sql = \"\"\"\n        CREATE FUNCTION test_procedure (P_I INTEGER) RETURNS void AS $$\n        DECLARE\n            V_I INTEGER;\n        BEGIN\n            V_I := P_I;\n        END;\n    $$ LANGUAGE plpgsql;\"\"\"\n    create_test_table_with_composite_primary_key = \"\"\"\n        CREATE TABLE test_table_composite_pk (\n            column_1 INTEGER NOT NULL,\n            column_2 INTEGER NOT NULL,\n            PRIMARY KEY(column_1, column_2)\n        )\n    \"\"\"\n    requires_casted_case_in_updates = True\n    supports_over_clause = True\n    supports_frame_exclusion = True\n    only_supports_unbounded_with_preceding_and_following = True\n    supports_aggregate_filter_clause = True\n    supports_aggregate_order_by_clause = True\n    supported_explain_formats = {\"JSON\", \"TEXT\", \"XML\", \"YAML\"}\n    supports_deferrable_unique_constraints = True\n    has_json_operators = True\n    json_key_contains_list_matching_requires_list = True\n    supports_update_conflicts = True\n    supports_update_conflicts_with_target = True\n    supports_covering_indexes = True\n    supports_stored_generated_columns = True\n    supports_virtual_generated_columns = False\n    can_rename_index = True\n    test_collations = {\n        \"deterministic\": \"C\",\n        \"non_default\": \"sv-x-icu\",\n        \"swedish_ci\": \"sv-x-icu\",\n        \"virtual\": \"sv-x-icu\",\n    }\n    test_now_utc_template = \"STATEMENT_TIMESTAMP() AT TIME ZONE 'UTC'\"\n    insert_test_table_with_defaults = \"INSERT INTO {} DEFAULT VALUES\"\n\n    @cached_property\n    def django_test_skips(self):\n        skips = {\n            \"opclasses are PostgreSQL only.\": {\n                \"indexes.tests.SchemaIndexesNotPostgreSQLTests.\"\n                \"test_create_index_ignores_opclasses\",\n            },\n            \"PostgreSQL requires casting to text.\": {\n                \"lookup.tests.LookupTests.test_textfield_exact_null\",\n            },\n        }\n        if self.connection.settings_dict[\"OPTIONS\"].get(\"pool\"):\n            skips.update(\n                {\n                    \"Pool does implicit health checks\": {\n                        \"backends.base.test_base.ConnectionHealthChecksTests.\"\n                        \"test_health_checks_enabled\",\n                        \"backends.base.test_base.ConnectionHealthChecksTests.\"\n                        \"test_set_autocommit_health_checks_enabled\",\n                    },\n                }\n            )\n        if self.uses_server_side_binding:\n            skips.update(\n                {\n                    \"The actual query cannot be determined for server side bindings\": {\n                        \"backends.base.test_base.ExecuteWrapperTests.\"\n                        \"test_wrapper_debug\",\n                    }\n                },\n            )\n        return skips\n\n    @cached_property\n    def django_test_expected_failures(self):\n        expected_failures = set()\n        if self.uses_server_side_binding:\n            expected_failures.update(\n                {\n                    # Parameters passed to expressions in SELECT and GROUP BY\n                    # clauses are not recognized as the same values when using\n                    # server-side binding cursors (#34255).\n                    \"aggregation.tests.AggregateTestCase.\"\n                    \"test_group_by_nested_expression_with_params\",\n                }\n            )\n        return expected_failures\n\n    @cached_property\n    def uses_server_side_binding(self):\n        options = self.connection.settings_dict[\"OPTIONS\"]\n        return is_psycopg3 and options.get(\"server_side_binding\") is True\n\n    @cached_property\n    def prohibits_null_characters_in_text_exception(self):\n        if is_psycopg3:\n            return DataError, \"PostgreSQL text fields cannot contain NUL (0x00) bytes\"\n        else:\n            return ValueError, \"A string literal cannot contain NUL (0x00) characters.\"\n\n    @cached_property\n    def introspected_field_types(self):\n        return {\n            **super().introspected_field_types,\n            \"PositiveBigIntegerField\": \"BigIntegerField\",\n            \"PositiveIntegerField\": \"IntegerField\",\n            \"PositiveSmallIntegerField\": \"SmallIntegerField\",\n        }\n\n    @cached_property\n    def is_postgresql_15(self):\n        return self.connection.pg_version >= 150000\n\n    @cached_property\n    def is_postgresql_16(self):\n        return self.connection.pg_version >= 160000\n\n    @cached_property\n    def is_postgresql_17(self):\n        return self.connection.pg_version >= 170000\n\n    supports_unlimited_charfield = True\n    supports_nulls_distinct_unique_constraints = property(\n        operator.attrgetter(\"is_postgresql_15\")\n    )"},{"start_line":76,"end_line":82,"belongs_to":{"file_name":"features.py","upper_path":"../django/django/db/backends/mysql","module":"mysql","define_class":["DatabaseFeatures"],"imports":["operator","django.db.backends.base.features.BaseDatabaseFeatures","django.utils.functional.cached_property"]},"relative_function":[],"code":"def test_collations(self):\n        return {\n            \"ci\": \"utf8mb4_general_ci\",\n            \"non_default\": \"utf8mb4_esperanto_ci\",\n            \"swedish_ci\": \"utf8mb4_swedish_ci\",\n            \"virtual\": \"utf8mb4_esperanto_ci\",\n        }"}],"ground_truth":null,"score":null},
{"question":"What is the purpose of the `function` attribute in the `SecondsToInterval`, and how does it affect other features?","answer":"","relative_code_list":[{"start_line":19,"end_line":26,"belongs_to":{"file_name":"functions.py","upper_path":"../django/django/db/backends/oracle","module":"oracle","define_class":["IntervalToSeconds","SecondsToInterval"],"imports":["django.db.models.DecimalField","django.db.models.DurationField","django.db.models.Func"]},"relative_function":[],"code":"class SecondsToInterval(Func):\n    function = \"NUMTODSINTERVAL\"\n    template = \"%(function)s(%(expressions)s, 'SECOND')\"\n\n    def __init__(self, expression, *, output_field=None, **extra):\n        super().__init__(\n            expression, output_field=output_field or DurationField(), **extra\n        )"}],"ground_truth":null,"score":null},
{"question":"What are the expected parameters and return values for the `binary_placeholder_sql` method in the context of `DatabaseOperations`?","answer":"","relative_code_list":[{"start_line":27,"end_line":422,"belongs_to":{"file_name":"operations.py","upper_path":"../django/django/db/backends/postgresql","module":"postgresql","define_class":["DatabaseOperations"],"imports":["json","functools.lru_cache","functools.partial","django.conf.settings","django.db.backends.base.operations.BaseDatabaseOperations","django.db.backends.postgresql.compiler.InsertUnnest","django.db.backends.postgresql.psycopg_any.Inet","django.db.backends.postgresql.psycopg_any.Jsonb","django.db.backends.postgresql.psycopg_any.errors","django.db.backends.postgresql.psycopg_any.is_psycopg3","django.db.backends.postgresql.psycopg_any.mogrify","django.db.backends.utils.split_tzname_delta","django.db.models.constants.OnConflict","django.db.models.functions.Cast","django.utils.regex_helper._lazy_re_compile","psycopg.types.numeric","django.db.models"]},"relative_function":[],"code":"class DatabaseOperations(BaseDatabaseOperations):\n    compiler_module = \"django.db.backends.postgresql.compiler\"\n    cast_char_field_without_max_length = \"varchar\"\n    explain_prefix = \"EXPLAIN\"\n    explain_options = frozenset(\n        [\n            \"ANALYZE\",\n            \"BUFFERS\",\n            \"COSTS\",\n            \"GENERIC_PLAN\",\n            \"MEMORY\",\n            \"SETTINGS\",\n            \"SERIALIZE\",\n            \"SUMMARY\",\n            \"TIMING\",\n            \"VERBOSE\",\n            \"WAL\",\n        ]\n    )\n    cast_data_types = {\n        \"AutoField\": \"integer\",\n        \"BigAutoField\": \"bigint\",\n        \"SmallAutoField\": \"smallint\",\n    }\n\n    if is_psycopg3:\n        from psycopg.types import numeric\n\n        integerfield_type_map = {\n            \"SmallIntegerField\": numeric.Int2,\n            \"IntegerField\": numeric.Int4,\n            \"BigIntegerField\": numeric.Int8,\n            \"PositiveSmallIntegerField\": numeric.Int2,\n            \"PositiveIntegerField\": numeric.Int4,\n            \"PositiveBigIntegerField\": numeric.Int8,\n        }\n\n    def unification_cast_sql(self, output_field):\n        internal_type = output_field.get_internal_type()\n        if internal_type in (\n            \"GenericIPAddressField\",\n            \"IPAddressField\",\n            \"TimeField\",\n            \"UUIDField\",\n        ):\n            # PostgreSQL will resolve a union as type 'text' if input types are\n            # 'unknown'.\n            # https://www.postgresql.org/docs/current/typeconv-union-case.html\n            # These fields cannot be implicitly cast back in the default\n            # PostgreSQL configuration so we need to explicitly cast them.\n            # We must also remove components of the type within brackets:\n            # varchar(255) -> varchar.\n            return (\n                \"CAST(%%s AS %s)\" % output_field.db_type(self.connection).split(\"(\")[0]\n            )\n        return \"%s\"\n\n    # EXTRACT format cannot be passed in parameters.\n    _extract_format_re = _lazy_re_compile(r\"[A-Z_]+\")\n\n    def date_extract_sql(self, lookup_type, sql, params):\n        # https://www.postgresql.org/docs/current/functions-datetime.html#FUNCTIONS-DATETIME-EXTRACT\n        if lookup_type == \"week_day\":\n            # For consistency across backends, we return Sunday=1, Saturday=7.\n            return f\"EXTRACT(DOW FROM {sql}) + 1\", params\n        elif lookup_type == \"iso_week_day\":\n            return f\"EXTRACT(ISODOW FROM {sql})\", params\n        elif lookup_type == \"iso_year\":\n            return f\"EXTRACT(ISOYEAR FROM {sql})\", params\n\n        lookup_type = lookup_type.upper()\n        if not self._extract_format_re.fullmatch(lookup_type):\n            raise ValueError(f\"Invalid lookup type: {lookup_type!r}\")\n        return f\"EXTRACT({lookup_type} FROM {sql})\", params\n\n    def date_trunc_sql(self, lookup_type, sql, params, tzname=None):\n        sql, params = self._convert_sql_to_tz(sql, params, tzname)\n        # https://www.postgresql.org/docs/current/functions-datetime.html#FUNCTIONS-DATETIME-TRUNC\n        return f\"DATE_TRUNC(%s, {sql})\", (lookup_type, *params)\n\n    def _prepare_tzname_delta(self, tzname):\n        tzname, sign, offset = split_tzname_delta(tzname)\n        if offset:\n            sign = \"-\" if sign == \"+\" else \"+\"\n            return f\"{tzname}{sign}{offset}\"\n        return tzname\n\n    def _convert_sql_to_tz(self, sql, params, tzname):\n        if tzname and settings.USE_TZ:\n            tzname_param = self._prepare_tzname_delta(tzname)\n            return f\"{sql} AT TIME ZONE %s\", (*params, tzname_param)\n        return sql, params\n\n    def datetime_cast_date_sql(self, sql, params, tzname):\n        sql, params = self._convert_sql_to_tz(sql, params, tzname)\n        return f\"({sql})::date\", params\n\n    def datetime_cast_time_sql(self, sql, params, tzname):\n        sql, params = self._convert_sql_to_tz(sql, params, tzname)\n        return f\"({sql})::time\", params\n\n    def datetime_extract_sql(self, lookup_type, sql, params, tzname):\n        sql, params = self._convert_sql_to_tz(sql, params, tzname)\n        if lookup_type == \"second\":\n            # Truncate fractional seconds.\n            return f\"EXTRACT(SECOND FROM DATE_TRUNC(%s, {sql}))\", (\"second\", *params)\n        return self.date_extract_sql(lookup_type, sql, params)\n\n    def datetime_trunc_sql(self, lookup_type, sql, params, tzname):\n        sql, params = self._convert_sql_to_tz(sql, params, tzname)\n        # https://www.postgresql.org/docs/current/functions-datetime.html#FUNCTIONS-DATETIME-TRUNC\n        return f\"DATE_TRUNC(%s, {sql})\", (lookup_type, *params)\n\n    def time_extract_sql(self, lookup_type, sql, params):\n        if lookup_type == \"second\":\n            # Truncate fractional seconds.\n            return f\"EXTRACT(SECOND FROM DATE_TRUNC(%s, {sql}))\", (\"second\", *params)\n        return self.date_extract_sql(lookup_type, sql, params)\n\n    def time_trunc_sql(self, lookup_type, sql, params, tzname=None):\n        sql, params = self._convert_sql_to_tz(sql, params, tzname)\n        return f\"DATE_TRUNC(%s, {sql})::time\", (lookup_type, *params)\n\n    def deferrable_sql(self):\n        return \" DEFERRABLE INITIALLY DEFERRED\"\n\n    def bulk_insert_sql(self, fields, placeholder_rows):\n        if isinstance(placeholder_rows, InsertUnnest):\n            return f\"SELECT * FROM {placeholder_rows}\"\n        return super().bulk_insert_sql(fields, placeholder_rows)\n\n    def fetch_returned_insert_rows(self, cursor):\n        \"\"\"\n        Given a cursor object that has just performed an INSERT...RETURNING\n        statement into a table, return the tuple of returned data.\n        \"\"\"\n        return cursor.fetchall()\n\n    def lookup_cast(self, lookup_type, internal_type=None):\n        lookup = \"%s\"\n        # Cast text lookups to text to allow things like filter(x__contains=4)\n        if lookup_type in (\n            \"iexact\",\n            \"contains\",\n            \"icontains\",\n            \"startswith\",\n            \"istartswith\",\n            \"endswith\",\n            \"iendswith\",\n            \"regex\",\n            \"iregex\",\n        ):\n            if internal_type in (\"IPAddressField\", \"GenericIPAddressField\"):\n                lookup = \"HOST(%s)\"\n            else:\n                lookup = \"%s::text\"\n\n        # Use UPPER(x) for case-insensitive lookups; it's faster.\n        if lookup_type in (\"iexact\", \"icontains\", \"istartswith\", \"iendswith\"):\n            lookup = \"UPPER(%s)\" % lookup\n\n        return lookup\n\n    def no_limit_value(self):\n        return None\n\n    def prepare_sql_script(self, sql):\n        return [sql]\n\n    def quote_name(self, name):\n        if name.startswith('\"') and name.endswith('\"'):\n            return name  # Quoting once is enough.\n        return '\"%s\"' % name\n\n    def compose_sql(self, sql, params):\n        return mogrify(sql, params, self.connection)\n\n    def set_time_zone_sql(self):\n        return \"SELECT set_config('TimeZone', %s, false)\"\n\n    def sql_flush(self, style, tables, *, reset_sequences=False, allow_cascade=False):\n        if not tables:\n            return []\n\n        # Perform a single SQL 'TRUNCATE x, y, z...;' statement. It allows us\n        # to truncate tables referenced by a foreign key in any other table.\n        sql_parts = [\n            style.SQL_KEYWORD(\"TRUNCATE\"),\n            \", \".join(style.SQL_FIELD(self.quote_name(table)) for table in tables),\n        ]\n        if reset_sequences:\n            sql_parts.append(style.SQL_KEYWORD(\"RESTART IDENTITY\"))\n        if allow_cascade:\n            sql_parts.append(style.SQL_KEYWORD(\"CASCADE\"))\n        return [\"%s;\" % \" \".join(sql_parts)]\n\n    def sequence_reset_by_name_sql(self, style, sequences):\n        # 'ALTER SEQUENCE sequence_name RESTART WITH 1;'... style SQL statements\n        # to reset sequence indices\n        sql = []\n        for sequence_info in sequences:\n            table_name = sequence_info[\"table\"]\n            # 'id' will be the case if it's an m2m using an autogenerated\n            # intermediate table (see BaseDatabaseIntrospection.sequence_list).\n            column_name = sequence_info[\"column\"] or \"id\"\n            sql.append(\n                \"%s setval(pg_get_serial_sequence('%s','%s'), 1, false);\"\n                % (\n                    style.SQL_KEYWORD(\"SELECT\"),\n                    style.SQL_TABLE(self.quote_name(table_name)),\n                    style.SQL_FIELD(column_name),\n                )\n            )\n        return sql\n\n    def tablespace_sql(self, tablespace, inline=False):\n        if inline:\n            return \"USING INDEX TABLESPACE %s\" % self.quote_name(tablespace)\n        else:\n            return \"TABLESPACE %s\" % self.quote_name(tablespace)\n\n    def sequence_reset_sql(self, style, model_list):\n        from django.db import models\n\n        output = []\n        qn = self.quote_name\n        for model in model_list:\n            # Use `coalesce` to set the sequence for each model to the max pk\n            # value if there are records, or 1 if there are none. Set the\n            # `is_called` property (the third argument to `setval`) to true if\n            # there are records (as the max pk value is already in use),\n            # otherwise set it to false. Use pg_get_serial_sequence to get the\n            # underlying sequence name from the table name and column name.\n\n            for f in model._meta.local_fields:\n                if isinstance(f, models.AutoField):\n                    output.append(\n                        \"%s setval(pg_get_serial_sequence('%s','%s'), \"\n                        \"coalesce(max(%s), 1), max(%s) %s null) %s %s;\"\n                        % (\n                            style.SQL_KEYWORD(\"SELECT\"),\n                            style.SQL_TABLE(qn(model._meta.db_table)),\n                            style.SQL_FIELD(f.column),\n                            style.SQL_FIELD(qn(f.column)),\n                            style.SQL_FIELD(qn(f.column)),\n                            style.SQL_KEYWORD(\"IS NOT\"),\n                            style.SQL_KEYWORD(\"FROM\"),\n                            style.SQL_TABLE(qn(model._meta.db_table)),\n                        )\n                    )\n                    # Only one AutoField is allowed per model, so don't bother\n                    # continuing.\n                    break\n        return output\n\n    def prep_for_iexact_query(self, x):\n        return x\n\n    def max_name_length(self):\n        \"\"\"\n        Return the maximum length of an identifier.\n\n        The maximum length of an identifier is 63 by default, but can be\n        changed by recompiling PostgreSQL after editing the NAMEDATALEN\n        macro in src/include/pg_config_manual.h.\n\n        This implementation returns 63, but can be overridden by a custom\n        database backend that inherits most of its behavior from this one.\n        \"\"\"\n        return 63\n\n    def distinct_sql(self, fields, params):\n        if fields:\n            params = [param for param_list in params for param in param_list]\n            return ([\"DISTINCT ON (%s)\" % \", \".join(fields)], params)\n        else:\n            return [\"DISTINCT\"], []\n\n    if is_psycopg3:\n\n        def last_executed_query(self, cursor, sql, params):\n            if self.connection.features.uses_server_side_binding:\n                try:\n                    return self.compose_sql(sql, params)\n                except errors.DataError:\n                    return None\n            else:\n                if cursor._query and cursor._query.query is not None:\n                    return cursor._query.query.decode()\n                return None\n\n    else:\n\n        def last_executed_query(self, cursor, sql, params):\n            # https://www.psycopg.org/docs/cursor.html#cursor.query\n            # The query attribute is a Psycopg extension to the DB API 2.0.\n            if cursor.query is not None:\n                return cursor.query.decode()\n            return None\n\n    def return_insert_columns(self, fields):\n        if not fields:\n            return \"\", ()\n        columns = [\n            \"%s.%s\"\n            % (\n                self.quote_name(field.model._meta.db_table),\n                self.quote_name(field.column),\n            )\n            for field in fields\n        ]\n        return \"RETURNING %s\" % \", \".join(columns), ()\n\n    if is_psycopg3:\n\n        def adapt_integerfield_value(self, value, internal_type):\n            if value is None or hasattr(value, \"resolve_expression\"):\n                return value\n            return self.integerfield_type_map[internal_type](value)\n\n    def adapt_datefield_value(self, value):\n        return value\n\n    def adapt_datetimefield_value(self, value):\n        return value\n\n    def adapt_timefield_value(self, value):\n        return value\n\n    def adapt_ipaddressfield_value(self, value):\n        if value:\n            return Inet(value)\n        return None\n\n    def adapt_json_value(self, value, encoder):\n        return Jsonb(value, dumps=get_json_dumps(encoder))\n\n    def subtract_temporals(self, internal_type, lhs, rhs):\n        if internal_type == \"DateField\":\n            lhs_sql, lhs_params = lhs\n            rhs_sql, rhs_params = rhs\n            params = (*lhs_params, *rhs_params)\n            return \"(interval '1 day' * (%s - %s))\" % (lhs_sql, rhs_sql), params\n        return super().subtract_temporals(internal_type, lhs, rhs)\n\n    def explain_query_prefix(self, format=None, **options):\n        extra = {}\n        if serialize := options.pop(\"serialize\", None):\n            if serialize.upper() in {\"TEXT\", \"BINARY\"}:\n                extra[\"SERIALIZE\"] = serialize.upper()\n        # Normalize options.\n        if options:\n            options = {\n                name.upper(): \"true\" if value else \"false\"\n                for name, value in options.items()\n            }\n            for valid_option in self.explain_options:\n                value = options.pop(valid_option, None)\n                if value is not None:\n                    extra[valid_option] = value\n        prefix = super().explain_query_prefix(format, **options)\n        if format:\n            extra[\"FORMAT\"] = format\n        if extra:\n            prefix += \" (%s)\" % \", \".join(\"%s %s\" % i for i in extra.items())\n        return prefix\n\n    def on_conflict_suffix_sql(self, fields, on_conflict, update_fields, unique_fields):\n        if on_conflict == OnConflict.IGNORE:\n            return \"ON CONFLICT DO NOTHING\"\n        if on_conflict == OnConflict.UPDATE:\n            return \"ON CONFLICT(%s) DO UPDATE SET %s\" % (\n                \", \".join(map(self.quote_name, unique_fields)),\n                \", \".join(\n                    [\n                        f\"{field} = EXCLUDED.{field}\"\n                        for field in map(self.quote_name, update_fields)\n                    ]\n                ),\n            )\n        return super().on_conflict_suffix_sql(\n            fields,\n            on_conflict,\n            update_fields,\n            unique_fields,\n        )\n\n    def prepare_join_on_clause(self, lhs_table, lhs_field, rhs_table, rhs_field):\n        lhs_expr, rhs_expr = super().prepare_join_on_clause(\n            lhs_table, lhs_field, rhs_table, rhs_field\n        )\n\n        if lhs_field.db_type(self.connection) != rhs_field.db_type(self.connection):\n            rhs_expr = Cast(rhs_expr, lhs_field)\n\n        return lhs_expr, rhs_expr"},{"start_line":330,"end_line":333,"belongs_to":{"file_name":"operations.py","upper_path":"../django/django/db/backends/mysql","module":"mysql","define_class":["DatabaseOperations"],"imports":["uuid","django.conf.settings","django.db.backends.base.operations.BaseDatabaseOperations","django.db.backends.utils.split_tzname_delta","django.db.models.Exists","django.db.models.ExpressionWrapper","django.db.models.Lookup","django.db.models.constants.OnConflict","django.utils.timezone","django.utils.encoding.force_str","django.utils.regex_helper._lazy_re_compile"]},"relative_function":[],"code":"def binary_placeholder_sql(self, value):\n        return (\n            \"_binary %s\" if value is not None and not hasattr(value, \"as_sql\") else \"%s\"\n        )"}],"ground_truth":null,"score":null},
{"question":"What are the expected parameters and return values for the `convert_timefield_value` method in the context of `DatabaseOperations`?","answer":"","relative_code_list":[{"start_line":27,"end_line":422,"belongs_to":{"file_name":"operations.py","upper_path":"../django/django/db/backends/postgresql","module":"postgresql","define_class":["DatabaseOperations"],"imports":["json","functools.lru_cache","functools.partial","django.conf.settings","django.db.backends.base.operations.BaseDatabaseOperations","django.db.backends.postgresql.compiler.InsertUnnest","django.db.backends.postgresql.psycopg_any.Inet","django.db.backends.postgresql.psycopg_any.Jsonb","django.db.backends.postgresql.psycopg_any.errors","django.db.backends.postgresql.psycopg_any.is_psycopg3","django.db.backends.postgresql.psycopg_any.mogrify","django.db.backends.utils.split_tzname_delta","django.db.models.constants.OnConflict","django.db.models.functions.Cast","django.utils.regex_helper._lazy_re_compile","psycopg.types.numeric","django.db.models"]},"relative_function":[],"code":"class DatabaseOperations(BaseDatabaseOperations):\n    compiler_module = \"django.db.backends.postgresql.compiler\"\n    cast_char_field_without_max_length = \"varchar\"\n    explain_prefix = \"EXPLAIN\"\n    explain_options = frozenset(\n        [\n            \"ANALYZE\",\n            \"BUFFERS\",\n            \"COSTS\",\n            \"GENERIC_PLAN\",\n            \"MEMORY\",\n            \"SETTINGS\",\n            \"SERIALIZE\",\n            \"SUMMARY\",\n            \"TIMING\",\n            \"VERBOSE\",\n            \"WAL\",\n        ]\n    )\n    cast_data_types = {\n        \"AutoField\": \"integer\",\n        \"BigAutoField\": \"bigint\",\n        \"SmallAutoField\": \"smallint\",\n    }\n\n    if is_psycopg3:\n        from psycopg.types import numeric\n\n        integerfield_type_map = {\n            \"SmallIntegerField\": numeric.Int2,\n            \"IntegerField\": numeric.Int4,\n            \"BigIntegerField\": numeric.Int8,\n            \"PositiveSmallIntegerField\": numeric.Int2,\n            \"PositiveIntegerField\": numeric.Int4,\n            \"PositiveBigIntegerField\": numeric.Int8,\n        }\n\n    def unification_cast_sql(self, output_field):\n        internal_type = output_field.get_internal_type()\n        if internal_type in (\n            \"GenericIPAddressField\",\n            \"IPAddressField\",\n            \"TimeField\",\n            \"UUIDField\",\n        ):\n            # PostgreSQL will resolve a union as type 'text' if input types are\n            # 'unknown'.\n            # https://www.postgresql.org/docs/current/typeconv-union-case.html\n            # These fields cannot be implicitly cast back in the default\n            # PostgreSQL configuration so we need to explicitly cast them.\n            # We must also remove components of the type within brackets:\n            # varchar(255) -> varchar.\n            return (\n                \"CAST(%%s AS %s)\" % output_field.db_type(self.connection).split(\"(\")[0]\n            )\n        return \"%s\"\n\n    # EXTRACT format cannot be passed in parameters.\n    _extract_format_re = _lazy_re_compile(r\"[A-Z_]+\")\n\n    def date_extract_sql(self, lookup_type, sql, params):\n        # https://www.postgresql.org/docs/current/functions-datetime.html#FUNCTIONS-DATETIME-EXTRACT\n        if lookup_type == \"week_day\":\n            # For consistency across backends, we return Sunday=1, Saturday=7.\n            return f\"EXTRACT(DOW FROM {sql}) + 1\", params\n        elif lookup_type == \"iso_week_day\":\n            return f\"EXTRACT(ISODOW FROM {sql})\", params\n        elif lookup_type == \"iso_year\":\n            return f\"EXTRACT(ISOYEAR FROM {sql})\", params\n\n        lookup_type = lookup_type.upper()\n        if not self._extract_format_re.fullmatch(lookup_type):\n            raise ValueError(f\"Invalid lookup type: {lookup_type!r}\")\n        return f\"EXTRACT({lookup_type} FROM {sql})\", params\n\n    def date_trunc_sql(self, lookup_type, sql, params, tzname=None):\n        sql, params = self._convert_sql_to_tz(sql, params, tzname)\n        # https://www.postgresql.org/docs/current/functions-datetime.html#FUNCTIONS-DATETIME-TRUNC\n        return f\"DATE_TRUNC(%s, {sql})\", (lookup_type, *params)\n\n    def _prepare_tzname_delta(self, tzname):\n        tzname, sign, offset = split_tzname_delta(tzname)\n        if offset:\n            sign = \"-\" if sign == \"+\" else \"+\"\n            return f\"{tzname}{sign}{offset}\"\n        return tzname\n\n    def _convert_sql_to_tz(self, sql, params, tzname):\n        if tzname and settings.USE_TZ:\n            tzname_param = self._prepare_tzname_delta(tzname)\n            return f\"{sql} AT TIME ZONE %s\", (*params, tzname_param)\n        return sql, params\n\n    def datetime_cast_date_sql(self, sql, params, tzname):\n        sql, params = self._convert_sql_to_tz(sql, params, tzname)\n        return f\"({sql})::date\", params\n\n    def datetime_cast_time_sql(self, sql, params, tzname):\n        sql, params = self._convert_sql_to_tz(sql, params, tzname)\n        return f\"({sql})::time\", params\n\n    def datetime_extract_sql(self, lookup_type, sql, params, tzname):\n        sql, params = self._convert_sql_to_tz(sql, params, tzname)\n        if lookup_type == \"second\":\n            # Truncate fractional seconds.\n            return f\"EXTRACT(SECOND FROM DATE_TRUNC(%s, {sql}))\", (\"second\", *params)\n        return self.date_extract_sql(lookup_type, sql, params)\n\n    def datetime_trunc_sql(self, lookup_type, sql, params, tzname):\n        sql, params = self._convert_sql_to_tz(sql, params, tzname)\n        # https://www.postgresql.org/docs/current/functions-datetime.html#FUNCTIONS-DATETIME-TRUNC\n        return f\"DATE_TRUNC(%s, {sql})\", (lookup_type, *params)\n\n    def time_extract_sql(self, lookup_type, sql, params):\n        if lookup_type == \"second\":\n            # Truncate fractional seconds.\n            return f\"EXTRACT(SECOND FROM DATE_TRUNC(%s, {sql}))\", (\"second\", *params)\n        return self.date_extract_sql(lookup_type, sql, params)\n\n    def time_trunc_sql(self, lookup_type, sql, params, tzname=None):\n        sql, params = self._convert_sql_to_tz(sql, params, tzname)\n        return f\"DATE_TRUNC(%s, {sql})::time\", (lookup_type, *params)\n\n    def deferrable_sql(self):\n        return \" DEFERRABLE INITIALLY DEFERRED\"\n\n    def bulk_insert_sql(self, fields, placeholder_rows):\n        if isinstance(placeholder_rows, InsertUnnest):\n            return f\"SELECT * FROM {placeholder_rows}\"\n        return super().bulk_insert_sql(fields, placeholder_rows)\n\n    def fetch_returned_insert_rows(self, cursor):\n        \"\"\"\n        Given a cursor object that has just performed an INSERT...RETURNING\n        statement into a table, return the tuple of returned data.\n        \"\"\"\n        return cursor.fetchall()\n\n    def lookup_cast(self, lookup_type, internal_type=None):\n        lookup = \"%s\"\n        # Cast text lookups to text to allow things like filter(x__contains=4)\n        if lookup_type in (\n            \"iexact\",\n            \"contains\",\n            \"icontains\",\n            \"startswith\",\n            \"istartswith\",\n            \"endswith\",\n            \"iendswith\",\n            \"regex\",\n            \"iregex\",\n        ):\n            if internal_type in (\"IPAddressField\", \"GenericIPAddressField\"):\n                lookup = \"HOST(%s)\"\n            else:\n                lookup = \"%s::text\"\n\n        # Use UPPER(x) for case-insensitive lookups; it's faster.\n        if lookup_type in (\"iexact\", \"icontains\", \"istartswith\", \"iendswith\"):\n            lookup = \"UPPER(%s)\" % lookup\n\n        return lookup\n\n    def no_limit_value(self):\n        return None\n\n    def prepare_sql_script(self, sql):\n        return [sql]\n\n    def quote_name(self, name):\n        if name.startswith('\"') and name.endswith('\"'):\n            return name  # Quoting once is enough.\n        return '\"%s\"' % name\n\n    def compose_sql(self, sql, params):\n        return mogrify(sql, params, self.connection)\n\n    def set_time_zone_sql(self):\n        return \"SELECT set_config('TimeZone', %s, false)\"\n\n    def sql_flush(self, style, tables, *, reset_sequences=False, allow_cascade=False):\n        if not tables:\n            return []\n\n        # Perform a single SQL 'TRUNCATE x, y, z...;' statement. It allows us\n        # to truncate tables referenced by a foreign key in any other table.\n        sql_parts = [\n            style.SQL_KEYWORD(\"TRUNCATE\"),\n            \", \".join(style.SQL_FIELD(self.quote_name(table)) for table in tables),\n        ]\n        if reset_sequences:\n            sql_parts.append(style.SQL_KEYWORD(\"RESTART IDENTITY\"))\n        if allow_cascade:\n            sql_parts.append(style.SQL_KEYWORD(\"CASCADE\"))\n        return [\"%s;\" % \" \".join(sql_parts)]\n\n    def sequence_reset_by_name_sql(self, style, sequences):\n        # 'ALTER SEQUENCE sequence_name RESTART WITH 1;'... style SQL statements\n        # to reset sequence indices\n        sql = []\n        for sequence_info in sequences:\n            table_name = sequence_info[\"table\"]\n            # 'id' will be the case if it's an m2m using an autogenerated\n            # intermediate table (see BaseDatabaseIntrospection.sequence_list).\n            column_name = sequence_info[\"column\"] or \"id\"\n            sql.append(\n                \"%s setval(pg_get_serial_sequence('%s','%s'), 1, false);\"\n                % (\n                    style.SQL_KEYWORD(\"SELECT\"),\n                    style.SQL_TABLE(self.quote_name(table_name)),\n                    style.SQL_FIELD(column_name),\n                )\n            )\n        return sql\n\n    def tablespace_sql(self, tablespace, inline=False):\n        if inline:\n            return \"USING INDEX TABLESPACE %s\" % self.quote_name(tablespace)\n        else:\n            return \"TABLESPACE %s\" % self.quote_name(tablespace)\n\n    def sequence_reset_sql(self, style, model_list):\n        from django.db import models\n\n        output = []\n        qn = self.quote_name\n        for model in model_list:\n            # Use `coalesce` to set the sequence for each model to the max pk\n            # value if there are records, or 1 if there are none. Set the\n            # `is_called` property (the third argument to `setval`) to true if\n            # there are records (as the max pk value is already in use),\n            # otherwise set it to false. Use pg_get_serial_sequence to get the\n            # underlying sequence name from the table name and column name.\n\n            for f in model._meta.local_fields:\n                if isinstance(f, models.AutoField):\n                    output.append(\n                        \"%s setval(pg_get_serial_sequence('%s','%s'), \"\n                        \"coalesce(max(%s), 1), max(%s) %s null) %s %s;\"\n                        % (\n                            style.SQL_KEYWORD(\"SELECT\"),\n                            style.SQL_TABLE(qn(model._meta.db_table)),\n                            style.SQL_FIELD(f.column),\n                            style.SQL_FIELD(qn(f.column)),\n                            style.SQL_FIELD(qn(f.column)),\n                            style.SQL_KEYWORD(\"IS NOT\"),\n                            style.SQL_KEYWORD(\"FROM\"),\n                            style.SQL_TABLE(qn(model._meta.db_table)),\n                        )\n                    )\n                    # Only one AutoField is allowed per model, so don't bother\n                    # continuing.\n                    break\n        return output\n\n    def prep_for_iexact_query(self, x):\n        return x\n\n    def max_name_length(self):\n        \"\"\"\n        Return the maximum length of an identifier.\n\n        The maximum length of an identifier is 63 by default, but can be\n        changed by recompiling PostgreSQL after editing the NAMEDATALEN\n        macro in src/include/pg_config_manual.h.\n\n        This implementation returns 63, but can be overridden by a custom\n        database backend that inherits most of its behavior from this one.\n        \"\"\"\n        return 63\n\n    def distinct_sql(self, fields, params):\n        if fields:\n            params = [param for param_list in params for param in param_list]\n            return ([\"DISTINCT ON (%s)\" % \", \".join(fields)], params)\n        else:\n            return [\"DISTINCT\"], []\n\n    if is_psycopg3:\n\n        def last_executed_query(self, cursor, sql, params):\n            if self.connection.features.uses_server_side_binding:\n                try:\n                    return self.compose_sql(sql, params)\n                except errors.DataError:\n                    return None\n            else:\n                if cursor._query and cursor._query.query is not None:\n                    return cursor._query.query.decode()\n                return None\n\n    else:\n\n        def last_executed_query(self, cursor, sql, params):\n            # https://www.psycopg.org/docs/cursor.html#cursor.query\n            # The query attribute is a Psycopg extension to the DB API 2.0.\n            if cursor.query is not None:\n                return cursor.query.decode()\n            return None\n\n    def return_insert_columns(self, fields):\n        if not fields:\n            return \"\", ()\n        columns = [\n            \"%s.%s\"\n            % (\n                self.quote_name(field.model._meta.db_table),\n                self.quote_name(field.column),\n            )\n            for field in fields\n        ]\n        return \"RETURNING %s\" % \", \".join(columns), ()\n\n    if is_psycopg3:\n\n        def adapt_integerfield_value(self, value, internal_type):\n            if value is None or hasattr(value, \"resolve_expression\"):\n                return value\n            return self.integerfield_type_map[internal_type](value)\n\n    def adapt_datefield_value(self, value):\n        return value\n\n    def adapt_datetimefield_value(self, value):\n        return value\n\n    def adapt_timefield_value(self, value):\n        return value\n\n    def adapt_ipaddressfield_value(self, value):\n        if value:\n            return Inet(value)\n        return None\n\n    def adapt_json_value(self, value, encoder):\n        return Jsonb(value, dumps=get_json_dumps(encoder))\n\n    def subtract_temporals(self, internal_type, lhs, rhs):\n        if internal_type == \"DateField\":\n            lhs_sql, lhs_params = lhs\n            rhs_sql, rhs_params = rhs\n            params = (*lhs_params, *rhs_params)\n            return \"(interval '1 day' * (%s - %s))\" % (lhs_sql, rhs_sql), params\n        return super().subtract_temporals(internal_type, lhs, rhs)\n\n    def explain_query_prefix(self, format=None, **options):\n        extra = {}\n        if serialize := options.pop(\"serialize\", None):\n            if serialize.upper() in {\"TEXT\", \"BINARY\"}:\n                extra[\"SERIALIZE\"] = serialize.upper()\n        # Normalize options.\n        if options:\n            options = {\n                name.upper(): \"true\" if value else \"false\"\n                for name, value in options.items()\n            }\n            for valid_option in self.explain_options:\n                value = options.pop(valid_option, None)\n                if value is not None:\n                    extra[valid_option] = value\n        prefix = super().explain_query_prefix(format, **options)\n        if format:\n            extra[\"FORMAT\"] = format\n        if extra:\n            prefix += \" (%s)\" % \", \".join(\"%s %s\" % i for i in extra.items())\n        return prefix\n\n    def on_conflict_suffix_sql(self, fields, on_conflict, update_fields, unique_fields):\n        if on_conflict == OnConflict.IGNORE:\n            return \"ON CONFLICT DO NOTHING\"\n        if on_conflict == OnConflict.UPDATE:\n            return \"ON CONFLICT(%s) DO UPDATE SET %s\" % (\n                \", \".join(map(self.quote_name, unique_fields)),\n                \", \".join(\n                    [\n                        f\"{field} = EXCLUDED.{field}\"\n                        for field in map(self.quote_name, update_fields)\n                    ]\n                ),\n            )\n        return super().on_conflict_suffix_sql(\n            fields,\n            on_conflict,\n            update_fields,\n            unique_fields,\n        )\n\n    def prepare_join_on_clause(self, lhs_table, lhs_field, rhs_table, rhs_field):\n        lhs_expr, rhs_expr = super().prepare_join_on_clause(\n            lhs_table, lhs_field, rhs_table, rhs_field\n        )\n\n        if lhs_field.db_type(self.connection) != rhs_field.db_type(self.connection):\n            rhs_expr = Cast(rhs_expr, lhs_field)\n\n        return lhs_expr, rhs_expr"},{"start_line":329,"end_line":333,"belongs_to":{"file_name":"operations.py","upper_path":"../django/django/db/backends/sqlite3","module":"sqlite3","define_class":["DatabaseOperations"],"imports":["datetime","decimal","uuid","functools.lru_cache","itertools.chain","django.conf.settings","django.core.exceptions.FieldError","django.db.DatabaseError","django.db.NotSupportedError","django.db.models","django.db.backends.base.operations.BaseDatabaseOperations","django.db.models.constants.OnConflict","django.db.models.expressions.Col","django.utils.timezone","django.utils.dateparse.parse_date","django.utils.dateparse.parse_datetime","django.utils.dateparse.parse_time","django.utils.functional.cached_property","base.Database"]},"relative_function":[],"code":"def convert_timefield_value(self, value, expression, connection):\n        if value is not None:\n            if not isinstance(value, datetime.time):\n                value = parse_time(value)\n        return value"}],"ground_truth":null,"score":null},
{"question":"What is the purpose of the `codes_for_integrityerror` attribute in the `CursorWrapper`, and how does it affect other features?","answer":"","relative_code_list":[{"start_line":17,"end_line":114,"belongs_to":{"file_name":"utils.py","upper_path":"../django/django/db/backends","module":"backends","define_class":["CursorWrapper","CursorDebugWrapper"],"imports":["datetime","decimal","functools","logging","time","warnings","contextlib.contextmanager","hashlib.md5","django.apps.apps","django.db.NotSupportedError","django.utils.dateparse.parse_time"]},"relative_function":[],"code":"class CursorWrapper:\n    def __init__(self, cursor, db):\n        self.cursor = cursor\n        self.db = db\n\n    WRAP_ERROR_ATTRS = frozenset([\"fetchone\", \"fetchmany\", \"fetchall\", \"nextset\"])\n\n    APPS_NOT_READY_WARNING_MSG = (\n        \"Accessing the database during app initialization is discouraged. To fix this \"\n        \"warning, avoid executing queries in AppConfig.ready() or when your app \"\n        \"modules are imported.\"\n    )\n\n    def __getattr__(self, attr):\n        cursor_attr = getattr(self.cursor, attr)\n        if attr in CursorWrapper.WRAP_ERROR_ATTRS:\n            return self.db.wrap_database_errors(cursor_attr)\n        else:\n            return cursor_attr\n\n    def __iter__(self):\n        with self.db.wrap_database_errors:\n            yield from self.cursor\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, type, value, traceback):\n        # Close instead of passing through to avoid backend-specific behavior\n        # (#17671). Catch errors liberally because errors in cleanup code\n        # aren't useful.\n        try:\n            self.close()\n        except self.db.Database.Error:\n            pass\n\n    # The following methods cannot be implemented in __getattr__, because the\n    # code must run when the method is invoked, not just when it is accessed.\n\n    def callproc(self, procname, params=None, kparams=None):\n        # Keyword parameters for callproc aren't supported in PEP 249, but the\n        # database driver may support them (e.g. oracledb).\n        if kparams is not None and not self.db.features.supports_callproc_kwargs:\n            raise NotSupportedError(\n                \"Keyword parameters for callproc are not supported on this \"\n                \"database backend.\"\n            )\n        # Raise a warning during app initialization (stored_app_configs is only\n        # ever set during testing).\n        if not apps.ready and not apps.stored_app_configs:\n            warnings.warn(self.APPS_NOT_READY_WARNING_MSG, category=RuntimeWarning)\n        self.db.validate_no_broken_transaction()\n        with self.db.wrap_database_errors:\n            if params is None and kparams is None:\n                return self.cursor.callproc(procname)\n            elif kparams is None:\n                return self.cursor.callproc(procname, params)\n            else:\n                params = params or ()\n                return self.cursor.callproc(procname, params, kparams)\n\n    def execute(self, sql, params=None):\n        return self._execute_with_wrappers(\n            sql, params, many=False, executor=self._execute\n        )\n\n    def executemany(self, sql, param_list):\n        return self._execute_with_wrappers(\n            sql, param_list, many=True, executor=self._executemany\n        )\n\n    def _execute_with_wrappers(self, sql, params, many, executor):\n        context = {\"connection\": self.db, \"cursor\": self}\n        for wrapper in reversed(self.db.execute_wrappers):\n            executor = functools.partial(wrapper, executor)\n        return executor(sql, params, many, context)\n\n    def _execute(self, sql, params, *ignored_wrapper_args):\n        # Raise a warning during app initialization (stored_app_configs is only\n        # ever set during testing).\n        if not apps.ready and not apps.stored_app_configs:\n            warnings.warn(self.APPS_NOT_READY_WARNING_MSG, category=RuntimeWarning)\n        self.db.validate_no_broken_transaction()\n        with self.db.wrap_database_errors:\n            if params is None:\n                # params default might be backend specific.\n                return self.cursor.execute(sql)\n            else:\n                return self.cursor.execute(sql, params)\n\n    def _executemany(self, sql, param_list, *ignored_wrapper_args):\n        # Raise a warning during app initialization (stored_app_configs is only\n        # ever set during testing).\n        if not apps.ready and not apps.stored_app_configs:\n            warnings.warn(self.APPS_NOT_READY_WARNING_MSG, category=RuntimeWarning)\n        self.db.validate_no_broken_transaction()\n        with self.db.wrap_database_errors:\n            return self.cursor.executemany(sql, param_list)"}],"ground_truth":null,"score":null},
{"question":"What is the purpose of the `<Function>` function in `dummy`?","answer":"","relative_code_list":[{"start_line":52,"end_line":75,"belongs_to":{"file_name":"base.py","upper_path":"../django/django/db/backends/dummy","module":"dummy","define_class":["DatabaseOperations","DatabaseClient","DatabaseCreation","DatabaseIntrospection","DatabaseWrapper"],"imports":["django.core.exceptions.ImproperlyConfigured","django.db.backends.base.base.BaseDatabaseWrapper","django.db.backends.base.client.BaseDatabaseClient","django.db.backends.base.creation.BaseDatabaseCreation","django.db.backends.base.introspection.BaseDatabaseIntrospection","django.db.backends.base.operations.BaseDatabaseOperations","django.db.backends.dummy.features.DummyDatabaseFeatures"]},"relative_function":[],"code":"class DatabaseWrapper(BaseDatabaseWrapper):\n    operators = {}\n    # Override the base class implementations with null\n    # implementations. Anything that tries to actually\n    # do something raises complain; anything that tries\n    # to rollback or undo something raises ignore.\n    _cursor = complain\n    ensure_connection = complain\n    _commit = complain\n    _rollback = ignore\n    _close = ignore\n    _savepoint = ignore\n    _savepoint_commit = complain\n    _savepoint_rollback = ignore\n    _set_autocommit = complain\n    # Classes instantiated in __init__().\n    client_class = DatabaseClient\n    creation_class = DatabaseCreation\n    features_class = DummyDatabaseFeatures\n    introspection_class = DatabaseIntrospection\n    ops_class = DatabaseOperations\n\n    def is_usable(self):\n        return True"}],"ground_truth":null,"score":null},
{"question":"What is the purpose of the `can_distinct_on_fields` attribute in the `DatabaseFeatures`, and how does it affect other features?","answer":"","relative_code_list":[{"start_line":9,"end_line":171,"belongs_to":{"file_name":"features.py","upper_path":"../django/django/db/backends/postgresql","module":"postgresql","define_class":["DatabaseFeatures"],"imports":["operator","django.db.DataError","django.db.InterfaceError","django.db.backends.base.features.BaseDatabaseFeatures","django.db.backends.postgresql.psycopg_any.is_psycopg3","django.utils.functional.cached_property"]},"relative_function":[],"code":"class DatabaseFeatures(BaseDatabaseFeatures):\n    minimum_database_version = (14,)\n    allows_group_by_selected_pks = True\n    can_return_columns_from_insert = True\n    can_return_rows_from_bulk_insert = True\n    has_real_datatype = True\n    has_native_uuid_field = True\n    has_native_duration_field = True\n    has_native_json_field = True\n    can_defer_constraint_checks = True\n    has_select_for_update = True\n    has_select_for_update_nowait = True\n    has_select_for_update_of = True\n    has_select_for_update_skip_locked = True\n    has_select_for_no_key_update = True\n    can_release_savepoints = True\n    supports_comments = True\n    supports_tablespaces = True\n    supports_transactions = True\n    can_introspect_materialized_views = True\n    can_distinct_on_fields = True\n    can_rollback_ddl = True\n    schema_editor_uses_clientside_param_binding = True\n    supports_combined_alters = True\n    nulls_order_largest = True\n    closed_cursor_error_class = InterfaceError\n    greatest_least_ignores_nulls = True\n    can_clone_databases = True\n    supports_temporal_subtraction = True\n    supports_slicing_ordering_in_compound = True\n    create_test_procedure_without_params_sql = \"\"\"\n        CREATE FUNCTION test_procedure () RETURNS void AS $$\n        DECLARE\n            V_I INTEGER;\n        BEGIN\n            V_I := 1;\n        END;\n    $$ LANGUAGE plpgsql;\"\"\"\n    create_test_procedure_with_int_param_sql = \"\"\"\n        CREATE FUNCTION test_procedure (P_I INTEGER) RETURNS void AS $$\n        DECLARE\n            V_I INTEGER;\n        BEGIN\n            V_I := P_I;\n        END;\n    $$ LANGUAGE plpgsql;\"\"\"\n    create_test_table_with_composite_primary_key = \"\"\"\n        CREATE TABLE test_table_composite_pk (\n            column_1 INTEGER NOT NULL,\n            column_2 INTEGER NOT NULL,\n            PRIMARY KEY(column_1, column_2)\n        )\n    \"\"\"\n    requires_casted_case_in_updates = True\n    supports_over_clause = True\n    supports_frame_exclusion = True\n    only_supports_unbounded_with_preceding_and_following = True\n    supports_aggregate_filter_clause = True\n    supports_aggregate_order_by_clause = True\n    supported_explain_formats = {\"JSON\", \"TEXT\", \"XML\", \"YAML\"}\n    supports_deferrable_unique_constraints = True\n    has_json_operators = True\n    json_key_contains_list_matching_requires_list = True\n    supports_update_conflicts = True\n    supports_update_conflicts_with_target = True\n    supports_covering_indexes = True\n    supports_stored_generated_columns = True\n    supports_virtual_generated_columns = False\n    can_rename_index = True\n    test_collations = {\n        \"deterministic\": \"C\",\n        \"non_default\": \"sv-x-icu\",\n        \"swedish_ci\": \"sv-x-icu\",\n        \"virtual\": \"sv-x-icu\",\n    }\n    test_now_utc_template = \"STATEMENT_TIMESTAMP() AT TIME ZONE 'UTC'\"\n    insert_test_table_with_defaults = \"INSERT INTO {} DEFAULT VALUES\"\n\n    @cached_property\n    def django_test_skips(self):\n        skips = {\n            \"opclasses are PostgreSQL only.\": {\n                \"indexes.tests.SchemaIndexesNotPostgreSQLTests.\"\n                \"test_create_index_ignores_opclasses\",\n            },\n            \"PostgreSQL requires casting to text.\": {\n                \"lookup.tests.LookupTests.test_textfield_exact_null\",\n            },\n        }\n        if self.connection.settings_dict[\"OPTIONS\"].get(\"pool\"):\n            skips.update(\n                {\n                    \"Pool does implicit health checks\": {\n                        \"backends.base.test_base.ConnectionHealthChecksTests.\"\n                        \"test_health_checks_enabled\",\n                        \"backends.base.test_base.ConnectionHealthChecksTests.\"\n                        \"test_set_autocommit_health_checks_enabled\",\n                    },\n                }\n            )\n        if self.uses_server_side_binding:\n            skips.update(\n                {\n                    \"The actual query cannot be determined for server side bindings\": {\n                        \"backends.base.test_base.ExecuteWrapperTests.\"\n                        \"test_wrapper_debug\",\n                    }\n                },\n            )\n        return skips\n\n    @cached_property\n    def django_test_expected_failures(self):\n        expected_failures = set()\n        if self.uses_server_side_binding:\n            expected_failures.update(\n                {\n                    # Parameters passed to expressions in SELECT and GROUP BY\n                    # clauses are not recognized as the same values when using\n                    # server-side binding cursors (#34255).\n                    \"aggregation.tests.AggregateTestCase.\"\n                    \"test_group_by_nested_expression_with_params\",\n                }\n            )\n        return expected_failures\n\n    @cached_property\n    def uses_server_side_binding(self):\n        options = self.connection.settings_dict[\"OPTIONS\"]\n        return is_psycopg3 and options.get(\"server_side_binding\") is True\n\n    @cached_property\n    def prohibits_null_characters_in_text_exception(self):\n        if is_psycopg3:\n            return DataError, \"PostgreSQL text fields cannot contain NUL (0x00) bytes\"\n        else:\n            return ValueError, \"A string literal cannot contain NUL (0x00) characters.\"\n\n    @cached_property\n    def introspected_field_types(self):\n        return {\n            **super().introspected_field_types,\n            \"PositiveBigIntegerField\": \"BigIntegerField\",\n            \"PositiveIntegerField\": \"IntegerField\",\n            \"PositiveSmallIntegerField\": \"SmallIntegerField\",\n        }\n\n    @cached_property\n    def is_postgresql_15(self):\n        return self.connection.pg_version >= 150000\n\n    @cached_property\n    def is_postgresql_16(self):\n        return self.connection.pg_version >= 160000\n\n    @cached_property\n    def is_postgresql_17(self):\n        return self.connection.pg_version >= 170000\n\n    supports_unlimited_charfield = True\n    supports_nulls_distinct_unique_constraints = property(\n        operator.attrgetter(\"is_postgresql_15\")\n    )"}],"ground_truth":null,"score":null},
{"question":"What is the purpose of the `supports_unlimited_charfield` attribute in the `DatabaseFeatures`, and how is it intended to be used?","answer":"","relative_code_list":[{"start_line":9,"end_line":171,"belongs_to":{"file_name":"features.py","upper_path":"../django/django/db/backends/postgresql","module":"postgresql","define_class":["DatabaseFeatures"],"imports":["operator","django.db.DataError","django.db.InterfaceError","django.db.backends.base.features.BaseDatabaseFeatures","django.db.backends.postgresql.psycopg_any.is_psycopg3","django.utils.functional.cached_property"]},"relative_function":[],"code":"class DatabaseFeatures(BaseDatabaseFeatures):\n    minimum_database_version = (14,)\n    allows_group_by_selected_pks = True\n    can_return_columns_from_insert = True\n    can_return_rows_from_bulk_insert = True\n    has_real_datatype = True\n    has_native_uuid_field = True\n    has_native_duration_field = True\n    has_native_json_field = True\n    can_defer_constraint_checks = True\n    has_select_for_update = True\n    has_select_for_update_nowait = True\n    has_select_for_update_of = True\n    has_select_for_update_skip_locked = True\n    has_select_for_no_key_update = True\n    can_release_savepoints = True\n    supports_comments = True\n    supports_tablespaces = True\n    supports_transactions = True\n    can_introspect_materialized_views = True\n    can_distinct_on_fields = True\n    can_rollback_ddl = True\n    schema_editor_uses_clientside_param_binding = True\n    supports_combined_alters = True\n    nulls_order_largest = True\n    closed_cursor_error_class = InterfaceError\n    greatest_least_ignores_nulls = True\n    can_clone_databases = True\n    supports_temporal_subtraction = True\n    supports_slicing_ordering_in_compound = True\n    create_test_procedure_without_params_sql = \"\"\"\n        CREATE FUNCTION test_procedure () RETURNS void AS $$\n        DECLARE\n            V_I INTEGER;\n        BEGIN\n            V_I := 1;\n        END;\n    $$ LANGUAGE plpgsql;\"\"\"\n    create_test_procedure_with_int_param_sql = \"\"\"\n        CREATE FUNCTION test_procedure (P_I INTEGER) RETURNS void AS $$\n        DECLARE\n            V_I INTEGER;\n        BEGIN\n            V_I := P_I;\n        END;\n    $$ LANGUAGE plpgsql;\"\"\"\n    create_test_table_with_composite_primary_key = \"\"\"\n        CREATE TABLE test_table_composite_pk (\n            column_1 INTEGER NOT NULL,\n            column_2 INTEGER NOT NULL,\n            PRIMARY KEY(column_1, column_2)\n        )\n    \"\"\"\n    requires_casted_case_in_updates = True\n    supports_over_clause = True\n    supports_frame_exclusion = True\n    only_supports_unbounded_with_preceding_and_following = True\n    supports_aggregate_filter_clause = True\n    supports_aggregate_order_by_clause = True\n    supported_explain_formats = {\"JSON\", \"TEXT\", \"XML\", \"YAML\"}\n    supports_deferrable_unique_constraints = True\n    has_json_operators = True\n    json_key_contains_list_matching_requires_list = True\n    supports_update_conflicts = True\n    supports_update_conflicts_with_target = True\n    supports_covering_indexes = True\n    supports_stored_generated_columns = True\n    supports_virtual_generated_columns = False\n    can_rename_index = True\n    test_collations = {\n        \"deterministic\": \"C\",\n        \"non_default\": \"sv-x-icu\",\n        \"swedish_ci\": \"sv-x-icu\",\n        \"virtual\": \"sv-x-icu\",\n    }\n    test_now_utc_template = \"STATEMENT_TIMESTAMP() AT TIME ZONE 'UTC'\"\n    insert_test_table_with_defaults = \"INSERT INTO {} DEFAULT VALUES\"\n\n    @cached_property\n    def django_test_skips(self):\n        skips = {\n            \"opclasses are PostgreSQL only.\": {\n                \"indexes.tests.SchemaIndexesNotPostgreSQLTests.\"\n                \"test_create_index_ignores_opclasses\",\n            },\n            \"PostgreSQL requires casting to text.\": {\n                \"lookup.tests.LookupTests.test_textfield_exact_null\",\n            },\n        }\n        if self.connection.settings_dict[\"OPTIONS\"].get(\"pool\"):\n            skips.update(\n                {\n                    \"Pool does implicit health checks\": {\n                        \"backends.base.test_base.ConnectionHealthChecksTests.\"\n                        \"test_health_checks_enabled\",\n                        \"backends.base.test_base.ConnectionHealthChecksTests.\"\n                        \"test_set_autocommit_health_checks_enabled\",\n                    },\n                }\n            )\n        if self.uses_server_side_binding:\n            skips.update(\n                {\n                    \"The actual query cannot be determined for server side bindings\": {\n                        \"backends.base.test_base.ExecuteWrapperTests.\"\n                        \"test_wrapper_debug\",\n                    }\n                },\n            )\n        return skips\n\n    @cached_property\n    def django_test_expected_failures(self):\n        expected_failures = set()\n        if self.uses_server_side_binding:\n            expected_failures.update(\n                {\n                    # Parameters passed to expressions in SELECT and GROUP BY\n                    # clauses are not recognized as the same values when using\n                    # server-side binding cursors (#34255).\n                    \"aggregation.tests.AggregateTestCase.\"\n                    \"test_group_by_nested_expression_with_params\",\n                }\n            )\n        return expected_failures\n\n    @cached_property\n    def uses_server_side_binding(self):\n        options = self.connection.settings_dict[\"OPTIONS\"]\n        return is_psycopg3 and options.get(\"server_side_binding\") is True\n\n    @cached_property\n    def prohibits_null_characters_in_text_exception(self):\n        if is_psycopg3:\n            return DataError, \"PostgreSQL text fields cannot contain NUL (0x00) bytes\"\n        else:\n            return ValueError, \"A string literal cannot contain NUL (0x00) characters.\"\n\n    @cached_property\n    def introspected_field_types(self):\n        return {\n            **super().introspected_field_types,\n            \"PositiveBigIntegerField\": \"BigIntegerField\",\n            \"PositiveIntegerField\": \"IntegerField\",\n            \"PositiveSmallIntegerField\": \"SmallIntegerField\",\n        }\n\n    @cached_property\n    def is_postgresql_15(self):\n        return self.connection.pg_version >= 150000\n\n    @cached_property\n    def is_postgresql_16(self):\n        return self.connection.pg_version >= 160000\n\n    @cached_property\n    def is_postgresql_17(self):\n        return self.connection.pg_version >= 170000\n\n    supports_unlimited_charfield = True\n    supports_nulls_distinct_unique_constraints = property(\n        operator.attrgetter(\"is_postgresql_15\")\n    )"}],"ground_truth":null,"score":null},
{"question":"What are the parameters and expected behavior of the `_test_database_tblspace_datafile` method in the `DatabaseCreation`?","answer":"","relative_code_list":[{"start_line":9,"end_line":91,"belongs_to":{"file_name":"creation.py","upper_path":"../django/django/db/backends/postgresql","module":"postgresql","define_class":["DatabaseCreation"],"imports":["sys","django.core.exceptions.ImproperlyConfigured","django.db.backends.base.creation.BaseDatabaseCreation","django.db.backends.postgresql.psycopg_any.errors","django.db.backends.utils.strip_quotes"]},"relative_function":[],"code":"class DatabaseCreation(BaseDatabaseCreation):\n    def _quote_name(self, name):\n        return self.connection.ops.quote_name(name)\n\n    def _get_database_create_suffix(self, encoding=None, template=None):\n        suffix = \"\"\n        if encoding:\n            suffix += \" ENCODING '{}'\".format(encoding)\n        if template:\n            suffix += \" TEMPLATE {}\".format(self._quote_name(template))\n        return suffix and \"WITH\" + suffix\n\n    def sql_table_creation_suffix(self):\n        test_settings = self.connection.settings_dict[\"TEST\"]\n        if test_settings.get(\"COLLATION\") is not None:\n            raise ImproperlyConfigured(\n                \"PostgreSQL does not support collation setting at database \"\n                \"creation time.\"\n            )\n        return self._get_database_create_suffix(\n            encoding=test_settings[\"CHARSET\"],\n            template=test_settings.get(\"TEMPLATE\"),\n        )\n\n    def _database_exists(self, cursor, database_name):\n        cursor.execute(\n            \"SELECT 1 FROM pg_catalog.pg_database WHERE datname = %s\",\n            [strip_quotes(database_name)],\n        )\n        return cursor.fetchone() is not None\n\n    def _execute_create_test_db(self, cursor, parameters, keepdb=False):\n        try:\n            if keepdb and self._database_exists(cursor, parameters[\"dbname\"]):\n                # If the database should be kept and it already exists, don't\n                # try to create a new one.\n                return\n            super()._execute_create_test_db(cursor, parameters, keepdb)\n        except Exception as e:\n            if not isinstance(e.__cause__, errors.DuplicateDatabase):\n                # All errors except \"database already exists\" cancel tests.\n                self.log(\"Got an error creating the test database: %s\" % e)\n                sys.exit(2)\n            elif not keepdb:\n                # If the database should be kept, ignore \"database already\n                # exists\".\n                raise\n\n    def _clone_test_db(self, suffix, verbosity, keepdb=False):\n        # CREATE DATABASE ... WITH TEMPLATE ... requires closing connections\n        # to the template database.\n        self.connection.close()\n        self.connection.close_pool()\n\n        source_database_name = self.connection.settings_dict[\"NAME\"]\n        target_database_name = self.get_test_db_clone_settings(suffix)[\"NAME\"]\n        test_db_params = {\n            \"dbname\": self._quote_name(target_database_name),\n            \"suffix\": self._get_database_create_suffix(template=source_database_name),\n        }\n        with self._nodb_cursor() as cursor:\n            try:\n                self._execute_create_test_db(cursor, test_db_params, keepdb)\n            except Exception:\n                try:\n                    if verbosity >= 1:\n                        self.log(\n                            \"Destroying old test database for alias %s...\"\n                            % (\n                                self._get_database_display_str(\n                                    verbosity, target_database_name\n                                ),\n                            )\n                        )\n                    cursor.execute(\"DROP DATABASE %(dbname)s\" % test_db_params)\n                    self._execute_create_test_db(cursor, test_db_params, keepdb)\n                except Exception as e:\n                    self.log(\"Got an error cloning the test database: %s\" % e)\n                    sys.exit(2)\n\n    def _destroy_test_db(self, test_database_name, verbosity):\n        self.connection.close_pool()\n        return super()._destroy_test_db(test_database_name, verbosity)"},{"start_line":422,"end_line":424,"belongs_to":{"file_name":"creation.py","upper_path":"../django/django/db/backends/oracle","module":"oracle","define_class":["DatabaseCreation"],"imports":["sys","django.conf.settings","django.db.DatabaseError","django.db.backends.base.creation.BaseDatabaseCreation","django.utils.crypto.get_random_string","django.utils.functional.cached_property"]},"relative_function":[],"code":"def _test_database_tblspace_datafile(self):\n        tblspace = \"%s.dbf\" % self._test_database_tblspace()\n        return self._test_settings_get(\"DATAFILE\", default=tblspace)"}],"ground_truth":null,"score":null},
{"question":"What are the parameters and expected behavior of the `__init__` method in the `BaseDatabaseCreation`?","answer":"","relative_code_list":[{"start_line":19,"end_line":398,"belongs_to":{"file_name":"creation.py","upper_path":"../django/django/db/backends/base","module":"base","define_class":["BaseDatabaseCreation"],"imports":["os","sys","warnings","io.StringIO","django.apps.apps","django.conf.settings","django.core.serializers","django.db.router","django.db.transaction.atomic","django.utils.deprecation.RemovedInDjango70Warning","django.utils.module_loading.import_string","django.core.management.call_command","unittest.expectedFailure","unittest.skip","django.db.migrations.loader.MigrationLoader"]},"relative_function":[],"code":"class BaseDatabaseCreation:\n    \"\"\"\n    Encapsulate backend-specific differences pertaining to creation and\n    destruction of the test database.\n    \"\"\"\n\n    def __init__(self, connection):\n        self.connection = connection\n\n    def _nodb_cursor(self):\n        return self.connection._nodb_cursor()\n\n    def log(self, msg):\n        sys.stderr.write(msg + os.linesep)\n\n    # RemovedInDjango70Warning: When the deprecation ends, replace with:\n    # def create_test_db(self, verbosity=1, autoclobber=False, keepdb=False):\n    def create_test_db(\n        self, verbosity=1, autoclobber=False, serialize=None, keepdb=False\n    ):\n        \"\"\"\n        Create a test database, prompting the user for confirmation if the\n        database already exists. Return the name of the test database created.\n        \"\"\"\n        # Don't import django.core.management if it isn't needed.\n        from django.core.management import call_command\n\n        test_database_name = self._get_test_db_name()\n\n        if verbosity >= 1:\n            action = \"Creating\"\n            if keepdb:\n                action = \"Using existing\"\n\n            self.log(\n                \"%s test database for alias %s...\"\n                % (\n                    action,\n                    self._get_database_display_str(verbosity, test_database_name),\n                )\n            )\n\n        # We could skip this call if keepdb is True, but we instead\n        # give it the keepdb param. This is to handle the case\n        # where the test DB doesn't exist, in which case we need to\n        # create it, then just not destroy it. If we instead skip\n        # this, we will get an exception.\n        self._create_test_db(verbosity, autoclobber, keepdb)\n\n        self.connection.close()\n        settings.DATABASES[self.connection.alias][\"NAME\"] = test_database_name\n        self.connection.settings_dict[\"NAME\"] = test_database_name\n\n        try:\n            if self.connection.settings_dict[\"TEST\"][\"MIGRATE\"] is False:\n                # Disable migrations for all apps.\n                old_migration_modules = settings.MIGRATION_MODULES\n                settings.MIGRATION_MODULES = {\n                    app.label: None for app in apps.get_app_configs()\n                }\n            # We report migrate messages at one level lower than that\n            # requested. This ensures we don't get flooded with messages during\n            # testing (unless you really ask to be flooded).\n            call_command(\n                \"migrate\",\n                verbosity=max(verbosity - 1, 0),\n                interactive=False,\n                database=self.connection.alias,\n                run_syncdb=True,\n            )\n        finally:\n            if self.connection.settings_dict[\"TEST\"][\"MIGRATE\"] is False:\n                settings.MIGRATION_MODULES = old_migration_modules\n\n        # We then serialize the current state of the database into a string\n        # and store it on the connection. This slightly horrific process is so people\n        # who are testing on databases without transactions or who are using\n        # a TransactionTestCase still get a clean database on every test run.\n        if serialize is not None:\n            warnings.warn(\n                \"DatabaseCreation.create_test_db(serialize) is deprecated. Call \"\n                \"DatabaseCreation.serialize_test_db() once all test databases are set \"\n                \"up instead if you need fixtures persistence between tests.\",\n                stacklevel=2,\n                category=RemovedInDjango70Warning,\n            )\n            if serialize:\n                self.connection._test_serialized_contents = (\n                    self.serialize_db_to_string()\n                )\n\n        call_command(\"createcachetable\", database=self.connection.alias)\n\n        # Ensure a connection for the side effect of initializing the test database.\n        self.connection.ensure_connection()\n\n        if os.environ.get(\"RUNNING_DJANGOS_TEST_SUITE\") == \"true\":\n            self.mark_expected_failures_and_skips()\n\n        return test_database_name\n\n    def set_as_test_mirror(self, primary_settings_dict):\n        \"\"\"\n        Set this database up to be used in testing as a mirror of a primary\n        database whose settings are given.\n        \"\"\"\n        self.connection.settings_dict[\"NAME\"] = primary_settings_dict[\"NAME\"]\n\n    def serialize_db_to_string(self):\n        \"\"\"\n        Serialize all data in the database into a JSON string.\n        Designed only for test runner usage; will not handle large\n        amounts of data.\n        \"\"\"\n\n        # Iteratively return every object for all models to serialize.\n        def get_objects():\n            from django.db.migrations.loader import MigrationLoader\n\n            loader = MigrationLoader(self.connection)\n            for app_config in apps.get_app_configs():\n                if (\n                    app_config.models_module is not None\n                    and app_config.label in loader.migrated_apps\n                    and app_config.name not in settings.TEST_NON_SERIALIZED_APPS\n                ):\n                    for model in app_config.get_models():\n                        if model._meta.can_migrate(\n                            self.connection\n                        ) and router.allow_migrate_model(self.connection.alias, model):\n                            queryset = model._base_manager.using(\n                                self.connection.alias,\n                            ).order_by(model._meta.pk.name)\n                            chunk_size = (\n                                2000 if queryset._prefetch_related_lookups else None\n                            )\n                            yield from queryset.iterator(chunk_size=chunk_size)\n\n        # Serialize to a string\n        out = StringIO()\n        serializers.serialize(\"json\", get_objects(), indent=None, stream=out)\n        return out.getvalue()\n\n    def deserialize_db_from_string(self, data):\n        \"\"\"\n        Reload the database with data from a string generated by\n        the serialize_db_to_string() method.\n        \"\"\"\n        data = StringIO(data)\n        table_names = set()\n        # Load data in a transaction to handle forward references and cycles.\n        with atomic(using=self.connection.alias):\n            # Disable constraint checks, because some databases (MySQL) doesn't\n            # support deferred checks.\n            with self.connection.constraint_checks_disabled():\n                for obj in serializers.deserialize(\n                    \"json\", data, using=self.connection.alias\n                ):\n                    obj.save()\n                    table_names.add(obj.object.__class__._meta.db_table)\n            # Manually check for any invalid keys that might have been added,\n            # because constraint checks were disabled.\n            self.connection.check_constraints(table_names=table_names)\n\n    def _get_database_display_str(self, verbosity, database_name):\n        \"\"\"\n        Return display string for a database for use in various actions.\n        \"\"\"\n        return \"'%s'%s\" % (\n            self.connection.alias,\n            (\" ('%s')\" % database_name) if verbosity >= 2 else \"\",\n        )\n\n    def _get_test_db_name(self):\n        \"\"\"\n        Internal implementation - return the name of the test DB that will be\n        created. Only useful when called from create_test_db() and\n        _create_test_db() and when no external munging is done with the 'NAME'\n        settings.\n        \"\"\"\n        if self.connection.settings_dict[\"TEST\"][\"NAME\"]:\n            return self.connection.settings_dict[\"TEST\"][\"NAME\"]\n        return TEST_DATABASE_PREFIX + self.connection.settings_dict[\"NAME\"]\n\n    def _execute_create_test_db(self, cursor, parameters, keepdb=False):\n        cursor.execute(\"CREATE DATABASE %(dbname)s %(suffix)s\" % parameters)\n\n    def _create_test_db(self, verbosity, autoclobber, keepdb=False):\n        \"\"\"\n        Internal implementation - create the test db tables.\n        \"\"\"\n        test_database_name = self._get_test_db_name()\n        test_db_params = {\n            \"dbname\": self.connection.ops.quote_name(test_database_name),\n            \"suffix\": self.sql_table_creation_suffix(),\n        }\n        # Create the test database and connect to it.\n        with self._nodb_cursor() as cursor:\n            try:\n                self._execute_create_test_db(cursor, test_db_params, keepdb)\n            except Exception as e:\n                # if we want to keep the db, then no need to do any of the below,\n                # just return and skip it all.\n                if keepdb:\n                    return test_database_name\n\n                self.log(\"Got an error creating the test database: %s\" % e)\n                if not autoclobber:\n                    confirm = input(\n                        \"Type 'yes' if you would like to try deleting the test \"\n                        \"database '%s', or 'no' to cancel: \" % test_database_name\n                    )\n                if autoclobber or confirm == \"yes\":\n                    try:\n                        if verbosity >= 1:\n                            self.log(\n                                \"Destroying old test database for alias %s...\"\n                                % (\n                                    self._get_database_display_str(\n                                        verbosity, test_database_name\n                                    ),\n                                )\n                            )\n                        cursor.execute(\"DROP DATABASE %(dbname)s\" % test_db_params)\n                        self._execute_create_test_db(cursor, test_db_params, keepdb)\n                    except Exception as e:\n                        self.log(\"Got an error recreating the test database: %s\" % e)\n                        sys.exit(2)\n                else:\n                    self.log(\"Tests cancelled.\")\n                    sys.exit(1)\n\n        return test_database_name\n\n    def clone_test_db(self, suffix, verbosity=1, autoclobber=False, keepdb=False):\n        \"\"\"\n        Clone a test database.\n        \"\"\"\n        source_database_name = self.connection.settings_dict[\"NAME\"]\n\n        if verbosity >= 1:\n            action = \"Cloning test database\"\n            if keepdb:\n                action = \"Using existing clone\"\n            self.log(\n                \"%s for alias %s...\"\n                % (\n                    action,\n                    self._get_database_display_str(verbosity, source_database_name),\n                )\n            )\n\n        # We could skip this call if keepdb is True, but we instead\n        # give it the keepdb param. See create_test_db for details.\n        self._clone_test_db(suffix, verbosity, keepdb)\n\n    def get_test_db_clone_settings(self, suffix):\n        \"\"\"\n        Return a modified connection settings dict for the n-th clone of a DB.\n        \"\"\"\n        # When this function is called, the test database has been created\n        # already and its name has been copied to settings_dict['NAME'] so\n        # we don't need to call _get_test_db_name.\n        orig_settings_dict = self.connection.settings_dict\n        return {\n            **orig_settings_dict,\n            \"NAME\": \"{}_{}\".format(orig_settings_dict[\"NAME\"], suffix),\n        }\n\n    def _clone_test_db(self, suffix, verbosity, keepdb=False):\n        \"\"\"\n        Internal implementation - duplicate the test db tables.\n        \"\"\"\n        raise NotImplementedError(\n            \"The database backend doesn't support cloning databases. \"\n            \"Disable the option to run tests in parallel processes.\"\n        )\n\n    def destroy_test_db(\n        self, old_database_name=None, verbosity=1, keepdb=False, suffix=None\n    ):\n        \"\"\"\n        Destroy a test database, prompting the user for confirmation if the\n        database already exists.\n        \"\"\"\n        self.connection.close()\n        if suffix is None:\n            test_database_name = self.connection.settings_dict[\"NAME\"]\n        else:\n            test_database_name = self.get_test_db_clone_settings(suffix)[\"NAME\"]\n\n        if verbosity >= 1:\n            action = \"Destroying\"\n            if keepdb:\n                action = \"Preserving\"\n            self.log(\n                \"%s test database for alias %s...\"\n                % (\n                    action,\n                    self._get_database_display_str(verbosity, test_database_name),\n                )\n            )\n\n        # if we want to preserve the database\n        # skip the actual destroying piece.\n        if not keepdb:\n            self._destroy_test_db(test_database_name, verbosity)\n\n        # Restore the original database name\n        if old_database_name is not None:\n            settings.DATABASES[self.connection.alias][\"NAME\"] = old_database_name\n            self.connection.settings_dict[\"NAME\"] = old_database_name\n\n    def _destroy_test_db(self, test_database_name, verbosity):\n        \"\"\"\n        Internal implementation - remove the test db tables.\n        \"\"\"\n        # Remove the test database to clean up after\n        # ourselves. Connect to the previous database (not the test database)\n        # to do so, because it's not allowed to delete a database while being\n        # connected to it.\n        with self._nodb_cursor() as cursor:\n            cursor.execute(\n                \"DROP DATABASE %s\" % self.connection.ops.quote_name(test_database_name)\n            )\n\n    def mark_expected_failures_and_skips(self):\n        \"\"\"\n        Mark tests in Django's test suite which are expected failures on this\n        database and test which should be skipped on this database.\n        \"\"\"\n        # Only load unittest if we're actually testing.\n        from unittest import expectedFailure, skip\n\n        for test_name in self.connection.features.django_test_expected_failures:\n            test_case_name, _, test_method_name = test_name.rpartition(\".\")\n            test_app = test_name.split(\".\")[0]\n            # Importing a test app that isn't installed raises RuntimeError.\n            if test_app in settings.INSTALLED_APPS:\n                test_case = import_string(test_case_name)\n                test_method = getattr(test_case, test_method_name)\n                setattr(test_case, test_method_name, expectedFailure(test_method))\n        for reason, tests in self.connection.features.django_test_skips.items():\n            for test_name in tests:\n                test_case_name, _, test_method_name = test_name.rpartition(\".\")\n                test_app = test_name.split(\".\")[0]\n                # Importing a test app that isn't installed raises RuntimeError.\n                if test_app in settings.INSTALLED_APPS:\n                    test_case = import_string(test_case_name)\n                    test_method = getattr(test_case, test_method_name)\n                    setattr(test_case, test_method_name, skip(reason)(test_method))\n\n    def sql_table_creation_suffix(self):\n        \"\"\"\n        SQL to append to the end of the test table creation statements.\n        \"\"\"\n        return \"\"\n\n    def test_db_signature(self):\n        \"\"\"\n        Return a tuple with elements of self.connection.settings_dict (a\n        DATABASES setting value) that uniquely identify a database\n        accordingly to the RDBMS particularities.\n        \"\"\"\n        settings_dict = self.connection.settings_dict\n        return (\n            settings_dict[\"HOST\"],\n            settings_dict[\"PORT\"],\n            settings_dict[\"ENGINE\"],\n            self._get_test_db_name(),\n        )\n\n    def setup_worker_connection(self, _worker_id):\n        settings_dict = self.get_test_db_clone_settings(str(_worker_id))\n        # connection.settings_dict must be updated in place for changes to be\n        # reflected in django.db.connections. If the following line assigned\n        # connection.settings_dict = settings_dict, new threads would connect\n        # to the default database instead of the appropriate clone.\n        self.connection.settings_dict.update(settings_dict)\n        self.connection.close()"},{"start_line":25,"end_line":26,"belongs_to":{"file_name":"creation.py","upper_path":"../django/django/db/backends/base","module":"base","define_class":["BaseDatabaseCreation"],"imports":["os","sys","warnings","io.StringIO","django.apps.apps","django.conf.settings","django.core.serializers","django.db.router","django.db.transaction.atomic","django.utils.deprecation.RemovedInDjango70Warning","django.utils.module_loading.import_string","django.core.management.call_command","unittest.expectedFailure","unittest.skip","django.db.migrations.loader.MigrationLoader"]},"relative_function":[],"code":"def __init__(self, connection):\n        self.connection = connection"}],"ground_truth":null,"score":null},
{"question":"Where can I find the file or module that defines the `SQLInsertCompiler` in the codebase?","answer":"","relative_code_list":[{"start_line":28,"end_line":50,"belongs_to":{"file_name":"compiler.py","upper_path":"../django/django/db/backends/postgresql","module":"postgresql","define_class":["InsertUnnest","SQLInsertCompiler"],"imports":["django.db.models.sql.compiler.SQLAggregateCompiler","django.db.models.sql.compiler.SQLCompiler","django.db.models.sql.compiler.SQLDeleteCompiler","django.db.models.sql.compiler.SQLInsertCompiler","django.db.models.sql.compiler.SQLUpdateCompiler"]},"relative_function":[],"code":"class SQLInsertCompiler(BaseSQLInsertCompiler):\n    def assemble_as_sql(self, fields, value_rows):\n        # Specialize bulk-insertion of literal non-array values through\n        # UNNEST to reduce the time spent planning the query.\n        if (\n            # The optimization is not worth doing if there is a single\n            # row as it will result in the same number of placeholders.\n            len(value_rows) <= 1\n            # Lack of fields denote the usage of the DEFAULT keyword\n            # for the insertion of empty rows.\n            or any(field is None for field in fields)\n            # Compilable cannot be combined in an array of literal values.\n            or any(any(hasattr(value, \"as_sql\") for value in row) for row in value_rows)\n        ):\n            return super().assemble_as_sql(fields, value_rows)\n        db_types = [field.db_type(self.connection) for field in fields]\n        # Abort if any of the fields are arrays as UNNEST indiscriminately\n        # flatten them instead of reducing their nesting by one.\n        if any(db_type.endswith(\"]\") for db_type in db_types):\n            return super().assemble_as_sql(fields, value_rows)\n        return InsertUnnest([\"(%%s)::%s[]\" % db_type for db_type in db_types]), [\n            list(map(list, zip(*value_rows)))\n        ]"}],"ground_truth":null,"score":null},
{"question":"Where can I find the implementation of the `convert_uuidfield_value` method in the `DatabaseOperations`?","answer":"","relative_code_list":[{"start_line":27,"end_line":422,"belongs_to":{"file_name":"operations.py","upper_path":"../django/django/db/backends/postgresql","module":"postgresql","define_class":["DatabaseOperations"],"imports":["json","functools.lru_cache","functools.partial","django.conf.settings","django.db.backends.base.operations.BaseDatabaseOperations","django.db.backends.postgresql.compiler.InsertUnnest","django.db.backends.postgresql.psycopg_any.Inet","django.db.backends.postgresql.psycopg_any.Jsonb","django.db.backends.postgresql.psycopg_any.errors","django.db.backends.postgresql.psycopg_any.is_psycopg3","django.db.backends.postgresql.psycopg_any.mogrify","django.db.backends.utils.split_tzname_delta","django.db.models.constants.OnConflict","django.db.models.functions.Cast","django.utils.regex_helper._lazy_re_compile","psycopg.types.numeric","django.db.models"]},"relative_function":[],"code":"class DatabaseOperations(BaseDatabaseOperations):\n    compiler_module = \"django.db.backends.postgresql.compiler\"\n    cast_char_field_without_max_length = \"varchar\"\n    explain_prefix = \"EXPLAIN\"\n    explain_options = frozenset(\n        [\n            \"ANALYZE\",\n            \"BUFFERS\",\n            \"COSTS\",\n            \"GENERIC_PLAN\",\n            \"MEMORY\",\n            \"SETTINGS\",\n            \"SERIALIZE\",\n            \"SUMMARY\",\n            \"TIMING\",\n            \"VERBOSE\",\n            \"WAL\",\n        ]\n    )\n    cast_data_types = {\n        \"AutoField\": \"integer\",\n        \"BigAutoField\": \"bigint\",\n        \"SmallAutoField\": \"smallint\",\n    }\n\n    if is_psycopg3:\n        from psycopg.types import numeric\n\n        integerfield_type_map = {\n            \"SmallIntegerField\": numeric.Int2,\n            \"IntegerField\": numeric.Int4,\n            \"BigIntegerField\": numeric.Int8,\n            \"PositiveSmallIntegerField\": numeric.Int2,\n            \"PositiveIntegerField\": numeric.Int4,\n            \"PositiveBigIntegerField\": numeric.Int8,\n        }\n\n    def unification_cast_sql(self, output_field):\n        internal_type = output_field.get_internal_type()\n        if internal_type in (\n            \"GenericIPAddressField\",\n            \"IPAddressField\",\n            \"TimeField\",\n            \"UUIDField\",\n        ):\n            # PostgreSQL will resolve a union as type 'text' if input types are\n            # 'unknown'.\n            # https://www.postgresql.org/docs/current/typeconv-union-case.html\n            # These fields cannot be implicitly cast back in the default\n            # PostgreSQL configuration so we need to explicitly cast them.\n            # We must also remove components of the type within brackets:\n            # varchar(255) -> varchar.\n            return (\n                \"CAST(%%s AS %s)\" % output_field.db_type(self.connection).split(\"(\")[0]\n            )\n        return \"%s\"\n\n    # EXTRACT format cannot be passed in parameters.\n    _extract_format_re = _lazy_re_compile(r\"[A-Z_]+\")\n\n    def date_extract_sql(self, lookup_type, sql, params):\n        # https://www.postgresql.org/docs/current/functions-datetime.html#FUNCTIONS-DATETIME-EXTRACT\n        if lookup_type == \"week_day\":\n            # For consistency across backends, we return Sunday=1, Saturday=7.\n            return f\"EXTRACT(DOW FROM {sql}) + 1\", params\n        elif lookup_type == \"iso_week_day\":\n            return f\"EXTRACT(ISODOW FROM {sql})\", params\n        elif lookup_type == \"iso_year\":\n            return f\"EXTRACT(ISOYEAR FROM {sql})\", params\n\n        lookup_type = lookup_type.upper()\n        if not self._extract_format_re.fullmatch(lookup_type):\n            raise ValueError(f\"Invalid lookup type: {lookup_type!r}\")\n        return f\"EXTRACT({lookup_type} FROM {sql})\", params\n\n    def date_trunc_sql(self, lookup_type, sql, params, tzname=None):\n        sql, params = self._convert_sql_to_tz(sql, params, tzname)\n        # https://www.postgresql.org/docs/current/functions-datetime.html#FUNCTIONS-DATETIME-TRUNC\n        return f\"DATE_TRUNC(%s, {sql})\", (lookup_type, *params)\n\n    def _prepare_tzname_delta(self, tzname):\n        tzname, sign, offset = split_tzname_delta(tzname)\n        if offset:\n            sign = \"-\" if sign == \"+\" else \"+\"\n            return f\"{tzname}{sign}{offset}\"\n        return tzname\n\n    def _convert_sql_to_tz(self, sql, params, tzname):\n        if tzname and settings.USE_TZ:\n            tzname_param = self._prepare_tzname_delta(tzname)\n            return f\"{sql} AT TIME ZONE %s\", (*params, tzname_param)\n        return sql, params\n\n    def datetime_cast_date_sql(self, sql, params, tzname):\n        sql, params = self._convert_sql_to_tz(sql, params, tzname)\n        return f\"({sql})::date\", params\n\n    def datetime_cast_time_sql(self, sql, params, tzname):\n        sql, params = self._convert_sql_to_tz(sql, params, tzname)\n        return f\"({sql})::time\", params\n\n    def datetime_extract_sql(self, lookup_type, sql, params, tzname):\n        sql, params = self._convert_sql_to_tz(sql, params, tzname)\n        if lookup_type == \"second\":\n            # Truncate fractional seconds.\n            return f\"EXTRACT(SECOND FROM DATE_TRUNC(%s, {sql}))\", (\"second\", *params)\n        return self.date_extract_sql(lookup_type, sql, params)\n\n    def datetime_trunc_sql(self, lookup_type, sql, params, tzname):\n        sql, params = self._convert_sql_to_tz(sql, params, tzname)\n        # https://www.postgresql.org/docs/current/functions-datetime.html#FUNCTIONS-DATETIME-TRUNC\n        return f\"DATE_TRUNC(%s, {sql})\", (lookup_type, *params)\n\n    def time_extract_sql(self, lookup_type, sql, params):\n        if lookup_type == \"second\":\n            # Truncate fractional seconds.\n            return f\"EXTRACT(SECOND FROM DATE_TRUNC(%s, {sql}))\", (\"second\", *params)\n        return self.date_extract_sql(lookup_type, sql, params)\n\n    def time_trunc_sql(self, lookup_type, sql, params, tzname=None):\n        sql, params = self._convert_sql_to_tz(sql, params, tzname)\n        return f\"DATE_TRUNC(%s, {sql})::time\", (lookup_type, *params)\n\n    def deferrable_sql(self):\n        return \" DEFERRABLE INITIALLY DEFERRED\"\n\n    def bulk_insert_sql(self, fields, placeholder_rows):\n        if isinstance(placeholder_rows, InsertUnnest):\n            return f\"SELECT * FROM {placeholder_rows}\"\n        return super().bulk_insert_sql(fields, placeholder_rows)\n\n    def fetch_returned_insert_rows(self, cursor):\n        \"\"\"\n        Given a cursor object that has just performed an INSERT...RETURNING\n        statement into a table, return the tuple of returned data.\n        \"\"\"\n        return cursor.fetchall()\n\n    def lookup_cast(self, lookup_type, internal_type=None):\n        lookup = \"%s\"\n        # Cast text lookups to text to allow things like filter(x__contains=4)\n        if lookup_type in (\n            \"iexact\",\n            \"contains\",\n            \"icontains\",\n            \"startswith\",\n            \"istartswith\",\n            \"endswith\",\n            \"iendswith\",\n            \"regex\",\n            \"iregex\",\n        ):\n            if internal_type in (\"IPAddressField\", \"GenericIPAddressField\"):\n                lookup = \"HOST(%s)\"\n            else:\n                lookup = \"%s::text\"\n\n        # Use UPPER(x) for case-insensitive lookups; it's faster.\n        if lookup_type in (\"iexact\", \"icontains\", \"istartswith\", \"iendswith\"):\n            lookup = \"UPPER(%s)\" % lookup\n\n        return lookup\n\n    def no_limit_value(self):\n        return None\n\n    def prepare_sql_script(self, sql):\n        return [sql]\n\n    def quote_name(self, name):\n        if name.startswith('\"') and name.endswith('\"'):\n            return name  # Quoting once is enough.\n        return '\"%s\"' % name\n\n    def compose_sql(self, sql, params):\n        return mogrify(sql, params, self.connection)\n\n    def set_time_zone_sql(self):\n        return \"SELECT set_config('TimeZone', %s, false)\"\n\n    def sql_flush(self, style, tables, *, reset_sequences=False, allow_cascade=False):\n        if not tables:\n            return []\n\n        # Perform a single SQL 'TRUNCATE x, y, z...;' statement. It allows us\n        # to truncate tables referenced by a foreign key in any other table.\n        sql_parts = [\n            style.SQL_KEYWORD(\"TRUNCATE\"),\n            \", \".join(style.SQL_FIELD(self.quote_name(table)) for table in tables),\n        ]\n        if reset_sequences:\n            sql_parts.append(style.SQL_KEYWORD(\"RESTART IDENTITY\"))\n        if allow_cascade:\n            sql_parts.append(style.SQL_KEYWORD(\"CASCADE\"))\n        return [\"%s;\" % \" \".join(sql_parts)]\n\n    def sequence_reset_by_name_sql(self, style, sequences):\n        # 'ALTER SEQUENCE sequence_name RESTART WITH 1;'... style SQL statements\n        # to reset sequence indices\n        sql = []\n        for sequence_info in sequences:\n            table_name = sequence_info[\"table\"]\n            # 'id' will be the case if it's an m2m using an autogenerated\n            # intermediate table (see BaseDatabaseIntrospection.sequence_list).\n            column_name = sequence_info[\"column\"] or \"id\"\n            sql.append(\n                \"%s setval(pg_get_serial_sequence('%s','%s'), 1, false);\"\n                % (\n                    style.SQL_KEYWORD(\"SELECT\"),\n                    style.SQL_TABLE(self.quote_name(table_name)),\n                    style.SQL_FIELD(column_name),\n                )\n            )\n        return sql\n\n    def tablespace_sql(self, tablespace, inline=False):\n        if inline:\n            return \"USING INDEX TABLESPACE %s\" % self.quote_name(tablespace)\n        else:\n            return \"TABLESPACE %s\" % self.quote_name(tablespace)\n\n    def sequence_reset_sql(self, style, model_list):\n        from django.db import models\n\n        output = []\n        qn = self.quote_name\n        for model in model_list:\n            # Use `coalesce` to set the sequence for each model to the max pk\n            # value if there are records, or 1 if there are none. Set the\n            # `is_called` property (the third argument to `setval`) to true if\n            # there are records (as the max pk value is already in use),\n            # otherwise set it to false. Use pg_get_serial_sequence to get the\n            # underlying sequence name from the table name and column name.\n\n            for f in model._meta.local_fields:\n                if isinstance(f, models.AutoField):\n                    output.append(\n                        \"%s setval(pg_get_serial_sequence('%s','%s'), \"\n                        \"coalesce(max(%s), 1), max(%s) %s null) %s %s;\"\n                        % (\n                            style.SQL_KEYWORD(\"SELECT\"),\n                            style.SQL_TABLE(qn(model._meta.db_table)),\n                            style.SQL_FIELD(f.column),\n                            style.SQL_FIELD(qn(f.column)),\n                            style.SQL_FIELD(qn(f.column)),\n                            style.SQL_KEYWORD(\"IS NOT\"),\n                            style.SQL_KEYWORD(\"FROM\"),\n                            style.SQL_TABLE(qn(model._meta.db_table)),\n                        )\n                    )\n                    # Only one AutoField is allowed per model, so don't bother\n                    # continuing.\n                    break\n        return output\n\n    def prep_for_iexact_query(self, x):\n        return x\n\n    def max_name_length(self):\n        \"\"\"\n        Return the maximum length of an identifier.\n\n        The maximum length of an identifier is 63 by default, but can be\n        changed by recompiling PostgreSQL after editing the NAMEDATALEN\n        macro in src/include/pg_config_manual.h.\n\n        This implementation returns 63, but can be overridden by a custom\n        database backend that inherits most of its behavior from this one.\n        \"\"\"\n        return 63\n\n    def distinct_sql(self, fields, params):\n        if fields:\n            params = [param for param_list in params for param in param_list]\n            return ([\"DISTINCT ON (%s)\" % \", \".join(fields)], params)\n        else:\n            return [\"DISTINCT\"], []\n\n    if is_psycopg3:\n\n        def last_executed_query(self, cursor, sql, params):\n            if self.connection.features.uses_server_side_binding:\n                try:\n                    return self.compose_sql(sql, params)\n                except errors.DataError:\n                    return None\n            else:\n                if cursor._query and cursor._query.query is not None:\n                    return cursor._query.query.decode()\n                return None\n\n    else:\n\n        def last_executed_query(self, cursor, sql, params):\n            # https://www.psycopg.org/docs/cursor.html#cursor.query\n            # The query attribute is a Psycopg extension to the DB API 2.0.\n            if cursor.query is not None:\n                return cursor.query.decode()\n            return None\n\n    def return_insert_columns(self, fields):\n        if not fields:\n            return \"\", ()\n        columns = [\n            \"%s.%s\"\n            % (\n                self.quote_name(field.model._meta.db_table),\n                self.quote_name(field.column),\n            )\n            for field in fields\n        ]\n        return \"RETURNING %s\" % \", \".join(columns), ()\n\n    if is_psycopg3:\n\n        def adapt_integerfield_value(self, value, internal_type):\n            if value is None or hasattr(value, \"resolve_expression\"):\n                return value\n            return self.integerfield_type_map[internal_type](value)\n\n    def adapt_datefield_value(self, value):\n        return value\n\n    def adapt_datetimefield_value(self, value):\n        return value\n\n    def adapt_timefield_value(self, value):\n        return value\n\n    def adapt_ipaddressfield_value(self, value):\n        if value:\n            return Inet(value)\n        return None\n\n    def adapt_json_value(self, value, encoder):\n        return Jsonb(value, dumps=get_json_dumps(encoder))\n\n    def subtract_temporals(self, internal_type, lhs, rhs):\n        if internal_type == \"DateField\":\n            lhs_sql, lhs_params = lhs\n            rhs_sql, rhs_params = rhs\n            params = (*lhs_params, *rhs_params)\n            return \"(interval '1 day' * (%s - %s))\" % (lhs_sql, rhs_sql), params\n        return super().subtract_temporals(internal_type, lhs, rhs)\n\n    def explain_query_prefix(self, format=None, **options):\n        extra = {}\n        if serialize := options.pop(\"serialize\", None):\n            if serialize.upper() in {\"TEXT\", \"BINARY\"}:\n                extra[\"SERIALIZE\"] = serialize.upper()\n        # Normalize options.\n        if options:\n            options = {\n                name.upper(): \"true\" if value else \"false\"\n                for name, value in options.items()\n            }\n            for valid_option in self.explain_options:\n                value = options.pop(valid_option, None)\n                if value is not None:\n                    extra[valid_option] = value\n        prefix = super().explain_query_prefix(format, **options)\n        if format:\n            extra[\"FORMAT\"] = format\n        if extra:\n            prefix += \" (%s)\" % \", \".join(\"%s %s\" % i for i in extra.items())\n        return prefix\n\n    def on_conflict_suffix_sql(self, fields, on_conflict, update_fields, unique_fields):\n        if on_conflict == OnConflict.IGNORE:\n            return \"ON CONFLICT DO NOTHING\"\n        if on_conflict == OnConflict.UPDATE:\n            return \"ON CONFLICT(%s) DO UPDATE SET %s\" % (\n                \", \".join(map(self.quote_name, unique_fields)),\n                \", \".join(\n                    [\n                        f\"{field} = EXCLUDED.{field}\"\n                        for field in map(self.quote_name, update_fields)\n                    ]\n                ),\n            )\n        return super().on_conflict_suffix_sql(\n            fields,\n            on_conflict,\n            update_fields,\n            unique_fields,\n        )\n\n    def prepare_join_on_clause(self, lhs_table, lhs_field, rhs_table, rhs_field):\n        lhs_expr, rhs_expr = super().prepare_join_on_clause(\n            lhs_table, lhs_field, rhs_table, rhs_field\n        )\n\n        if lhs_field.db_type(self.connection) != rhs_field.db_type(self.connection):\n            rhs_expr = Cast(rhs_expr, lhs_field)\n\n        return lhs_expr, rhs_expr"},{"start_line":285,"end_line":288,"belongs_to":{"file_name":"operations.py","upper_path":"../django/django/db/backends/oracle","module":"oracle","define_class":["DatabaseOperations"],"imports":["datetime","uuid","functools.lru_cache","itertools.chain","django.conf.settings","django.db.NotSupportedError","django.db.backends.base.operations.BaseDatabaseOperations","django.db.backends.utils.split_tzname_delta","django.db.backends.utils.strip_quotes","django.db.backends.utils.truncate_name","django.db.models.AutoField","django.db.models.CompositePrimaryKey","django.db.models.Exists","django.db.models.ExpressionWrapper","django.db.models.Lookup","django.db.models.expressions.RawSQL","django.db.models.sql.where.WhereNode","django.utils.timezone","django.utils.encoding.force_bytes","django.utils.encoding.force_str","django.utils.functional.cached_property","django.utils.regex_helper._lazy_re_compile","base.Database","utils.BulkInsertMapper","utils.InsertVar","utils.Oracle_datetime"]},"relative_function":[],"code":"def convert_uuidfield_value(self, value, expression, connection):\n        if value is not None:\n            value = uuid.UUID(value)\n        return value"}],"ground_truth":null,"score":null},
{"question":"What is the purpose of the `prohibits_null_characters_in_text_exception` attribute in `BaseDatabaseFeatures`?","answer":"","relative_code_list":[{"start_line":5,"end_line":432,"belongs_to":{"file_name":"features.py","upper_path":"../django/django/db/backends/base","module":"base","define_class":["BaseDatabaseFeatures"],"imports":["django.db.ProgrammingError","django.utils.functional.cached_property"]},"relative_function":[],"code":"class BaseDatabaseFeatures:\n    # An optional tuple indicating the minimum supported database version.\n    minimum_database_version = None\n    gis_enabled = False\n    # Oracle can't group by LOB (large object) data types.\n    allows_group_by_lob = True\n    allows_group_by_selected_pks = False\n    allows_group_by_select_index = True\n    empty_fetchmany_value = []\n    update_can_self_select = True\n    # Does the backend support self-reference subqueries in the DELETE\n    # statement?\n    delete_can_self_reference_subquery = True\n\n    # Does the backend distinguish between '' and None?\n    interprets_empty_strings_as_nulls = False\n\n    # Does the backend allow inserting duplicate NULL rows in a nullable\n    # unique field? All core backends implement this correctly, but other\n    # databases such as SQL Server do not.\n    supports_nullable_unique_constraints = True\n\n    # Does the backend allow inserting duplicate rows when a unique_together\n    # constraint exists and some fields are nullable but not all of them?\n    supports_partially_nullable_unique_constraints = True\n\n    # Does the backend supports specifying whether NULL values should be\n    # considered distinct in unique constraints?\n    supports_nulls_distinct_unique_constraints = False\n\n    # Does the backend support initially deferrable unique constraints?\n    supports_deferrable_unique_constraints = False\n\n    can_use_chunked_reads = True\n    can_return_columns_from_insert = False\n    can_return_rows_from_bulk_insert = False\n    has_bulk_insert = True\n    uses_savepoints = True\n    can_release_savepoints = False\n\n    # If True, don't use integer foreign keys referring to, e.g., positive\n    # integer primary keys.\n    related_fields_match_type = False\n    allow_sliced_subqueries_with_in = True\n    has_select_for_update = False\n    has_select_for_update_nowait = False\n    has_select_for_update_skip_locked = False\n    has_select_for_update_of = False\n    has_select_for_no_key_update = False\n    # Does the database's SELECT FOR UPDATE OF syntax require a column rather\n    # than a table?\n    select_for_update_of_column = False\n\n    # Does the default test database allow multiple connections?\n    # Usually an indication that the test database is in-memory\n    test_db_allows_multiple_connections = True\n\n    # Can an object be saved without an explicit primary key?\n    supports_unspecified_pk = False\n\n    # Can a fixture contain forward references? i.e., are\n    # FK constraints checked at the end of transaction, or\n    # at the end of each save operation?\n    supports_forward_references = True\n\n    # Does the backend truncate names properly when they are too long?\n    truncates_names = False\n\n    # Is there a REAL datatype in addition to floats/doubles?\n    has_real_datatype = False\n    supports_subqueries_in_group_by = True\n\n    # Does the backend ignore unnecessary ORDER BY clauses in subqueries?\n    ignores_unnecessary_order_by_in_subqueries = True\n\n    # Is there a true datatype for uuid?\n    has_native_uuid_field = False\n\n    # Is there a true datatype for timedeltas?\n    has_native_duration_field = False\n\n    # Does the database driver supports same type temporal data subtraction\n    # by returning the type used to store duration field?\n    supports_temporal_subtraction = False\n\n    # Does the __regex lookup support backreferencing and grouping?\n    supports_regex_backreferencing = True\n\n    # Can date/datetime lookups be performed using a string?\n    supports_date_lookup_using_string = True\n\n    # Can datetimes with timezones be used?\n    supports_timezones = True\n\n    # Does the database have a copy of the zoneinfo database?\n    has_zoneinfo_database = True\n\n    # When performing a GROUP BY, is an ORDER BY NULL required\n    # to remove any ordering?\n    requires_explicit_null_ordering_when_grouping = False\n\n    # Does the backend order NULL values as largest or smallest?\n    nulls_order_largest = False\n\n    # Does the backend support NULLS FIRST and NULLS LAST in ORDER BY?\n    supports_order_by_nulls_modifier = True\n\n    # Does the backend orders NULLS FIRST by default?\n    order_by_nulls_first = False\n\n    # The database's limit on the number of query parameters.\n    max_query_params = None\n\n    # Can an object have an autoincrement primary key of 0?\n    allows_auto_pk_0 = True\n\n    # Do we need to NULL a ForeignKey out, or can the constraint check be\n    # deferred\n    can_defer_constraint_checks = False\n\n    # Does the backend support tablespaces? Default to False because it isn't\n    # in the SQL standard.\n    supports_tablespaces = False\n\n    # Does the backend reset sequences between tests?\n    supports_sequence_reset = True\n\n    # Can the backend introspect the default value of a column?\n    can_introspect_default = True\n\n    # Confirm support for introspected foreign keys\n    # Every database can do this reliably, except MySQL,\n    # which can't do it for MyISAM tables\n    can_introspect_foreign_keys = True\n\n    # Map fields which some backends may not be able to differentiate to the\n    # field it's introspected as.\n    introspected_field_types = {\n        \"AutoField\": \"AutoField\",\n        \"BigAutoField\": \"BigAutoField\",\n        \"BigIntegerField\": \"BigIntegerField\",\n        \"BinaryField\": \"BinaryField\",\n        \"BooleanField\": \"BooleanField\",\n        \"CharField\": \"CharField\",\n        \"DurationField\": \"DurationField\",\n        \"GenericIPAddressField\": \"GenericIPAddressField\",\n        \"IntegerField\": \"IntegerField\",\n        \"PositiveBigIntegerField\": \"PositiveBigIntegerField\",\n        \"PositiveIntegerField\": \"PositiveIntegerField\",\n        \"PositiveSmallIntegerField\": \"PositiveSmallIntegerField\",\n        \"SmallAutoField\": \"SmallAutoField\",\n        \"SmallIntegerField\": \"SmallIntegerField\",\n        \"TimeField\": \"TimeField\",\n    }\n\n    # Can the backend introspect the column order (ASC/DESC) for indexes?\n    supports_index_column_ordering = True\n\n    # Does the backend support introspection of materialized views?\n    can_introspect_materialized_views = False\n\n    # Support for the DISTINCT ON clause\n    can_distinct_on_fields = False\n\n    # Does the backend prevent running SQL queries in broken transactions?\n    atomic_transactions = True\n\n    # Can we roll back DDL in a transaction?\n    can_rollback_ddl = False\n\n    schema_editor_uses_clientside_param_binding = False\n\n    # Can we issue more than one ALTER COLUMN clause in an ALTER TABLE?\n    supports_combined_alters = False\n\n    # Does it support foreign keys?\n    supports_foreign_keys = True\n\n    # Can it create foreign key constraints inline when adding columns?\n    can_create_inline_fk = True\n\n    # Can an index be renamed?\n    can_rename_index = False\n\n    # Does it automatically index foreign keys?\n    indexes_foreign_keys = True\n\n    # Does it support CHECK constraints?\n    supports_column_check_constraints = True\n    supports_table_check_constraints = True\n    # Does the backend support introspection of CHECK constraints?\n    can_introspect_check_constraints = True\n\n    # Does the backend support 'pyformat' style (\"... %(name)s ...\", {'name': value})\n    # parameter passing? Note this can be provided by the backend even if not\n    # supported by the Python driver\n    supports_paramstyle_pyformat = True\n\n    # Does the backend require literal defaults, rather than parameterized ones?\n    requires_literal_defaults = False\n\n    # Does the backend support functions in defaults?\n    supports_expression_defaults = True\n\n    # Does the backend support the DEFAULT keyword in insert queries?\n    supports_default_keyword_in_insert = True\n\n    # Does the backend support the DEFAULT keyword in bulk insert queries?\n    supports_default_keyword_in_bulk_insert = True\n\n    # Does the backend require a connection reset after each material schema change?\n    connection_persists_old_columns = False\n\n    # What kind of error does the backend throw when accessing closed cursor?\n    closed_cursor_error_class = ProgrammingError\n\n    # Does 'a' LIKE 'A' match?\n    has_case_insensitive_like = False\n\n    # Suffix for backends that don't support \"SELECT xxx;\" queries.\n    bare_select_suffix = \"\"\n\n    # If NULL is implied on columns without needing to be explicitly specified\n    implied_column_null = False\n\n    # Does the backend support \"select for update\" queries with limit (and offset)?\n    supports_select_for_update_with_limit = True\n\n    # Does the backend ignore null expressions in GREATEST and LEAST queries unless\n    # every expression is null?\n    greatest_least_ignores_nulls = False\n\n    # Can the backend clone databases for parallel test execution?\n    # Defaults to False to allow third-party backends to opt-in.\n    can_clone_databases = False\n\n    # Does the backend consider table names with different casing to\n    # be equal?\n    ignores_table_name_case = False\n\n    # Place FOR UPDATE right after FROM clause. Used on MSSQL.\n    for_update_after_from = False\n\n    # Combinatorial flags\n    supports_select_union = True\n    supports_select_intersection = True\n    supports_select_difference = True\n    supports_slicing_ordering_in_compound = False\n    supports_parentheses_in_compound = True\n    requires_compound_order_by_subquery = False\n\n    # Does the database support SQL 2003 FILTER (WHERE ...) in aggregate\n    # expressions?\n    supports_aggregate_filter_clause = False\n\n    # Does the database support ORDER BY in aggregate expressions?\n    supports_aggregate_order_by_clause = False\n\n    # Does the database backend support DISTINCT when using multiple arguments in an\n    # aggregate expression? For example, Sqlite treats the \"delimiter\" argument of\n    # STRING_AGG/GROUP_CONCAT as an extra argument and does not allow using a custom\n    # delimiter along with DISTINCT.\n    supports_aggregate_distinct_multiple_argument = True\n\n    # Does the backend support indexing a TextField?\n    supports_index_on_text_field = True\n\n    # Does the backend support window expressions (expression OVER (...))?\n    supports_over_clause = False\n    supports_frame_range_fixed_distance = False\n    supports_frame_exclusion = False\n    only_supports_unbounded_with_preceding_and_following = False\n\n    # Does the backend support CAST with precision?\n    supports_cast_with_precision = True\n\n    # How many second decimals does the database return when casting a value to\n    # a type with time?\n    time_cast_precision = 6\n\n    # SQL to create a procedure for use by the Django test suite. The\n    # functionality of the procedure isn't important.\n    create_test_procedure_without_params_sql = None\n    create_test_procedure_with_int_param_sql = None\n\n    # SQL to create a table with a composite primary key for use by the Django\n    # test suite.\n    create_test_table_with_composite_primary_key = None\n\n    # Does the backend support keyword parameters for cursor.callproc()?\n    supports_callproc_kwargs = False\n\n    # What formats does the backend EXPLAIN syntax support?\n    supported_explain_formats = set()\n\n    # Does the backend support the default parameter in lead() and lag()?\n    supports_default_in_lead_lag = True\n\n    # Does the backend support ignoring constraint or uniqueness errors during\n    # INSERT?\n    supports_ignore_conflicts = True\n    # Does the backend support updating rows on constraint or uniqueness errors\n    # during INSERT?\n    supports_update_conflicts = False\n    supports_update_conflicts_with_target = False\n\n    # Does this backend require casting the results of CASE expressions used\n    # in UPDATE statements to ensure the expression has the correct type?\n    requires_casted_case_in_updates = False\n\n    # Does the backend support partial indexes (CREATE INDEX ... WHERE ...)?\n    supports_partial_indexes = True\n    supports_functions_in_partial_indexes = True\n    # Does the backend support covering indexes (CREATE INDEX ... INCLUDE ...)?\n    supports_covering_indexes = False\n    # Does the backend support indexes on expressions?\n    supports_expression_indexes = True\n    # Does the backend treat COLLATE as an indexed expression?\n    collate_as_index_expression = False\n\n    # Does the database allow more than one constraint or index on the same\n    # field(s)?\n    allows_multiple_constraints_on_same_fields = True\n\n    # Does the backend support boolean expressions in SELECT and GROUP BY\n    # clauses?\n    supports_boolean_expr_in_select_clause = True\n    # Does the backend support comparing boolean expressions in WHERE clauses?\n    # Eg: WHERE (price > 0) IS NOT NULL\n    supports_comparing_boolean_expr = True\n\n    # Does the backend support JSONField?\n    supports_json_field = True\n    # Can the backend introspect a JSONField?\n    can_introspect_json_field = True\n    # Does the backend support primitives in JSONField?\n    supports_primitives_in_json_field = True\n    # Is there a true datatype for JSON?\n    has_native_json_field = False\n    # Does the backend use PostgreSQL-style JSON operators like '->'?\n    has_json_operators = False\n    # Does the backend support __contains and __contained_by lookups for\n    # a JSONField?\n    supports_json_field_contains = True\n    # Does value__d__contains={'f': 'g'} (without a list around the dict) match\n    # {'d': [{'f': 'g'}]}?\n    json_key_contains_list_matching_requires_list = False\n    # Does the backend support JSONObject() database function?\n    has_json_object_function = True\n\n    # Does the backend support column collations?\n    supports_collation_on_charfield = True\n    supports_collation_on_textfield = True\n    # Does the backend support non-deterministic collations?\n    supports_non_deterministic_collations = True\n\n    # Does the backend support column and table comments?\n    supports_comments = False\n    # Does the backend support column comments in ADD COLUMN statements?\n    supports_comments_inline = False\n\n    # Does the backend support stored generated columns?\n    supports_stored_generated_columns = False\n    # Does the backend support virtual generated columns?\n    supports_virtual_generated_columns = False\n\n    # Does the backend support the logical XOR operator?\n    supports_logical_xor = False\n\n    # Set to (exception, message) if null characters in text are disallowed.\n    prohibits_null_characters_in_text_exception = None\n\n    # Does the backend support unlimited character columns?\n    supports_unlimited_charfield = False\n\n    # Does the backend support native tuple lookups (=, >, <, IN)?\n    supports_tuple_lookups = True\n\n    # Collation names for use by the Django test suite.\n    test_collations = {\n        \"ci\": None,  # Case-insensitive.\n        \"cs\": None,  # Case-sensitive.\n        \"non_default\": None,  # Non-default.\n        \"swedish_ci\": None,  # Swedish case-insensitive.\n        \"virtual\": None,  # A collation that can be used for virtual columns.\n    }\n    # SQL template override for tests.aggregation.tests.NowUTC\n    test_now_utc_template = None\n\n    # SQL to create a model instance using the database defaults.\n    insert_test_table_with_defaults = None\n\n    # Does the Round() database function round to even?\n    rounds_to_even = False\n\n    # A set of dotted paths to tests in Django's test suite that are expected\n    # to fail on this database.\n    django_test_expected_failures = set()\n    # A map of reasons to sets of dotted paths to tests in Django's test suite\n    # that should be skipped for this database.\n    django_test_skips = {}\n\n    def __init__(self, connection):\n        self.connection = connection\n\n    @cached_property\n    def supports_explaining_query_execution(self):\n        \"\"\"Does this backend support explaining query execution?\"\"\"\n        return self.connection.ops.explain_prefix is not None\n\n    @cached_property\n    def supports_transactions(self):\n        \"\"\"Confirm support for transactions.\"\"\"\n        with self.connection.cursor() as cursor:\n            cursor.execute(\"CREATE TABLE ROLLBACK_TEST (X INT)\")\n            self.connection.set_autocommit(False)\n            cursor.execute(\"INSERT INTO ROLLBACK_TEST (X) VALUES (8)\")\n            self.connection.rollback()\n            self.connection.set_autocommit(True)\n            cursor.execute(\"SELECT COUNT(X) FROM ROLLBACK_TEST\")\n            (count,) = cursor.fetchone()\n            cursor.execute(\"DROP TABLE ROLLBACK_TEST\")\n        return count == 0\n\n    def allows_group_by_selected_pks_on_model(self, model):\n        if not self.allows_group_by_selected_pks:\n            return False\n        return model._meta.managed"}],"ground_truth":null,"score":null},
{"question":"How does the class `Reference` handle `Rename all references to the old_column to the new_column`?","answer":"","relative_code_list":[{"start_line":36,"end_line":40,"belongs_to":{"file_name":"ddl_references.py","upper_path":"../django/django/db/backends","module":"backends","define_class":["Reference","Table","TableColumns","Columns","IndexName","IndexColumns","ForeignKeyName","Statement","Expressions"],"imports":["copy.deepcopy"]},"relative_function":[],"code":"def rename_column_references(self, table, old_column, new_column):\n        \"\"\"\n        Rename all references to the old_column to the new_column.\n        \"\"\"\n        pass"},{"start_line":9,"end_line":48,"belongs_to":{"file_name":"ddl_references.py","upper_path":"../django/django/db/backends","module":"backends","define_class":["Reference","Table","TableColumns","Columns","IndexName","IndexColumns","ForeignKeyName","Statement","Expressions"],"imports":["copy.deepcopy"]},"relative_function":[],"code":"class Reference:\n    \"\"\"Base class that defines the reference interface.\"\"\"\n\n    def references_table(self, table):\n        \"\"\"\n        Return whether or not this instance references the specified table.\n        \"\"\"\n        return False\n\n    def references_column(self, table, column):\n        \"\"\"\n        Return whether or not this instance references the specified column.\n        \"\"\"\n        return False\n\n    def references_index(self, table, index):\n        \"\"\"\n        Return whether or not this instance references the specified index.\n        \"\"\"\n        return False\n\n    def rename_table_references(self, old_table, new_table):\n        \"\"\"\n        Rename all references to the old_name to the new_table.\n        \"\"\"\n        pass\n\n    def rename_column_references(self, table, old_column, new_column):\n        \"\"\"\n        Rename all references to the old_column to the new_column.\n        \"\"\"\n        pass\n\n    def __repr__(self):\n        return \"<%s %r>\" % (self.__class__.__name__, str(self))\n\n    def __str__(self):\n        raise NotImplementedError(\n            \"Subclasses must define how they should be converted to string.\"\n        )"}],"ground_truth":null,"score":null},
{"question":"What is the purpose of the `explain_prefix` attribute in the `DatabaseOperations`, and how does it affect other features?","answer":"","relative_code_list":[{"start_line":27,"end_line":422,"belongs_to":{"file_name":"operations.py","upper_path":"../django/django/db/backends/postgresql","module":"postgresql","define_class":["DatabaseOperations"],"imports":["json","functools.lru_cache","functools.partial","django.conf.settings","django.db.backends.base.operations.BaseDatabaseOperations","django.db.backends.postgresql.compiler.InsertUnnest","django.db.backends.postgresql.psycopg_any.Inet","django.db.backends.postgresql.psycopg_any.Jsonb","django.db.backends.postgresql.psycopg_any.errors","django.db.backends.postgresql.psycopg_any.is_psycopg3","django.db.backends.postgresql.psycopg_any.mogrify","django.db.backends.utils.split_tzname_delta","django.db.models.constants.OnConflict","django.db.models.functions.Cast","django.utils.regex_helper._lazy_re_compile","psycopg.types.numeric","django.db.models"]},"relative_function":[],"code":"class DatabaseOperations(BaseDatabaseOperations):\n    compiler_module = \"django.db.backends.postgresql.compiler\"\n    cast_char_field_without_max_length = \"varchar\"\n    explain_prefix = \"EXPLAIN\"\n    explain_options = frozenset(\n        [\n            \"ANALYZE\",\n            \"BUFFERS\",\n            \"COSTS\",\n            \"GENERIC_PLAN\",\n            \"MEMORY\",\n            \"SETTINGS\",\n            \"SERIALIZE\",\n            \"SUMMARY\",\n            \"TIMING\",\n            \"VERBOSE\",\n            \"WAL\",\n        ]\n    )\n    cast_data_types = {\n        \"AutoField\": \"integer\",\n        \"BigAutoField\": \"bigint\",\n        \"SmallAutoField\": \"smallint\",\n    }\n\n    if is_psycopg3:\n        from psycopg.types import numeric\n\n        integerfield_type_map = {\n            \"SmallIntegerField\": numeric.Int2,\n            \"IntegerField\": numeric.Int4,\n            \"BigIntegerField\": numeric.Int8,\n            \"PositiveSmallIntegerField\": numeric.Int2,\n            \"PositiveIntegerField\": numeric.Int4,\n            \"PositiveBigIntegerField\": numeric.Int8,\n        }\n\n    def unification_cast_sql(self, output_field):\n        internal_type = output_field.get_internal_type()\n        if internal_type in (\n            \"GenericIPAddressField\",\n            \"IPAddressField\",\n            \"TimeField\",\n            \"UUIDField\",\n        ):\n            # PostgreSQL will resolve a union as type 'text' if input types are\n            # 'unknown'.\n            # https://www.postgresql.org/docs/current/typeconv-union-case.html\n            # These fields cannot be implicitly cast back in the default\n            # PostgreSQL configuration so we need to explicitly cast them.\n            # We must also remove components of the type within brackets:\n            # varchar(255) -> varchar.\n            return (\n                \"CAST(%%s AS %s)\" % output_field.db_type(self.connection).split(\"(\")[0]\n            )\n        return \"%s\"\n\n    # EXTRACT format cannot be passed in parameters.\n    _extract_format_re = _lazy_re_compile(r\"[A-Z_]+\")\n\n    def date_extract_sql(self, lookup_type, sql, params):\n        # https://www.postgresql.org/docs/current/functions-datetime.html#FUNCTIONS-DATETIME-EXTRACT\n        if lookup_type == \"week_day\":\n            # For consistency across backends, we return Sunday=1, Saturday=7.\n            return f\"EXTRACT(DOW FROM {sql}) + 1\", params\n        elif lookup_type == \"iso_week_day\":\n            return f\"EXTRACT(ISODOW FROM {sql})\", params\n        elif lookup_type == \"iso_year\":\n            return f\"EXTRACT(ISOYEAR FROM {sql})\", params\n\n        lookup_type = lookup_type.upper()\n        if not self._extract_format_re.fullmatch(lookup_type):\n            raise ValueError(f\"Invalid lookup type: {lookup_type!r}\")\n        return f\"EXTRACT({lookup_type} FROM {sql})\", params\n\n    def date_trunc_sql(self, lookup_type, sql, params, tzname=None):\n        sql, params = self._convert_sql_to_tz(sql, params, tzname)\n        # https://www.postgresql.org/docs/current/functions-datetime.html#FUNCTIONS-DATETIME-TRUNC\n        return f\"DATE_TRUNC(%s, {sql})\", (lookup_type, *params)\n\n    def _prepare_tzname_delta(self, tzname):\n        tzname, sign, offset = split_tzname_delta(tzname)\n        if offset:\n            sign = \"-\" if sign == \"+\" else \"+\"\n            return f\"{tzname}{sign}{offset}\"\n        return tzname\n\n    def _convert_sql_to_tz(self, sql, params, tzname):\n        if tzname and settings.USE_TZ:\n            tzname_param = self._prepare_tzname_delta(tzname)\n            return f\"{sql} AT TIME ZONE %s\", (*params, tzname_param)\n        return sql, params\n\n    def datetime_cast_date_sql(self, sql, params, tzname):\n        sql, params = self._convert_sql_to_tz(sql, params, tzname)\n        return f\"({sql})::date\", params\n\n    def datetime_cast_time_sql(self, sql, params, tzname):\n        sql, params = self._convert_sql_to_tz(sql, params, tzname)\n        return f\"({sql})::time\", params\n\n    def datetime_extract_sql(self, lookup_type, sql, params, tzname):\n        sql, params = self._convert_sql_to_tz(sql, params, tzname)\n        if lookup_type == \"second\":\n            # Truncate fractional seconds.\n            return f\"EXTRACT(SECOND FROM DATE_TRUNC(%s, {sql}))\", (\"second\", *params)\n        return self.date_extract_sql(lookup_type, sql, params)\n\n    def datetime_trunc_sql(self, lookup_type, sql, params, tzname):\n        sql, params = self._convert_sql_to_tz(sql, params, tzname)\n        # https://www.postgresql.org/docs/current/functions-datetime.html#FUNCTIONS-DATETIME-TRUNC\n        return f\"DATE_TRUNC(%s, {sql})\", (lookup_type, *params)\n\n    def time_extract_sql(self, lookup_type, sql, params):\n        if lookup_type == \"second\":\n            # Truncate fractional seconds.\n            return f\"EXTRACT(SECOND FROM DATE_TRUNC(%s, {sql}))\", (\"second\", *params)\n        return self.date_extract_sql(lookup_type, sql, params)\n\n    def time_trunc_sql(self, lookup_type, sql, params, tzname=None):\n        sql, params = self._convert_sql_to_tz(sql, params, tzname)\n        return f\"DATE_TRUNC(%s, {sql})::time\", (lookup_type, *params)\n\n    def deferrable_sql(self):\n        return \" DEFERRABLE INITIALLY DEFERRED\"\n\n    def bulk_insert_sql(self, fields, placeholder_rows):\n        if isinstance(placeholder_rows, InsertUnnest):\n            return f\"SELECT * FROM {placeholder_rows}\"\n        return super().bulk_insert_sql(fields, placeholder_rows)\n\n    def fetch_returned_insert_rows(self, cursor):\n        \"\"\"\n        Given a cursor object that has just performed an INSERT...RETURNING\n        statement into a table, return the tuple of returned data.\n        \"\"\"\n        return cursor.fetchall()\n\n    def lookup_cast(self, lookup_type, internal_type=None):\n        lookup = \"%s\"\n        # Cast text lookups to text to allow things like filter(x__contains=4)\n        if lookup_type in (\n            \"iexact\",\n            \"contains\",\n            \"icontains\",\n            \"startswith\",\n            \"istartswith\",\n            \"endswith\",\n            \"iendswith\",\n            \"regex\",\n            \"iregex\",\n        ):\n            if internal_type in (\"IPAddressField\", \"GenericIPAddressField\"):\n                lookup = \"HOST(%s)\"\n            else:\n                lookup = \"%s::text\"\n\n        # Use UPPER(x) for case-insensitive lookups; it's faster.\n        if lookup_type in (\"iexact\", \"icontains\", \"istartswith\", \"iendswith\"):\n            lookup = \"UPPER(%s)\" % lookup\n\n        return lookup\n\n    def no_limit_value(self):\n        return None\n\n    def prepare_sql_script(self, sql):\n        return [sql]\n\n    def quote_name(self, name):\n        if name.startswith('\"') and name.endswith('\"'):\n            return name  # Quoting once is enough.\n        return '\"%s\"' % name\n\n    def compose_sql(self, sql, params):\n        return mogrify(sql, params, self.connection)\n\n    def set_time_zone_sql(self):\n        return \"SELECT set_config('TimeZone', %s, false)\"\n\n    def sql_flush(self, style, tables, *, reset_sequences=False, allow_cascade=False):\n        if not tables:\n            return []\n\n        # Perform a single SQL 'TRUNCATE x, y, z...;' statement. It allows us\n        # to truncate tables referenced by a foreign key in any other table.\n        sql_parts = [\n            style.SQL_KEYWORD(\"TRUNCATE\"),\n            \", \".join(style.SQL_FIELD(self.quote_name(table)) for table in tables),\n        ]\n        if reset_sequences:\n            sql_parts.append(style.SQL_KEYWORD(\"RESTART IDENTITY\"))\n        if allow_cascade:\n            sql_parts.append(style.SQL_KEYWORD(\"CASCADE\"))\n        return [\"%s;\" % \" \".join(sql_parts)]\n\n    def sequence_reset_by_name_sql(self, style, sequences):\n        # 'ALTER SEQUENCE sequence_name RESTART WITH 1;'... style SQL statements\n        # to reset sequence indices\n        sql = []\n        for sequence_info in sequences:\n            table_name = sequence_info[\"table\"]\n            # 'id' will be the case if it's an m2m using an autogenerated\n            # intermediate table (see BaseDatabaseIntrospection.sequence_list).\n            column_name = sequence_info[\"column\"] or \"id\"\n            sql.append(\n                \"%s setval(pg_get_serial_sequence('%s','%s'), 1, false);\"\n                % (\n                    style.SQL_KEYWORD(\"SELECT\"),\n                    style.SQL_TABLE(self.quote_name(table_name)),\n                    style.SQL_FIELD(column_name),\n                )\n            )\n        return sql\n\n    def tablespace_sql(self, tablespace, inline=False):\n        if inline:\n            return \"USING INDEX TABLESPACE %s\" % self.quote_name(tablespace)\n        else:\n            return \"TABLESPACE %s\" % self.quote_name(tablespace)\n\n    def sequence_reset_sql(self, style, model_list):\n        from django.db import models\n\n        output = []\n        qn = self.quote_name\n        for model in model_list:\n            # Use `coalesce` to set the sequence for each model to the max pk\n            # value if there are records, or 1 if there are none. Set the\n            # `is_called` property (the third argument to `setval`) to true if\n            # there are records (as the max pk value is already in use),\n            # otherwise set it to false. Use pg_get_serial_sequence to get the\n            # underlying sequence name from the table name and column name.\n\n            for f in model._meta.local_fields:\n                if isinstance(f, models.AutoField):\n                    output.append(\n                        \"%s setval(pg_get_serial_sequence('%s','%s'), \"\n                        \"coalesce(max(%s), 1), max(%s) %s null) %s %s;\"\n                        % (\n                            style.SQL_KEYWORD(\"SELECT\"),\n                            style.SQL_TABLE(qn(model._meta.db_table)),\n                            style.SQL_FIELD(f.column),\n                            style.SQL_FIELD(qn(f.column)),\n                            style.SQL_FIELD(qn(f.column)),\n                            style.SQL_KEYWORD(\"IS NOT\"),\n                            style.SQL_KEYWORD(\"FROM\"),\n                            style.SQL_TABLE(qn(model._meta.db_table)),\n                        )\n                    )\n                    # Only one AutoField is allowed per model, so don't bother\n                    # continuing.\n                    break\n        return output\n\n    def prep_for_iexact_query(self, x):\n        return x\n\n    def max_name_length(self):\n        \"\"\"\n        Return the maximum length of an identifier.\n\n        The maximum length of an identifier is 63 by default, but can be\n        changed by recompiling PostgreSQL after editing the NAMEDATALEN\n        macro in src/include/pg_config_manual.h.\n\n        This implementation returns 63, but can be overridden by a custom\n        database backend that inherits most of its behavior from this one.\n        \"\"\"\n        return 63\n\n    def distinct_sql(self, fields, params):\n        if fields:\n            params = [param for param_list in params for param in param_list]\n            return ([\"DISTINCT ON (%s)\" % \", \".join(fields)], params)\n        else:\n            return [\"DISTINCT\"], []\n\n    if is_psycopg3:\n\n        def last_executed_query(self, cursor, sql, params):\n            if self.connection.features.uses_server_side_binding:\n                try:\n                    return self.compose_sql(sql, params)\n                except errors.DataError:\n                    return None\n            else:\n                if cursor._query and cursor._query.query is not None:\n                    return cursor._query.query.decode()\n                return None\n\n    else:\n\n        def last_executed_query(self, cursor, sql, params):\n            # https://www.psycopg.org/docs/cursor.html#cursor.query\n            # The query attribute is a Psycopg extension to the DB API 2.0.\n            if cursor.query is not None:\n                return cursor.query.decode()\n            return None\n\n    def return_insert_columns(self, fields):\n        if not fields:\n            return \"\", ()\n        columns = [\n            \"%s.%s\"\n            % (\n                self.quote_name(field.model._meta.db_table),\n                self.quote_name(field.column),\n            )\n            for field in fields\n        ]\n        return \"RETURNING %s\" % \", \".join(columns), ()\n\n    if is_psycopg3:\n\n        def adapt_integerfield_value(self, value, internal_type):\n            if value is None or hasattr(value, \"resolve_expression\"):\n                return value\n            return self.integerfield_type_map[internal_type](value)\n\n    def adapt_datefield_value(self, value):\n        return value\n\n    def adapt_datetimefield_value(self, value):\n        return value\n\n    def adapt_timefield_value(self, value):\n        return value\n\n    def adapt_ipaddressfield_value(self, value):\n        if value:\n            return Inet(value)\n        return None\n\n    def adapt_json_value(self, value, encoder):\n        return Jsonb(value, dumps=get_json_dumps(encoder))\n\n    def subtract_temporals(self, internal_type, lhs, rhs):\n        if internal_type == \"DateField\":\n            lhs_sql, lhs_params = lhs\n            rhs_sql, rhs_params = rhs\n            params = (*lhs_params, *rhs_params)\n            return \"(interval '1 day' * (%s - %s))\" % (lhs_sql, rhs_sql), params\n        return super().subtract_temporals(internal_type, lhs, rhs)\n\n    def explain_query_prefix(self, format=None, **options):\n        extra = {}\n        if serialize := options.pop(\"serialize\", None):\n            if serialize.upper() in {\"TEXT\", \"BINARY\"}:\n                extra[\"SERIALIZE\"] = serialize.upper()\n        # Normalize options.\n        if options:\n            options = {\n                name.upper(): \"true\" if value else \"false\"\n                for name, value in options.items()\n            }\n            for valid_option in self.explain_options:\n                value = options.pop(valid_option, None)\n                if value is not None:\n                    extra[valid_option] = value\n        prefix = super().explain_query_prefix(format, **options)\n        if format:\n            extra[\"FORMAT\"] = format\n        if extra:\n            prefix += \" (%s)\" % \", \".join(\"%s %s\" % i for i in extra.items())\n        return prefix\n\n    def on_conflict_suffix_sql(self, fields, on_conflict, update_fields, unique_fields):\n        if on_conflict == OnConflict.IGNORE:\n            return \"ON CONFLICT DO NOTHING\"\n        if on_conflict == OnConflict.UPDATE:\n            return \"ON CONFLICT(%s) DO UPDATE SET %s\" % (\n                \", \".join(map(self.quote_name, unique_fields)),\n                \", \".join(\n                    [\n                        f\"{field} = EXCLUDED.{field}\"\n                        for field in map(self.quote_name, update_fields)\n                    ]\n                ),\n            )\n        return super().on_conflict_suffix_sql(\n            fields,\n            on_conflict,\n            update_fields,\n            unique_fields,\n        )\n\n    def prepare_join_on_clause(self, lhs_table, lhs_field, rhs_table, rhs_field):\n        lhs_expr, rhs_expr = super().prepare_join_on_clause(\n            lhs_table, lhs_field, rhs_table, rhs_field\n        )\n\n        if lhs_field.db_type(self.connection) != rhs_field.db_type(self.connection):\n            rhs_expr = Cast(rhs_expr, lhs_field)\n\n        return lhs_expr, rhs_expr"}],"ground_truth":null,"score":null},
{"question":"How is the `format_for_duration_arithmetic` currently being used in the `DatabaseOperations`?","answer":"","relative_code_list":[{"start_line":27,"end_line":422,"belongs_to":{"file_name":"operations.py","upper_path":"../django/django/db/backends/postgresql","module":"postgresql","define_class":["DatabaseOperations"],"imports":["json","functools.lru_cache","functools.partial","django.conf.settings","django.db.backends.base.operations.BaseDatabaseOperations","django.db.backends.postgresql.compiler.InsertUnnest","django.db.backends.postgresql.psycopg_any.Inet","django.db.backends.postgresql.psycopg_any.Jsonb","django.db.backends.postgresql.psycopg_any.errors","django.db.backends.postgresql.psycopg_any.is_psycopg3","django.db.backends.postgresql.psycopg_any.mogrify","django.db.backends.utils.split_tzname_delta","django.db.models.constants.OnConflict","django.db.models.functions.Cast","django.utils.regex_helper._lazy_re_compile","psycopg.types.numeric","django.db.models"]},"relative_function":[],"code":"class DatabaseOperations(BaseDatabaseOperations):\n    compiler_module = \"django.db.backends.postgresql.compiler\"\n    cast_char_field_without_max_length = \"varchar\"\n    explain_prefix = \"EXPLAIN\"\n    explain_options = frozenset(\n        [\n            \"ANALYZE\",\n            \"BUFFERS\",\n            \"COSTS\",\n            \"GENERIC_PLAN\",\n            \"MEMORY\",\n            \"SETTINGS\",\n            \"SERIALIZE\",\n            \"SUMMARY\",\n            \"TIMING\",\n            \"VERBOSE\",\n            \"WAL\",\n        ]\n    )\n    cast_data_types = {\n        \"AutoField\": \"integer\",\n        \"BigAutoField\": \"bigint\",\n        \"SmallAutoField\": \"smallint\",\n    }\n\n    if is_psycopg3:\n        from psycopg.types import numeric\n\n        integerfield_type_map = {\n            \"SmallIntegerField\": numeric.Int2,\n            \"IntegerField\": numeric.Int4,\n            \"BigIntegerField\": numeric.Int8,\n            \"PositiveSmallIntegerField\": numeric.Int2,\n            \"PositiveIntegerField\": numeric.Int4,\n            \"PositiveBigIntegerField\": numeric.Int8,\n        }\n\n    def unification_cast_sql(self, output_field):\n        internal_type = output_field.get_internal_type()\n        if internal_type in (\n            \"GenericIPAddressField\",\n            \"IPAddressField\",\n            \"TimeField\",\n            \"UUIDField\",\n        ):\n            # PostgreSQL will resolve a union as type 'text' if input types are\n            # 'unknown'.\n            # https://www.postgresql.org/docs/current/typeconv-union-case.html\n            # These fields cannot be implicitly cast back in the default\n            # PostgreSQL configuration so we need to explicitly cast them.\n            # We must also remove components of the type within brackets:\n            # varchar(255) -> varchar.\n            return (\n                \"CAST(%%s AS %s)\" % output_field.db_type(self.connection).split(\"(\")[0]\n            )\n        return \"%s\"\n\n    # EXTRACT format cannot be passed in parameters.\n    _extract_format_re = _lazy_re_compile(r\"[A-Z_]+\")\n\n    def date_extract_sql(self, lookup_type, sql, params):\n        # https://www.postgresql.org/docs/current/functions-datetime.html#FUNCTIONS-DATETIME-EXTRACT\n        if lookup_type == \"week_day\":\n            # For consistency across backends, we return Sunday=1, Saturday=7.\n            return f\"EXTRACT(DOW FROM {sql}) + 1\", params\n        elif lookup_type == \"iso_week_day\":\n            return f\"EXTRACT(ISODOW FROM {sql})\", params\n        elif lookup_type == \"iso_year\":\n            return f\"EXTRACT(ISOYEAR FROM {sql})\", params\n\n        lookup_type = lookup_type.upper()\n        if not self._extract_format_re.fullmatch(lookup_type):\n            raise ValueError(f\"Invalid lookup type: {lookup_type!r}\")\n        return f\"EXTRACT({lookup_type} FROM {sql})\", params\n\n    def date_trunc_sql(self, lookup_type, sql, params, tzname=None):\n        sql, params = self._convert_sql_to_tz(sql, params, tzname)\n        # https://www.postgresql.org/docs/current/functions-datetime.html#FUNCTIONS-DATETIME-TRUNC\n        return f\"DATE_TRUNC(%s, {sql})\", (lookup_type, *params)\n\n    def _prepare_tzname_delta(self, tzname):\n        tzname, sign, offset = split_tzname_delta(tzname)\n        if offset:\n            sign = \"-\" if sign == \"+\" else \"+\"\n            return f\"{tzname}{sign}{offset}\"\n        return tzname\n\n    def _convert_sql_to_tz(self, sql, params, tzname):\n        if tzname and settings.USE_TZ:\n            tzname_param = self._prepare_tzname_delta(tzname)\n            return f\"{sql} AT TIME ZONE %s\", (*params, tzname_param)\n        return sql, params\n\n    def datetime_cast_date_sql(self, sql, params, tzname):\n        sql, params = self._convert_sql_to_tz(sql, params, tzname)\n        return f\"({sql})::date\", params\n\n    def datetime_cast_time_sql(self, sql, params, tzname):\n        sql, params = self._convert_sql_to_tz(sql, params, tzname)\n        return f\"({sql})::time\", params\n\n    def datetime_extract_sql(self, lookup_type, sql, params, tzname):\n        sql, params = self._convert_sql_to_tz(sql, params, tzname)\n        if lookup_type == \"second\":\n            # Truncate fractional seconds.\n            return f\"EXTRACT(SECOND FROM DATE_TRUNC(%s, {sql}))\", (\"second\", *params)\n        return self.date_extract_sql(lookup_type, sql, params)\n\n    def datetime_trunc_sql(self, lookup_type, sql, params, tzname):\n        sql, params = self._convert_sql_to_tz(sql, params, tzname)\n        # https://www.postgresql.org/docs/current/functions-datetime.html#FUNCTIONS-DATETIME-TRUNC\n        return f\"DATE_TRUNC(%s, {sql})\", (lookup_type, *params)\n\n    def time_extract_sql(self, lookup_type, sql, params):\n        if lookup_type == \"second\":\n            # Truncate fractional seconds.\n            return f\"EXTRACT(SECOND FROM DATE_TRUNC(%s, {sql}))\", (\"second\", *params)\n        return self.date_extract_sql(lookup_type, sql, params)\n\n    def time_trunc_sql(self, lookup_type, sql, params, tzname=None):\n        sql, params = self._convert_sql_to_tz(sql, params, tzname)\n        return f\"DATE_TRUNC(%s, {sql})::time\", (lookup_type, *params)\n\n    def deferrable_sql(self):\n        return \" DEFERRABLE INITIALLY DEFERRED\"\n\n    def bulk_insert_sql(self, fields, placeholder_rows):\n        if isinstance(placeholder_rows, InsertUnnest):\n            return f\"SELECT * FROM {placeholder_rows}\"\n        return super().bulk_insert_sql(fields, placeholder_rows)\n\n    def fetch_returned_insert_rows(self, cursor):\n        \"\"\"\n        Given a cursor object that has just performed an INSERT...RETURNING\n        statement into a table, return the tuple of returned data.\n        \"\"\"\n        return cursor.fetchall()\n\n    def lookup_cast(self, lookup_type, internal_type=None):\n        lookup = \"%s\"\n        # Cast text lookups to text to allow things like filter(x__contains=4)\n        if lookup_type in (\n            \"iexact\",\n            \"contains\",\n            \"icontains\",\n            \"startswith\",\n            \"istartswith\",\n            \"endswith\",\n            \"iendswith\",\n            \"regex\",\n            \"iregex\",\n        ):\n            if internal_type in (\"IPAddressField\", \"GenericIPAddressField\"):\n                lookup = \"HOST(%s)\"\n            else:\n                lookup = \"%s::text\"\n\n        # Use UPPER(x) for case-insensitive lookups; it's faster.\n        if lookup_type in (\"iexact\", \"icontains\", \"istartswith\", \"iendswith\"):\n            lookup = \"UPPER(%s)\" % lookup\n\n        return lookup\n\n    def no_limit_value(self):\n        return None\n\n    def prepare_sql_script(self, sql):\n        return [sql]\n\n    def quote_name(self, name):\n        if name.startswith('\"') and name.endswith('\"'):\n            return name  # Quoting once is enough.\n        return '\"%s\"' % name\n\n    def compose_sql(self, sql, params):\n        return mogrify(sql, params, self.connection)\n\n    def set_time_zone_sql(self):\n        return \"SELECT set_config('TimeZone', %s, false)\"\n\n    def sql_flush(self, style, tables, *, reset_sequences=False, allow_cascade=False):\n        if not tables:\n            return []\n\n        # Perform a single SQL 'TRUNCATE x, y, z...;' statement. It allows us\n        # to truncate tables referenced by a foreign key in any other table.\n        sql_parts = [\n            style.SQL_KEYWORD(\"TRUNCATE\"),\n            \", \".join(style.SQL_FIELD(self.quote_name(table)) for table in tables),\n        ]\n        if reset_sequences:\n            sql_parts.append(style.SQL_KEYWORD(\"RESTART IDENTITY\"))\n        if allow_cascade:\n            sql_parts.append(style.SQL_KEYWORD(\"CASCADE\"))\n        return [\"%s;\" % \" \".join(sql_parts)]\n\n    def sequence_reset_by_name_sql(self, style, sequences):\n        # 'ALTER SEQUENCE sequence_name RESTART WITH 1;'... style SQL statements\n        # to reset sequence indices\n        sql = []\n        for sequence_info in sequences:\n            table_name = sequence_info[\"table\"]\n            # 'id' will be the case if it's an m2m using an autogenerated\n            # intermediate table (see BaseDatabaseIntrospection.sequence_list).\n            column_name = sequence_info[\"column\"] or \"id\"\n            sql.append(\n                \"%s setval(pg_get_serial_sequence('%s','%s'), 1, false);\"\n                % (\n                    style.SQL_KEYWORD(\"SELECT\"),\n                    style.SQL_TABLE(self.quote_name(table_name)),\n                    style.SQL_FIELD(column_name),\n                )\n            )\n        return sql\n\n    def tablespace_sql(self, tablespace, inline=False):\n        if inline:\n            return \"USING INDEX TABLESPACE %s\" % self.quote_name(tablespace)\n        else:\n            return \"TABLESPACE %s\" % self.quote_name(tablespace)\n\n    def sequence_reset_sql(self, style, model_list):\n        from django.db import models\n\n        output = []\n        qn = self.quote_name\n        for model in model_list:\n            # Use `coalesce` to set the sequence for each model to the max pk\n            # value if there are records, or 1 if there are none. Set the\n            # `is_called` property (the third argument to `setval`) to true if\n            # there are records (as the max pk value is already in use),\n            # otherwise set it to false. Use pg_get_serial_sequence to get the\n            # underlying sequence name from the table name and column name.\n\n            for f in model._meta.local_fields:\n                if isinstance(f, models.AutoField):\n                    output.append(\n                        \"%s setval(pg_get_serial_sequence('%s','%s'), \"\n                        \"coalesce(max(%s), 1), max(%s) %s null) %s %s;\"\n                        % (\n                            style.SQL_KEYWORD(\"SELECT\"),\n                            style.SQL_TABLE(qn(model._meta.db_table)),\n                            style.SQL_FIELD(f.column),\n                            style.SQL_FIELD(qn(f.column)),\n                            style.SQL_FIELD(qn(f.column)),\n                            style.SQL_KEYWORD(\"IS NOT\"),\n                            style.SQL_KEYWORD(\"FROM\"),\n                            style.SQL_TABLE(qn(model._meta.db_table)),\n                        )\n                    )\n                    # Only one AutoField is allowed per model, so don't bother\n                    # continuing.\n                    break\n        return output\n\n    def prep_for_iexact_query(self, x):\n        return x\n\n    def max_name_length(self):\n        \"\"\"\n        Return the maximum length of an identifier.\n\n        The maximum length of an identifier is 63 by default, but can be\n        changed by recompiling PostgreSQL after editing the NAMEDATALEN\n        macro in src/include/pg_config_manual.h.\n\n        This implementation returns 63, but can be overridden by a custom\n        database backend that inherits most of its behavior from this one.\n        \"\"\"\n        return 63\n\n    def distinct_sql(self, fields, params):\n        if fields:\n            params = [param for param_list in params for param in param_list]\n            return ([\"DISTINCT ON (%s)\" % \", \".join(fields)], params)\n        else:\n            return [\"DISTINCT\"], []\n\n    if is_psycopg3:\n\n        def last_executed_query(self, cursor, sql, params):\n            if self.connection.features.uses_server_side_binding:\n                try:\n                    return self.compose_sql(sql, params)\n                except errors.DataError:\n                    return None\n            else:\n                if cursor._query and cursor._query.query is not None:\n                    return cursor._query.query.decode()\n                return None\n\n    else:\n\n        def last_executed_query(self, cursor, sql, params):\n            # https://www.psycopg.org/docs/cursor.html#cursor.query\n            # The query attribute is a Psycopg extension to the DB API 2.0.\n            if cursor.query is not None:\n                return cursor.query.decode()\n            return None\n\n    def return_insert_columns(self, fields):\n        if not fields:\n            return \"\", ()\n        columns = [\n            \"%s.%s\"\n            % (\n                self.quote_name(field.model._meta.db_table),\n                self.quote_name(field.column),\n            )\n            for field in fields\n        ]\n        return \"RETURNING %s\" % \", \".join(columns), ()\n\n    if is_psycopg3:\n\n        def adapt_integerfield_value(self, value, internal_type):\n            if value is None or hasattr(value, \"resolve_expression\"):\n                return value\n            return self.integerfield_type_map[internal_type](value)\n\n    def adapt_datefield_value(self, value):\n        return value\n\n    def adapt_datetimefield_value(self, value):\n        return value\n\n    def adapt_timefield_value(self, value):\n        return value\n\n    def adapt_ipaddressfield_value(self, value):\n        if value:\n            return Inet(value)\n        return None\n\n    def adapt_json_value(self, value, encoder):\n        return Jsonb(value, dumps=get_json_dumps(encoder))\n\n    def subtract_temporals(self, internal_type, lhs, rhs):\n        if internal_type == \"DateField\":\n            lhs_sql, lhs_params = lhs\n            rhs_sql, rhs_params = rhs\n            params = (*lhs_params, *rhs_params)\n            return \"(interval '1 day' * (%s - %s))\" % (lhs_sql, rhs_sql), params\n        return super().subtract_temporals(internal_type, lhs, rhs)\n\n    def explain_query_prefix(self, format=None, **options):\n        extra = {}\n        if serialize := options.pop(\"serialize\", None):\n            if serialize.upper() in {\"TEXT\", \"BINARY\"}:\n                extra[\"SERIALIZE\"] = serialize.upper()\n        # Normalize options.\n        if options:\n            options = {\n                name.upper(): \"true\" if value else \"false\"\n                for name, value in options.items()\n            }\n            for valid_option in self.explain_options:\n                value = options.pop(valid_option, None)\n                if value is not None:\n                    extra[valid_option] = value\n        prefix = super().explain_query_prefix(format, **options)\n        if format:\n            extra[\"FORMAT\"] = format\n        if extra:\n            prefix += \" (%s)\" % \", \".join(\"%s %s\" % i for i in extra.items())\n        return prefix\n\n    def on_conflict_suffix_sql(self, fields, on_conflict, update_fields, unique_fields):\n        if on_conflict == OnConflict.IGNORE:\n            return \"ON CONFLICT DO NOTHING\"\n        if on_conflict == OnConflict.UPDATE:\n            return \"ON CONFLICT(%s) DO UPDATE SET %s\" % (\n                \", \".join(map(self.quote_name, unique_fields)),\n                \", \".join(\n                    [\n                        f\"{field} = EXCLUDED.{field}\"\n                        for field in map(self.quote_name, update_fields)\n                    ]\n                ),\n            )\n        return super().on_conflict_suffix_sql(\n            fields,\n            on_conflict,\n            update_fields,\n            unique_fields,\n        )\n\n    def prepare_join_on_clause(self, lhs_table, lhs_field, rhs_table, rhs_field):\n        lhs_expr, rhs_expr = super().prepare_join_on_clause(\n            lhs_table, lhs_field, rhs_table, rhs_field\n        )\n\n        if lhs_field.db_type(self.connection) != rhs_field.db_type(self.connection):\n            rhs_expr = Cast(rhs_expr, lhs_field)\n\n        return lhs_expr, rhs_expr"},{"start_line":99,"end_line":101,"belongs_to":{"file_name":"operations.py","upper_path":"../django/django/db/backends/sqlite3","module":"sqlite3","define_class":["DatabaseOperations"],"imports":["datetime","decimal","uuid","functools.lru_cache","itertools.chain","django.conf.settings","django.core.exceptions.FieldError","django.db.DatabaseError","django.db.NotSupportedError","django.db.models","django.db.backends.base.operations.BaseDatabaseOperations","django.db.models.constants.OnConflict","django.db.models.expressions.Col","django.utils.timezone","django.utils.dateparse.parse_date","django.utils.dateparse.parse_datetime","django.utils.dateparse.parse_time","django.utils.functional.cached_property","base.Database"]},"relative_function":[],"code":"def format_for_duration_arithmetic(self, sql):\n        \"\"\"Do nothing since formatting is handled in the custom function.\"\"\"\n        return sql"}],"ground_truth":null,"score":null},
{"question":"What is the purpose of the `has_native_uuid_field` attribute in the `DatabaseFeatures`, and how is it typically used?","answer":"","relative_code_list":[{"start_line":9,"end_line":171,"belongs_to":{"file_name":"features.py","upper_path":"../django/django/db/backends/postgresql","module":"postgresql","define_class":["DatabaseFeatures"],"imports":["operator","django.db.DataError","django.db.InterfaceError","django.db.backends.base.features.BaseDatabaseFeatures","django.db.backends.postgresql.psycopg_any.is_psycopg3","django.utils.functional.cached_property"]},"relative_function":[],"code":"class DatabaseFeatures(BaseDatabaseFeatures):\n    minimum_database_version = (14,)\n    allows_group_by_selected_pks = True\n    can_return_columns_from_insert = True\n    can_return_rows_from_bulk_insert = True\n    has_real_datatype = True\n    has_native_uuid_field = True\n    has_native_duration_field = True\n    has_native_json_field = True\n    can_defer_constraint_checks = True\n    has_select_for_update = True\n    has_select_for_update_nowait = True\n    has_select_for_update_of = True\n    has_select_for_update_skip_locked = True\n    has_select_for_no_key_update = True\n    can_release_savepoints = True\n    supports_comments = True\n    supports_tablespaces = True\n    supports_transactions = True\n    can_introspect_materialized_views = True\n    can_distinct_on_fields = True\n    can_rollback_ddl = True\n    schema_editor_uses_clientside_param_binding = True\n    supports_combined_alters = True\n    nulls_order_largest = True\n    closed_cursor_error_class = InterfaceError\n    greatest_least_ignores_nulls = True\n    can_clone_databases = True\n    supports_temporal_subtraction = True\n    supports_slicing_ordering_in_compound = True\n    create_test_procedure_without_params_sql = \"\"\"\n        CREATE FUNCTION test_procedure () RETURNS void AS $$\n        DECLARE\n            V_I INTEGER;\n        BEGIN\n            V_I := 1;\n        END;\n    $$ LANGUAGE plpgsql;\"\"\"\n    create_test_procedure_with_int_param_sql = \"\"\"\n        CREATE FUNCTION test_procedure (P_I INTEGER) RETURNS void AS $$\n        DECLARE\n            V_I INTEGER;\n        BEGIN\n            V_I := P_I;\n        END;\n    $$ LANGUAGE plpgsql;\"\"\"\n    create_test_table_with_composite_primary_key = \"\"\"\n        CREATE TABLE test_table_composite_pk (\n            column_1 INTEGER NOT NULL,\n            column_2 INTEGER NOT NULL,\n            PRIMARY KEY(column_1, column_2)\n        )\n    \"\"\"\n    requires_casted_case_in_updates = True\n    supports_over_clause = True\n    supports_frame_exclusion = True\n    only_supports_unbounded_with_preceding_and_following = True\n    supports_aggregate_filter_clause = True\n    supports_aggregate_order_by_clause = True\n    supported_explain_formats = {\"JSON\", \"TEXT\", \"XML\", \"YAML\"}\n    supports_deferrable_unique_constraints = True\n    has_json_operators = True\n    json_key_contains_list_matching_requires_list = True\n    supports_update_conflicts = True\n    supports_update_conflicts_with_target = True\n    supports_covering_indexes = True\n    supports_stored_generated_columns = True\n    supports_virtual_generated_columns = False\n    can_rename_index = True\n    test_collations = {\n        \"deterministic\": \"C\",\n        \"non_default\": \"sv-x-icu\",\n        \"swedish_ci\": \"sv-x-icu\",\n        \"virtual\": \"sv-x-icu\",\n    }\n    test_now_utc_template = \"STATEMENT_TIMESTAMP() AT TIME ZONE 'UTC'\"\n    insert_test_table_with_defaults = \"INSERT INTO {} DEFAULT VALUES\"\n\n    @cached_property\n    def django_test_skips(self):\n        skips = {\n            \"opclasses are PostgreSQL only.\": {\n                \"indexes.tests.SchemaIndexesNotPostgreSQLTests.\"\n                \"test_create_index_ignores_opclasses\",\n            },\n            \"PostgreSQL requires casting to text.\": {\n                \"lookup.tests.LookupTests.test_textfield_exact_null\",\n            },\n        }\n        if self.connection.settings_dict[\"OPTIONS\"].get(\"pool\"):\n            skips.update(\n                {\n                    \"Pool does implicit health checks\": {\n                        \"backends.base.test_base.ConnectionHealthChecksTests.\"\n                        \"test_health_checks_enabled\",\n                        \"backends.base.test_base.ConnectionHealthChecksTests.\"\n                        \"test_set_autocommit_health_checks_enabled\",\n                    },\n                }\n            )\n        if self.uses_server_side_binding:\n            skips.update(\n                {\n                    \"The actual query cannot be determined for server side bindings\": {\n                        \"backends.base.test_base.ExecuteWrapperTests.\"\n                        \"test_wrapper_debug\",\n                    }\n                },\n            )\n        return skips\n\n    @cached_property\n    def django_test_expected_failures(self):\n        expected_failures = set()\n        if self.uses_server_side_binding:\n            expected_failures.update(\n                {\n                    # Parameters passed to expressions in SELECT and GROUP BY\n                    # clauses are not recognized as the same values when using\n                    # server-side binding cursors (#34255).\n                    \"aggregation.tests.AggregateTestCase.\"\n                    \"test_group_by_nested_expression_with_params\",\n                }\n            )\n        return expected_failures\n\n    @cached_property\n    def uses_server_side_binding(self):\n        options = self.connection.settings_dict[\"OPTIONS\"]\n        return is_psycopg3 and options.get(\"server_side_binding\") is True\n\n    @cached_property\n    def prohibits_null_characters_in_text_exception(self):\n        if is_psycopg3:\n            return DataError, \"PostgreSQL text fields cannot contain NUL (0x00) bytes\"\n        else:\n            return ValueError, \"A string literal cannot contain NUL (0x00) characters.\"\n\n    @cached_property\n    def introspected_field_types(self):\n        return {\n            **super().introspected_field_types,\n            \"PositiveBigIntegerField\": \"BigIntegerField\",\n            \"PositiveIntegerField\": \"IntegerField\",\n            \"PositiveSmallIntegerField\": \"SmallIntegerField\",\n        }\n\n    @cached_property\n    def is_postgresql_15(self):\n        return self.connection.pg_version >= 150000\n\n    @cached_property\n    def is_postgresql_16(self):\n        return self.connection.pg_version >= 160000\n\n    @cached_property\n    def is_postgresql_17(self):\n        return self.connection.pg_version >= 170000\n\n    supports_unlimited_charfield = True\n    supports_nulls_distinct_unique_constraints = property(\n        operator.attrgetter(\"is_postgresql_15\")\n    )"}],"ground_truth":null,"score":null},
{"question":"What is the purpose of the `<Module>` module in the context of `Encapsulate backend-specific introspection utilities`?","answer":"","relative_code_list":[{"start_line":14,"end_line":212,"belongs_to":{"file_name":"introspection.py","upper_path":"../django/django/db/backends/base","module":"base","define_class":["BaseDatabaseIntrospection"],"imports":["collections.namedtuple","django.apps.apps","django.db.router"]},"relative_function":[],"code":"class BaseDatabaseIntrospection:\n    \"\"\"Encapsulate backend-specific introspection utilities.\"\"\"\n\n    data_types_reverse = {}\n\n    def __init__(self, connection):\n        self.connection = connection\n\n    def get_field_type(self, data_type, description):\n        \"\"\"\n        Hook for a database backend to use the cursor description to\n        match a Django field type to a database column.\n\n        For Oracle, the column data_type on its own is insufficient to\n        distinguish between a FloatField and IntegerField, for example.\n        \"\"\"\n        return self.data_types_reverse[data_type]\n\n    def identifier_converter(self, name):\n        \"\"\"\n        Apply a conversion to the identifier for the purposes of comparison.\n\n        The default identifier converter is for case sensitive comparison.\n        \"\"\"\n        return name\n\n    def table_names(self, cursor=None, include_views=False):\n        \"\"\"\n        Return a list of names of all tables that exist in the database.\n        Sort the returned table list by Python's default sorting. Do NOT use\n        the database's ORDER BY here to avoid subtle differences in sorting\n        order between databases.\n        \"\"\"\n\n        def get_names(cursor):\n            return sorted(\n                ti.name\n                for ti in self.get_table_list(cursor)\n                if include_views or ti.type == \"t\"\n            )\n\n        if cursor is None:\n            with self.connection.cursor() as cursor:\n                return get_names(cursor)\n        return get_names(cursor)\n\n    def get_table_list(self, cursor):\n        \"\"\"\n        Return an unsorted list of TableInfo named tuples of all tables and\n        views that exist in the database.\n        \"\"\"\n        raise NotImplementedError(\n            \"subclasses of BaseDatabaseIntrospection may require a get_table_list() \"\n            \"method\"\n        )\n\n    def get_table_description(self, cursor, table_name):\n        \"\"\"\n        Return a description of the table with the DB-API cursor.description\n        interface.\n        \"\"\"\n        raise NotImplementedError(\n            \"subclasses of BaseDatabaseIntrospection may require a \"\n            \"get_table_description() method.\"\n        )\n\n    def get_migratable_models(self):\n        from django.apps import apps\n        from django.db import router\n\n        return (\n            model\n            for app_config in apps.get_app_configs()\n            for model in router.get_migratable_models(app_config, self.connection.alias)\n            if model._meta.can_migrate(self.connection)\n        )\n\n    def django_table_names(self, only_existing=False, include_views=True):\n        \"\"\"\n        Return a list of all table names that have associated Django models and\n        are in INSTALLED_APPS.\n\n        If only_existing is True, include only the tables in the database.\n        \"\"\"\n        tables = set()\n        for model in self.get_migratable_models():\n            if not model._meta.managed:\n                continue\n            tables.add(model._meta.db_table)\n            tables.update(\n                f.m2m_db_table()\n                for f in model._meta.local_many_to_many\n                if f.remote_field.through._meta.managed\n            )\n        tables = list(tables)\n        if only_existing:\n            existing_tables = set(self.table_names(include_views=include_views))\n            tables = [\n                t for t in tables if self.identifier_converter(t) in existing_tables\n            ]\n        return tables\n\n    def installed_models(self, tables):\n        \"\"\"\n        Return a set of all models represented by the provided list of table\n        names.\n        \"\"\"\n        tables = set(map(self.identifier_converter, tables))\n        return {\n            m\n            for m in self.get_migratable_models()\n            if self.identifier_converter(m._meta.db_table) in tables\n        }\n\n    def sequence_list(self):\n        \"\"\"\n        Return a list of information about all DB sequences for all models in\n        all apps.\n        \"\"\"\n        sequence_list = []\n        with self.connection.cursor() as cursor:\n            for model in self.get_migratable_models():\n                if not model._meta.managed:\n                    continue\n                if model._meta.swapped:\n                    continue\n                sequence_list.extend(\n                    self.get_sequences(\n                        cursor, model._meta.db_table, model._meta.local_fields\n                    )\n                )\n                for f in model._meta.local_many_to_many:\n                    # If this is an m2m using an intermediate table,\n                    # we don't need to reset the sequence.\n                    if f.remote_field.through._meta.auto_created:\n                        sequence = self.get_sequences(cursor, f.m2m_db_table())\n                        sequence_list.extend(\n                            sequence or [{\"table\": f.m2m_db_table(), \"column\": None}]\n                        )\n        return sequence_list\n\n    def get_sequences(self, cursor, table_name, table_fields=()):\n        \"\"\"\n        Return a list of introspected sequences for table_name. Each sequence\n        is a dict: {'table': <table_name>, 'column': <column_name>}. An optional\n        'name' key can be added if the backend supports named sequences.\n        \"\"\"\n        raise NotImplementedError(\n            \"subclasses of BaseDatabaseIntrospection may require a get_sequences() \"\n            \"method\"\n        )\n\n    def get_relations(self, cursor, table_name):\n        \"\"\"\n        Return a dictionary of {field_name: (field_name_other_table, other_table)}\n        representing all foreign keys in the given table.\n        \"\"\"\n        raise NotImplementedError(\n            \"subclasses of BaseDatabaseIntrospection may require a \"\n            \"get_relations() method.\"\n        )\n\n    def get_primary_key_column(self, cursor, table_name):\n        \"\"\"\n        Return the name of the primary key column for the given table.\n        \"\"\"\n        columns = self.get_primary_key_columns(cursor, table_name)\n        return columns[0] if columns else None\n\n    def get_primary_key_columns(self, cursor, table_name):\n        \"\"\"Return a list of primary key columns for the given table.\"\"\"\n        for constraint in self.get_constraints(cursor, table_name).values():\n            if constraint[\"primary_key\"]:\n                return constraint[\"columns\"]\n        return None\n\n    def get_constraints(self, cursor, table_name):\n        \"\"\"\n        Retrieve any constraints or keys (unique, pk, fk, check, index)\n        across one or more columns.\n\n        Return a dict mapping constraint names to their attributes,\n        where attributes is a dict with keys:\n         * columns: List of columns this covers\n         * primary_key: True if primary key, False otherwise\n         * unique: True if this is a unique constraint, False otherwise\n         * foreign_key: (table, column) of target, or None\n         * check: True if check constraint, False otherwise\n         * index: True if index, False otherwise.\n         * orders: The order (ASC/DESC) defined for the columns of indexes\n         * type: The type of the index (btree, hash, etc.)\n\n        Some backends may return special constraint names that don't exist\n        if they don't name constraints of a certain type (e.g. SQLite)\n        \"\"\"\n        raise NotImplementedError(\n            \"subclasses of BaseDatabaseIntrospection may require a get_constraints() \"\n            \"method\"\n        )"}],"ground_truth":null,"score":null},
{"question":"Which API or function is responsible for `Retrieve any constraints or keys (unique, pk, fk, check, index)\nacross one or more columns`, and what parameters does it accept?","answer":"","relative_code_list":[{"start_line":190,"end_line":212,"belongs_to":{"file_name":"introspection.py","upper_path":"../django/django/db/backends/base","module":"base","define_class":["BaseDatabaseIntrospection"],"imports":["collections.namedtuple","django.apps.apps","django.db.router"]},"relative_function":[],"code":"def get_constraints(self, cursor, table_name):\n        \"\"\"\n        Retrieve any constraints or keys (unique, pk, fk, check, index)\n        across one or more columns.\n\n        Return a dict mapping constraint names to their attributes,\n        where attributes is a dict with keys:\n         * columns: List of columns this covers\n         * primary_key: True if primary key, False otherwise\n         * unique: True if this is a unique constraint, False otherwise\n         * foreign_key: (table, column) of target, or None\n         * check: True if check constraint, False otherwise\n         * index: True if index, False otherwise.\n         * orders: The order (ASC/DESC) defined for the columns of indexes\n         * type: The type of the index (btree, hash, etc.)\n\n        Some backends may return special constraint names that don't exist\n        if they don't name constraints of a certain type (e.g. SQLite)\n        \"\"\"\n        raise NotImplementedError(\n            \"subclasses of BaseDatabaseIntrospection may require a get_constraints() \"\n            \"method\"\n        )"},{"start_line":14,"end_line":212,"belongs_to":{"file_name":"introspection.py","upper_path":"../django/django/db/backends/base","module":"base","define_class":["BaseDatabaseIntrospection"],"imports":["collections.namedtuple","django.apps.apps","django.db.router"]},"relative_function":[],"code":"class BaseDatabaseIntrospection:\n    \"\"\"Encapsulate backend-specific introspection utilities.\"\"\"\n\n    data_types_reverse = {}\n\n    def __init__(self, connection):\n        self.connection = connection\n\n    def get_field_type(self, data_type, description):\n        \"\"\"\n        Hook for a database backend to use the cursor description to\n        match a Django field type to a database column.\n\n        For Oracle, the column data_type on its own is insufficient to\n        distinguish between a FloatField and IntegerField, for example.\n        \"\"\"\n        return self.data_types_reverse[data_type]\n\n    def identifier_converter(self, name):\n        \"\"\"\n        Apply a conversion to the identifier for the purposes of comparison.\n\n        The default identifier converter is for case sensitive comparison.\n        \"\"\"\n        return name\n\n    def table_names(self, cursor=None, include_views=False):\n        \"\"\"\n        Return a list of names of all tables that exist in the database.\n        Sort the returned table list by Python's default sorting. Do NOT use\n        the database's ORDER BY here to avoid subtle differences in sorting\n        order between databases.\n        \"\"\"\n\n        def get_names(cursor):\n            return sorted(\n                ti.name\n                for ti in self.get_table_list(cursor)\n                if include_views or ti.type == \"t\"\n            )\n\n        if cursor is None:\n            with self.connection.cursor() as cursor:\n                return get_names(cursor)\n        return get_names(cursor)\n\n    def get_table_list(self, cursor):\n        \"\"\"\n        Return an unsorted list of TableInfo named tuples of all tables and\n        views that exist in the database.\n        \"\"\"\n        raise NotImplementedError(\n            \"subclasses of BaseDatabaseIntrospection may require a get_table_list() \"\n            \"method\"\n        )\n\n    def get_table_description(self, cursor, table_name):\n        \"\"\"\n        Return a description of the table with the DB-API cursor.description\n        interface.\n        \"\"\"\n        raise NotImplementedError(\n            \"subclasses of BaseDatabaseIntrospection may require a \"\n            \"get_table_description() method.\"\n        )\n\n    def get_migratable_models(self):\n        from django.apps import apps\n        from django.db import router\n\n        return (\n            model\n            for app_config in apps.get_app_configs()\n            for model in router.get_migratable_models(app_config, self.connection.alias)\n            if model._meta.can_migrate(self.connection)\n        )\n\n    def django_table_names(self, only_existing=False, include_views=True):\n        \"\"\"\n        Return a list of all table names that have associated Django models and\n        are in INSTALLED_APPS.\n\n        If only_existing is True, include only the tables in the database.\n        \"\"\"\n        tables = set()\n        for model in self.get_migratable_models():\n            if not model._meta.managed:\n                continue\n            tables.add(model._meta.db_table)\n            tables.update(\n                f.m2m_db_table()\n                for f in model._meta.local_many_to_many\n                if f.remote_field.through._meta.managed\n            )\n        tables = list(tables)\n        if only_existing:\n            existing_tables = set(self.table_names(include_views=include_views))\n            tables = [\n                t for t in tables if self.identifier_converter(t) in existing_tables\n            ]\n        return tables\n\n    def installed_models(self, tables):\n        \"\"\"\n        Return a set of all models represented by the provided list of table\n        names.\n        \"\"\"\n        tables = set(map(self.identifier_converter, tables))\n        return {\n            m\n            for m in self.get_migratable_models()\n            if self.identifier_converter(m._meta.db_table) in tables\n        }\n\n    def sequence_list(self):\n        \"\"\"\n        Return a list of information about all DB sequences for all models in\n        all apps.\n        \"\"\"\n        sequence_list = []\n        with self.connection.cursor() as cursor:\n            for model in self.get_migratable_models():\n                if not model._meta.managed:\n                    continue\n                if model._meta.swapped:\n                    continue\n                sequence_list.extend(\n                    self.get_sequences(\n                        cursor, model._meta.db_table, model._meta.local_fields\n                    )\n                )\n                for f in model._meta.local_many_to_many:\n                    # If this is an m2m using an intermediate table,\n                    # we don't need to reset the sequence.\n                    if f.remote_field.through._meta.auto_created:\n                        sequence = self.get_sequences(cursor, f.m2m_db_table())\n                        sequence_list.extend(\n                            sequence or [{\"table\": f.m2m_db_table(), \"column\": None}]\n                        )\n        return sequence_list\n\n    def get_sequences(self, cursor, table_name, table_fields=()):\n        \"\"\"\n        Return a list of introspected sequences for table_name. Each sequence\n        is a dict: {'table': <table_name>, 'column': <column_name>}. An optional\n        'name' key can be added if the backend supports named sequences.\n        \"\"\"\n        raise NotImplementedError(\n            \"subclasses of BaseDatabaseIntrospection may require a get_sequences() \"\n            \"method\"\n        )\n\n    def get_relations(self, cursor, table_name):\n        \"\"\"\n        Return a dictionary of {field_name: (field_name_other_table, other_table)}\n        representing all foreign keys in the given table.\n        \"\"\"\n        raise NotImplementedError(\n            \"subclasses of BaseDatabaseIntrospection may require a \"\n            \"get_relations() method.\"\n        )\n\n    def get_primary_key_column(self, cursor, table_name):\n        \"\"\"\n        Return the name of the primary key column for the given table.\n        \"\"\"\n        columns = self.get_primary_key_columns(cursor, table_name)\n        return columns[0] if columns else None\n\n    def get_primary_key_columns(self, cursor, table_name):\n        \"\"\"Return a list of primary key columns for the given table.\"\"\"\n        for constraint in self.get_constraints(cursor, table_name).values():\n            if constraint[\"primary_key\"]:\n                return constraint[\"columns\"]\n        return None\n\n    def get_constraints(self, cursor, table_name):\n        \"\"\"\n        Retrieve any constraints or keys (unique, pk, fk, check, index)\n        across one or more columns.\n\n        Return a dict mapping constraint names to their attributes,\n        where attributes is a dict with keys:\n         * columns: List of columns this covers\n         * primary_key: True if primary key, False otherwise\n         * unique: True if this is a unique constraint, False otherwise\n         * foreign_key: (table, column) of target, or None\n         * check: True if check constraint, False otherwise\n         * index: True if index, False otherwise.\n         * orders: The order (ASC/DESC) defined for the columns of indexes\n         * type: The type of the index (btree, hash, etc.)\n\n        Some backends may return special constraint names that don't exist\n        if they don't name constraints of a certain type (e.g. SQLite)\n        \"\"\"\n        raise NotImplementedError(\n            \"subclasses of BaseDatabaseIntrospection may require a get_constraints() \"\n            \"method\"\n        )"}],"ground_truth":null,"score":null},
{"question":"What are the expected parameters and return values for the `can_return_columns_from_insert` method in the context of `DatabaseFeatures`?","answer":"","relative_code_list":[{"start_line":9,"end_line":171,"belongs_to":{"file_name":"features.py","upper_path":"../django/django/db/backends/postgresql","module":"postgresql","define_class":["DatabaseFeatures"],"imports":["operator","django.db.DataError","django.db.InterfaceError","django.db.backends.base.features.BaseDatabaseFeatures","django.db.backends.postgresql.psycopg_any.is_psycopg3","django.utils.functional.cached_property"]},"relative_function":[],"code":"class DatabaseFeatures(BaseDatabaseFeatures):\n    minimum_database_version = (14,)\n    allows_group_by_selected_pks = True\n    can_return_columns_from_insert = True\n    can_return_rows_from_bulk_insert = True\n    has_real_datatype = True\n    has_native_uuid_field = True\n    has_native_duration_field = True\n    has_native_json_field = True\n    can_defer_constraint_checks = True\n    has_select_for_update = True\n    has_select_for_update_nowait = True\n    has_select_for_update_of = True\n    has_select_for_update_skip_locked = True\n    has_select_for_no_key_update = True\n    can_release_savepoints = True\n    supports_comments = True\n    supports_tablespaces = True\n    supports_transactions = True\n    can_introspect_materialized_views = True\n    can_distinct_on_fields = True\n    can_rollback_ddl = True\n    schema_editor_uses_clientside_param_binding = True\n    supports_combined_alters = True\n    nulls_order_largest = True\n    closed_cursor_error_class = InterfaceError\n    greatest_least_ignores_nulls = True\n    can_clone_databases = True\n    supports_temporal_subtraction = True\n    supports_slicing_ordering_in_compound = True\n    create_test_procedure_without_params_sql = \"\"\"\n        CREATE FUNCTION test_procedure () RETURNS void AS $$\n        DECLARE\n            V_I INTEGER;\n        BEGIN\n            V_I := 1;\n        END;\n    $$ LANGUAGE plpgsql;\"\"\"\n    create_test_procedure_with_int_param_sql = \"\"\"\n        CREATE FUNCTION test_procedure (P_I INTEGER) RETURNS void AS $$\n        DECLARE\n            V_I INTEGER;\n        BEGIN\n            V_I := P_I;\n        END;\n    $$ LANGUAGE plpgsql;\"\"\"\n    create_test_table_with_composite_primary_key = \"\"\"\n        CREATE TABLE test_table_composite_pk (\n            column_1 INTEGER NOT NULL,\n            column_2 INTEGER NOT NULL,\n            PRIMARY KEY(column_1, column_2)\n        )\n    \"\"\"\n    requires_casted_case_in_updates = True\n    supports_over_clause = True\n    supports_frame_exclusion = True\n    only_supports_unbounded_with_preceding_and_following = True\n    supports_aggregate_filter_clause = True\n    supports_aggregate_order_by_clause = True\n    supported_explain_formats = {\"JSON\", \"TEXT\", \"XML\", \"YAML\"}\n    supports_deferrable_unique_constraints = True\n    has_json_operators = True\n    json_key_contains_list_matching_requires_list = True\n    supports_update_conflicts = True\n    supports_update_conflicts_with_target = True\n    supports_covering_indexes = True\n    supports_stored_generated_columns = True\n    supports_virtual_generated_columns = False\n    can_rename_index = True\n    test_collations = {\n        \"deterministic\": \"C\",\n        \"non_default\": \"sv-x-icu\",\n        \"swedish_ci\": \"sv-x-icu\",\n        \"virtual\": \"sv-x-icu\",\n    }\n    test_now_utc_template = \"STATEMENT_TIMESTAMP() AT TIME ZONE 'UTC'\"\n    insert_test_table_with_defaults = \"INSERT INTO {} DEFAULT VALUES\"\n\n    @cached_property\n    def django_test_skips(self):\n        skips = {\n            \"opclasses are PostgreSQL only.\": {\n                \"indexes.tests.SchemaIndexesNotPostgreSQLTests.\"\n                \"test_create_index_ignores_opclasses\",\n            },\n            \"PostgreSQL requires casting to text.\": {\n                \"lookup.tests.LookupTests.test_textfield_exact_null\",\n            },\n        }\n        if self.connection.settings_dict[\"OPTIONS\"].get(\"pool\"):\n            skips.update(\n                {\n                    \"Pool does implicit health checks\": {\n                        \"backends.base.test_base.ConnectionHealthChecksTests.\"\n                        \"test_health_checks_enabled\",\n                        \"backends.base.test_base.ConnectionHealthChecksTests.\"\n                        \"test_set_autocommit_health_checks_enabled\",\n                    },\n                }\n            )\n        if self.uses_server_side_binding:\n            skips.update(\n                {\n                    \"The actual query cannot be determined for server side bindings\": {\n                        \"backends.base.test_base.ExecuteWrapperTests.\"\n                        \"test_wrapper_debug\",\n                    }\n                },\n            )\n        return skips\n\n    @cached_property\n    def django_test_expected_failures(self):\n        expected_failures = set()\n        if self.uses_server_side_binding:\n            expected_failures.update(\n                {\n                    # Parameters passed to expressions in SELECT and GROUP BY\n                    # clauses are not recognized as the same values when using\n                    # server-side binding cursors (#34255).\n                    \"aggregation.tests.AggregateTestCase.\"\n                    \"test_group_by_nested_expression_with_params\",\n                }\n            )\n        return expected_failures\n\n    @cached_property\n    def uses_server_side_binding(self):\n        options = self.connection.settings_dict[\"OPTIONS\"]\n        return is_psycopg3 and options.get(\"server_side_binding\") is True\n\n    @cached_property\n    def prohibits_null_characters_in_text_exception(self):\n        if is_psycopg3:\n            return DataError, \"PostgreSQL text fields cannot contain NUL (0x00) bytes\"\n        else:\n            return ValueError, \"A string literal cannot contain NUL (0x00) characters.\"\n\n    @cached_property\n    def introspected_field_types(self):\n        return {\n            **super().introspected_field_types,\n            \"PositiveBigIntegerField\": \"BigIntegerField\",\n            \"PositiveIntegerField\": \"IntegerField\",\n            \"PositiveSmallIntegerField\": \"SmallIntegerField\",\n        }\n\n    @cached_property\n    def is_postgresql_15(self):\n        return self.connection.pg_version >= 150000\n\n    @cached_property\n    def is_postgresql_16(self):\n        return self.connection.pg_version >= 160000\n\n    @cached_property\n    def is_postgresql_17(self):\n        return self.connection.pg_version >= 170000\n\n    supports_unlimited_charfield = True\n    supports_nulls_distinct_unique_constraints = property(\n        operator.attrgetter(\"is_postgresql_15\")\n    )"},{"start_line":164,"end_line":165,"belongs_to":{"file_name":"features.py","upper_path":"../django/django/db/backends/sqlite3","module":"sqlite3","define_class":["DatabaseFeatures"],"imports":["operator","django.db.transaction","django.db.backends.base.features.BaseDatabaseFeatures","django.db.utils.OperationalError","django.utils.functional.cached_property","base.Database"]},"relative_function":[],"code":"def can_return_columns_from_insert(self):\n        return Database.sqlite_version_info >= (3, 35)"}],"ground_truth":null,"score":null},
{"question":"What is the purpose of the `has_select_for_no_key_update` attribute in `DatabaseFeatures`?","answer":"","relative_code_list":[{"start_line":9,"end_line":171,"belongs_to":{"file_name":"features.py","upper_path":"../django/django/db/backends/postgresql","module":"postgresql","define_class":["DatabaseFeatures"],"imports":["operator","django.db.DataError","django.db.InterfaceError","django.db.backends.base.features.BaseDatabaseFeatures","django.db.backends.postgresql.psycopg_any.is_psycopg3","django.utils.functional.cached_property"]},"relative_function":[],"code":"class DatabaseFeatures(BaseDatabaseFeatures):\n    minimum_database_version = (14,)\n    allows_group_by_selected_pks = True\n    can_return_columns_from_insert = True\n    can_return_rows_from_bulk_insert = True\n    has_real_datatype = True\n    has_native_uuid_field = True\n    has_native_duration_field = True\n    has_native_json_field = True\n    can_defer_constraint_checks = True\n    has_select_for_update = True\n    has_select_for_update_nowait = True\n    has_select_for_update_of = True\n    has_select_for_update_skip_locked = True\n    has_select_for_no_key_update = True\n    can_release_savepoints = True\n    supports_comments = True\n    supports_tablespaces = True\n    supports_transactions = True\n    can_introspect_materialized_views = True\n    can_distinct_on_fields = True\n    can_rollback_ddl = True\n    schema_editor_uses_clientside_param_binding = True\n    supports_combined_alters = True\n    nulls_order_largest = True\n    closed_cursor_error_class = InterfaceError\n    greatest_least_ignores_nulls = True\n    can_clone_databases = True\n    supports_temporal_subtraction = True\n    supports_slicing_ordering_in_compound = True\n    create_test_procedure_without_params_sql = \"\"\"\n        CREATE FUNCTION test_procedure () RETURNS void AS $$\n        DECLARE\n            V_I INTEGER;\n        BEGIN\n            V_I := 1;\n        END;\n    $$ LANGUAGE plpgsql;\"\"\"\n    create_test_procedure_with_int_param_sql = \"\"\"\n        CREATE FUNCTION test_procedure (P_I INTEGER) RETURNS void AS $$\n        DECLARE\n            V_I INTEGER;\n        BEGIN\n            V_I := P_I;\n        END;\n    $$ LANGUAGE plpgsql;\"\"\"\n    create_test_table_with_composite_primary_key = \"\"\"\n        CREATE TABLE test_table_composite_pk (\n            column_1 INTEGER NOT NULL,\n            column_2 INTEGER NOT NULL,\n            PRIMARY KEY(column_1, column_2)\n        )\n    \"\"\"\n    requires_casted_case_in_updates = True\n    supports_over_clause = True\n    supports_frame_exclusion = True\n    only_supports_unbounded_with_preceding_and_following = True\n    supports_aggregate_filter_clause = True\n    supports_aggregate_order_by_clause = True\n    supported_explain_formats = {\"JSON\", \"TEXT\", \"XML\", \"YAML\"}\n    supports_deferrable_unique_constraints = True\n    has_json_operators = True\n    json_key_contains_list_matching_requires_list = True\n    supports_update_conflicts = True\n    supports_update_conflicts_with_target = True\n    supports_covering_indexes = True\n    supports_stored_generated_columns = True\n    supports_virtual_generated_columns = False\n    can_rename_index = True\n    test_collations = {\n        \"deterministic\": \"C\",\n        \"non_default\": \"sv-x-icu\",\n        \"swedish_ci\": \"sv-x-icu\",\n        \"virtual\": \"sv-x-icu\",\n    }\n    test_now_utc_template = \"STATEMENT_TIMESTAMP() AT TIME ZONE 'UTC'\"\n    insert_test_table_with_defaults = \"INSERT INTO {} DEFAULT VALUES\"\n\n    @cached_property\n    def django_test_skips(self):\n        skips = {\n            \"opclasses are PostgreSQL only.\": {\n                \"indexes.tests.SchemaIndexesNotPostgreSQLTests.\"\n                \"test_create_index_ignores_opclasses\",\n            },\n            \"PostgreSQL requires casting to text.\": {\n                \"lookup.tests.LookupTests.test_textfield_exact_null\",\n            },\n        }\n        if self.connection.settings_dict[\"OPTIONS\"].get(\"pool\"):\n            skips.update(\n                {\n                    \"Pool does implicit health checks\": {\n                        \"backends.base.test_base.ConnectionHealthChecksTests.\"\n                        \"test_health_checks_enabled\",\n                        \"backends.base.test_base.ConnectionHealthChecksTests.\"\n                        \"test_set_autocommit_health_checks_enabled\",\n                    },\n                }\n            )\n        if self.uses_server_side_binding:\n            skips.update(\n                {\n                    \"The actual query cannot be determined for server side bindings\": {\n                        \"backends.base.test_base.ExecuteWrapperTests.\"\n                        \"test_wrapper_debug\",\n                    }\n                },\n            )\n        return skips\n\n    @cached_property\n    def django_test_expected_failures(self):\n        expected_failures = set()\n        if self.uses_server_side_binding:\n            expected_failures.update(\n                {\n                    # Parameters passed to expressions in SELECT and GROUP BY\n                    # clauses are not recognized as the same values when using\n                    # server-side binding cursors (#34255).\n                    \"aggregation.tests.AggregateTestCase.\"\n                    \"test_group_by_nested_expression_with_params\",\n                }\n            )\n        return expected_failures\n\n    @cached_property\n    def uses_server_side_binding(self):\n        options = self.connection.settings_dict[\"OPTIONS\"]\n        return is_psycopg3 and options.get(\"server_side_binding\") is True\n\n    @cached_property\n    def prohibits_null_characters_in_text_exception(self):\n        if is_psycopg3:\n            return DataError, \"PostgreSQL text fields cannot contain NUL (0x00) bytes\"\n        else:\n            return ValueError, \"A string literal cannot contain NUL (0x00) characters.\"\n\n    @cached_property\n    def introspected_field_types(self):\n        return {\n            **super().introspected_field_types,\n            \"PositiveBigIntegerField\": \"BigIntegerField\",\n            \"PositiveIntegerField\": \"IntegerField\",\n            \"PositiveSmallIntegerField\": \"SmallIntegerField\",\n        }\n\n    @cached_property\n    def is_postgresql_15(self):\n        return self.connection.pg_version >= 150000\n\n    @cached_property\n    def is_postgresql_16(self):\n        return self.connection.pg_version >= 160000\n\n    @cached_property\n    def is_postgresql_17(self):\n        return self.connection.pg_version >= 170000\n\n    supports_unlimited_charfield = True\n    supports_nulls_distinct_unique_constraints = property(\n        operator.attrgetter(\"is_postgresql_15\")\n    )"}],"ground_truth":null,"score":null},
{"question":"Does the `DatabaseOperations` have any dependencies or interactions with external APIs that might be affected by changes to the `quote_name` method?","answer":"","relative_code_list":[{"start_line":27,"end_line":422,"belongs_to":{"file_name":"operations.py","upper_path":"../django/django/db/backends/postgresql","module":"postgresql","define_class":["DatabaseOperations"],"imports":["json","functools.lru_cache","functools.partial","django.conf.settings","django.db.backends.base.operations.BaseDatabaseOperations","django.db.backends.postgresql.compiler.InsertUnnest","django.db.backends.postgresql.psycopg_any.Inet","django.db.backends.postgresql.psycopg_any.Jsonb","django.db.backends.postgresql.psycopg_any.errors","django.db.backends.postgresql.psycopg_any.is_psycopg3","django.db.backends.postgresql.psycopg_any.mogrify","django.db.backends.utils.split_tzname_delta","django.db.models.constants.OnConflict","django.db.models.functions.Cast","django.utils.regex_helper._lazy_re_compile","psycopg.types.numeric","django.db.models"]},"relative_function":[],"code":"class DatabaseOperations(BaseDatabaseOperations):\n    compiler_module = \"django.db.backends.postgresql.compiler\"\n    cast_char_field_without_max_length = \"varchar\"\n    explain_prefix = \"EXPLAIN\"\n    explain_options = frozenset(\n        [\n            \"ANALYZE\",\n            \"BUFFERS\",\n            \"COSTS\",\n            \"GENERIC_PLAN\",\n            \"MEMORY\",\n            \"SETTINGS\",\n            \"SERIALIZE\",\n            \"SUMMARY\",\n            \"TIMING\",\n            \"VERBOSE\",\n            \"WAL\",\n        ]\n    )\n    cast_data_types = {\n        \"AutoField\": \"integer\",\n        \"BigAutoField\": \"bigint\",\n        \"SmallAutoField\": \"smallint\",\n    }\n\n    if is_psycopg3:\n        from psycopg.types import numeric\n\n        integerfield_type_map = {\n            \"SmallIntegerField\": numeric.Int2,\n            \"IntegerField\": numeric.Int4,\n            \"BigIntegerField\": numeric.Int8,\n            \"PositiveSmallIntegerField\": numeric.Int2,\n            \"PositiveIntegerField\": numeric.Int4,\n            \"PositiveBigIntegerField\": numeric.Int8,\n        }\n\n    def unification_cast_sql(self, output_field):\n        internal_type = output_field.get_internal_type()\n        if internal_type in (\n            \"GenericIPAddressField\",\n            \"IPAddressField\",\n            \"TimeField\",\n            \"UUIDField\",\n        ):\n            # PostgreSQL will resolve a union as type 'text' if input types are\n            # 'unknown'.\n            # https://www.postgresql.org/docs/current/typeconv-union-case.html\n            # These fields cannot be implicitly cast back in the default\n            # PostgreSQL configuration so we need to explicitly cast them.\n            # We must also remove components of the type within brackets:\n            # varchar(255) -> varchar.\n            return (\n                \"CAST(%%s AS %s)\" % output_field.db_type(self.connection).split(\"(\")[0]\n            )\n        return \"%s\"\n\n    # EXTRACT format cannot be passed in parameters.\n    _extract_format_re = _lazy_re_compile(r\"[A-Z_]+\")\n\n    def date_extract_sql(self, lookup_type, sql, params):\n        # https://www.postgresql.org/docs/current/functions-datetime.html#FUNCTIONS-DATETIME-EXTRACT\n        if lookup_type == \"week_day\":\n            # For consistency across backends, we return Sunday=1, Saturday=7.\n            return f\"EXTRACT(DOW FROM {sql}) + 1\", params\n        elif lookup_type == \"iso_week_day\":\n            return f\"EXTRACT(ISODOW FROM {sql})\", params\n        elif lookup_type == \"iso_year\":\n            return f\"EXTRACT(ISOYEAR FROM {sql})\", params\n\n        lookup_type = lookup_type.upper()\n        if not self._extract_format_re.fullmatch(lookup_type):\n            raise ValueError(f\"Invalid lookup type: {lookup_type!r}\")\n        return f\"EXTRACT({lookup_type} FROM {sql})\", params\n\n    def date_trunc_sql(self, lookup_type, sql, params, tzname=None):\n        sql, params = self._convert_sql_to_tz(sql, params, tzname)\n        # https://www.postgresql.org/docs/current/functions-datetime.html#FUNCTIONS-DATETIME-TRUNC\n        return f\"DATE_TRUNC(%s, {sql})\", (lookup_type, *params)\n\n    def _prepare_tzname_delta(self, tzname):\n        tzname, sign, offset = split_tzname_delta(tzname)\n        if offset:\n            sign = \"-\" if sign == \"+\" else \"+\"\n            return f\"{tzname}{sign}{offset}\"\n        return tzname\n\n    def _convert_sql_to_tz(self, sql, params, tzname):\n        if tzname and settings.USE_TZ:\n            tzname_param = self._prepare_tzname_delta(tzname)\n            return f\"{sql} AT TIME ZONE %s\", (*params, tzname_param)\n        return sql, params\n\n    def datetime_cast_date_sql(self, sql, params, tzname):\n        sql, params = self._convert_sql_to_tz(sql, params, tzname)\n        return f\"({sql})::date\", params\n\n    def datetime_cast_time_sql(self, sql, params, tzname):\n        sql, params = self._convert_sql_to_tz(sql, params, tzname)\n        return f\"({sql})::time\", params\n\n    def datetime_extract_sql(self, lookup_type, sql, params, tzname):\n        sql, params = self._convert_sql_to_tz(sql, params, tzname)\n        if lookup_type == \"second\":\n            # Truncate fractional seconds.\n            return f\"EXTRACT(SECOND FROM DATE_TRUNC(%s, {sql}))\", (\"second\", *params)\n        return self.date_extract_sql(lookup_type, sql, params)\n\n    def datetime_trunc_sql(self, lookup_type, sql, params, tzname):\n        sql, params = self._convert_sql_to_tz(sql, params, tzname)\n        # https://www.postgresql.org/docs/current/functions-datetime.html#FUNCTIONS-DATETIME-TRUNC\n        return f\"DATE_TRUNC(%s, {sql})\", (lookup_type, *params)\n\n    def time_extract_sql(self, lookup_type, sql, params):\n        if lookup_type == \"second\":\n            # Truncate fractional seconds.\n            return f\"EXTRACT(SECOND FROM DATE_TRUNC(%s, {sql}))\", (\"second\", *params)\n        return self.date_extract_sql(lookup_type, sql, params)\n\n    def time_trunc_sql(self, lookup_type, sql, params, tzname=None):\n        sql, params = self._convert_sql_to_tz(sql, params, tzname)\n        return f\"DATE_TRUNC(%s, {sql})::time\", (lookup_type, *params)\n\n    def deferrable_sql(self):\n        return \" DEFERRABLE INITIALLY DEFERRED\"\n\n    def bulk_insert_sql(self, fields, placeholder_rows):\n        if isinstance(placeholder_rows, InsertUnnest):\n            return f\"SELECT * FROM {placeholder_rows}\"\n        return super().bulk_insert_sql(fields, placeholder_rows)\n\n    def fetch_returned_insert_rows(self, cursor):\n        \"\"\"\n        Given a cursor object that has just performed an INSERT...RETURNING\n        statement into a table, return the tuple of returned data.\n        \"\"\"\n        return cursor.fetchall()\n\n    def lookup_cast(self, lookup_type, internal_type=None):\n        lookup = \"%s\"\n        # Cast text lookups to text to allow things like filter(x__contains=4)\n        if lookup_type in (\n            \"iexact\",\n            \"contains\",\n            \"icontains\",\n            \"startswith\",\n            \"istartswith\",\n            \"endswith\",\n            \"iendswith\",\n            \"regex\",\n            \"iregex\",\n        ):\n            if internal_type in (\"IPAddressField\", \"GenericIPAddressField\"):\n                lookup = \"HOST(%s)\"\n            else:\n                lookup = \"%s::text\"\n\n        # Use UPPER(x) for case-insensitive lookups; it's faster.\n        if lookup_type in (\"iexact\", \"icontains\", \"istartswith\", \"iendswith\"):\n            lookup = \"UPPER(%s)\" % lookup\n\n        return lookup\n\n    def no_limit_value(self):\n        return None\n\n    def prepare_sql_script(self, sql):\n        return [sql]\n\n    def quote_name(self, name):\n        if name.startswith('\"') and name.endswith('\"'):\n            return name  # Quoting once is enough.\n        return '\"%s\"' % name\n\n    def compose_sql(self, sql, params):\n        return mogrify(sql, params, self.connection)\n\n    def set_time_zone_sql(self):\n        return \"SELECT set_config('TimeZone', %s, false)\"\n\n    def sql_flush(self, style, tables, *, reset_sequences=False, allow_cascade=False):\n        if not tables:\n            return []\n\n        # Perform a single SQL 'TRUNCATE x, y, z...;' statement. It allows us\n        # to truncate tables referenced by a foreign key in any other table.\n        sql_parts = [\n            style.SQL_KEYWORD(\"TRUNCATE\"),\n            \", \".join(style.SQL_FIELD(self.quote_name(table)) for table in tables),\n        ]\n        if reset_sequences:\n            sql_parts.append(style.SQL_KEYWORD(\"RESTART IDENTITY\"))\n        if allow_cascade:\n            sql_parts.append(style.SQL_KEYWORD(\"CASCADE\"))\n        return [\"%s;\" % \" \".join(sql_parts)]\n\n    def sequence_reset_by_name_sql(self, style, sequences):\n        # 'ALTER SEQUENCE sequence_name RESTART WITH 1;'... style SQL statements\n        # to reset sequence indices\n        sql = []\n        for sequence_info in sequences:\n            table_name = sequence_info[\"table\"]\n            # 'id' will be the case if it's an m2m using an autogenerated\n            # intermediate table (see BaseDatabaseIntrospection.sequence_list).\n            column_name = sequence_info[\"column\"] or \"id\"\n            sql.append(\n                \"%s setval(pg_get_serial_sequence('%s','%s'), 1, false);\"\n                % (\n                    style.SQL_KEYWORD(\"SELECT\"),\n                    style.SQL_TABLE(self.quote_name(table_name)),\n                    style.SQL_FIELD(column_name),\n                )\n            )\n        return sql\n\n    def tablespace_sql(self, tablespace, inline=False):\n        if inline:\n            return \"USING INDEX TABLESPACE %s\" % self.quote_name(tablespace)\n        else:\n            return \"TABLESPACE %s\" % self.quote_name(tablespace)\n\n    def sequence_reset_sql(self, style, model_list):\n        from django.db import models\n\n        output = []\n        qn = self.quote_name\n        for model in model_list:\n            # Use `coalesce` to set the sequence for each model to the max pk\n            # value if there are records, or 1 if there are none. Set the\n            # `is_called` property (the third argument to `setval`) to true if\n            # there are records (as the max pk value is already in use),\n            # otherwise set it to false. Use pg_get_serial_sequence to get the\n            # underlying sequence name from the table name and column name.\n\n            for f in model._meta.local_fields:\n                if isinstance(f, models.AutoField):\n                    output.append(\n                        \"%s setval(pg_get_serial_sequence('%s','%s'), \"\n                        \"coalesce(max(%s), 1), max(%s) %s null) %s %s;\"\n                        % (\n                            style.SQL_KEYWORD(\"SELECT\"),\n                            style.SQL_TABLE(qn(model._meta.db_table)),\n                            style.SQL_FIELD(f.column),\n                            style.SQL_FIELD(qn(f.column)),\n                            style.SQL_FIELD(qn(f.column)),\n                            style.SQL_KEYWORD(\"IS NOT\"),\n                            style.SQL_KEYWORD(\"FROM\"),\n                            style.SQL_TABLE(qn(model._meta.db_table)),\n                        )\n                    )\n                    # Only one AutoField is allowed per model, so don't bother\n                    # continuing.\n                    break\n        return output\n\n    def prep_for_iexact_query(self, x):\n        return x\n\n    def max_name_length(self):\n        \"\"\"\n        Return the maximum length of an identifier.\n\n        The maximum length of an identifier is 63 by default, but can be\n        changed by recompiling PostgreSQL after editing the NAMEDATALEN\n        macro in src/include/pg_config_manual.h.\n\n        This implementation returns 63, but can be overridden by a custom\n        database backend that inherits most of its behavior from this one.\n        \"\"\"\n        return 63\n\n    def distinct_sql(self, fields, params):\n        if fields:\n            params = [param for param_list in params for param in param_list]\n            return ([\"DISTINCT ON (%s)\" % \", \".join(fields)], params)\n        else:\n            return [\"DISTINCT\"], []\n\n    if is_psycopg3:\n\n        def last_executed_query(self, cursor, sql, params):\n            if self.connection.features.uses_server_side_binding:\n                try:\n                    return self.compose_sql(sql, params)\n                except errors.DataError:\n                    return None\n            else:\n                if cursor._query and cursor._query.query is not None:\n                    return cursor._query.query.decode()\n                return None\n\n    else:\n\n        def last_executed_query(self, cursor, sql, params):\n            # https://www.psycopg.org/docs/cursor.html#cursor.query\n            # The query attribute is a Psycopg extension to the DB API 2.0.\n            if cursor.query is not None:\n                return cursor.query.decode()\n            return None\n\n    def return_insert_columns(self, fields):\n        if not fields:\n            return \"\", ()\n        columns = [\n            \"%s.%s\"\n            % (\n                self.quote_name(field.model._meta.db_table),\n                self.quote_name(field.column),\n            )\n            for field in fields\n        ]\n        return \"RETURNING %s\" % \", \".join(columns), ()\n\n    if is_psycopg3:\n\n        def adapt_integerfield_value(self, value, internal_type):\n            if value is None or hasattr(value, \"resolve_expression\"):\n                return value\n            return self.integerfield_type_map[internal_type](value)\n\n    def adapt_datefield_value(self, value):\n        return value\n\n    def adapt_datetimefield_value(self, value):\n        return value\n\n    def adapt_timefield_value(self, value):\n        return value\n\n    def adapt_ipaddressfield_value(self, value):\n        if value:\n            return Inet(value)\n        return None\n\n    def adapt_json_value(self, value, encoder):\n        return Jsonb(value, dumps=get_json_dumps(encoder))\n\n    def subtract_temporals(self, internal_type, lhs, rhs):\n        if internal_type == \"DateField\":\n            lhs_sql, lhs_params = lhs\n            rhs_sql, rhs_params = rhs\n            params = (*lhs_params, *rhs_params)\n            return \"(interval '1 day' * (%s - %s))\" % (lhs_sql, rhs_sql), params\n        return super().subtract_temporals(internal_type, lhs, rhs)\n\n    def explain_query_prefix(self, format=None, **options):\n        extra = {}\n        if serialize := options.pop(\"serialize\", None):\n            if serialize.upper() in {\"TEXT\", \"BINARY\"}:\n                extra[\"SERIALIZE\"] = serialize.upper()\n        # Normalize options.\n        if options:\n            options = {\n                name.upper(): \"true\" if value else \"false\"\n                for name, value in options.items()\n            }\n            for valid_option in self.explain_options:\n                value = options.pop(valid_option, None)\n                if value is not None:\n                    extra[valid_option] = value\n        prefix = super().explain_query_prefix(format, **options)\n        if format:\n            extra[\"FORMAT\"] = format\n        if extra:\n            prefix += \" (%s)\" % \", \".join(\"%s %s\" % i for i in extra.items())\n        return prefix\n\n    def on_conflict_suffix_sql(self, fields, on_conflict, update_fields, unique_fields):\n        if on_conflict == OnConflict.IGNORE:\n            return \"ON CONFLICT DO NOTHING\"\n        if on_conflict == OnConflict.UPDATE:\n            return \"ON CONFLICT(%s) DO UPDATE SET %s\" % (\n                \", \".join(map(self.quote_name, unique_fields)),\n                \", \".join(\n                    [\n                        f\"{field} = EXCLUDED.{field}\"\n                        for field in map(self.quote_name, update_fields)\n                    ]\n                ),\n            )\n        return super().on_conflict_suffix_sql(\n            fields,\n            on_conflict,\n            update_fields,\n            unique_fields,\n        )\n\n    def prepare_join_on_clause(self, lhs_table, lhs_field, rhs_table, rhs_field):\n        lhs_expr, rhs_expr = super().prepare_join_on_clause(\n            lhs_table, lhs_field, rhs_table, rhs_field\n        )\n\n        if lhs_field.db_type(self.connection) != rhs_field.db_type(self.connection):\n            rhs_expr = Cast(rhs_expr, lhs_field)\n\n        return lhs_expr, rhs_expr"},{"start_line":196,"end_line":199,"belongs_to":{"file_name":"operations.py","upper_path":"../django/django/db/backends/postgresql","module":"postgresql","define_class":["DatabaseOperations"],"imports":["json","functools.lru_cache","functools.partial","django.conf.settings","django.db.backends.base.operations.BaseDatabaseOperations","django.db.backends.postgresql.compiler.InsertUnnest","django.db.backends.postgresql.psycopg_any.Inet","django.db.backends.postgresql.psycopg_any.Jsonb","django.db.backends.postgresql.psycopg_any.errors","django.db.backends.postgresql.psycopg_any.is_psycopg3","django.db.backends.postgresql.psycopg_any.mogrify","django.db.backends.utils.split_tzname_delta","django.db.models.constants.OnConflict","django.db.models.functions.Cast","django.utils.regex_helper._lazy_re_compile","psycopg.types.numeric","django.db.models"]},"relative_function":[],"code":"def quote_name(self, name):\n        if name.startswith('\"') and name.endswith('\"'):\n            return name  # Quoting once is enough.\n        return '\"%s\"' % name"}],"ground_truth":null,"score":null},
{"question":"Where can I find the implementation of the `DatabaseCreation` in the codebase?","answer":"","relative_code_list":[{"start_line":10,"end_line":87,"belongs_to":{"file_name":"creation.py","upper_path":"../django/django/db/backends/mysql","module":"mysql","define_class":["DatabaseCreation"],"imports":["os","subprocess","sys","django.db.backends.base.creation.BaseDatabaseCreation","client.DatabaseClient"]},"relative_function":[],"code":"class DatabaseCreation(BaseDatabaseCreation):\n    def sql_table_creation_suffix(self):\n        suffix = []\n        test_settings = self.connection.settings_dict[\"TEST\"]\n        if test_settings[\"CHARSET\"]:\n            suffix.append(\"CHARACTER SET %s\" % test_settings[\"CHARSET\"])\n        if test_settings[\"COLLATION\"]:\n            suffix.append(\"COLLATE %s\" % test_settings[\"COLLATION\"])\n        return \" \".join(suffix)\n\n    def _execute_create_test_db(self, cursor, parameters, keepdb=False):\n        try:\n            super()._execute_create_test_db(cursor, parameters, keepdb)\n        except Exception as e:\n            if len(e.args) < 1 or e.args[0] != 1007:\n                # All errors except \"database exists\" (1007) cancel tests.\n                self.log(\"Got an error creating the test database: %s\" % e)\n                sys.exit(2)\n            else:\n                raise\n\n    def _clone_test_db(self, suffix, verbosity, keepdb=False):\n        source_database_name = self.connection.settings_dict[\"NAME\"]\n        target_database_name = self.get_test_db_clone_settings(suffix)[\"NAME\"]\n        test_db_params = {\n            \"dbname\": self.connection.ops.quote_name(target_database_name),\n            \"suffix\": self.sql_table_creation_suffix(),\n        }\n        with self._nodb_cursor() as cursor:\n            try:\n                self._execute_create_test_db(cursor, test_db_params, keepdb)\n            except Exception:\n                if keepdb:\n                    # If the database should be kept, skip everything else.\n                    return\n                try:\n                    if verbosity >= 1:\n                        self.log(\n                            \"Destroying old test database for alias %s...\"\n                            % (\n                                self._get_database_display_str(\n                                    verbosity, target_database_name\n                                ),\n                            )\n                        )\n                    cursor.execute(\"DROP DATABASE %(dbname)s\" % test_db_params)\n                    self._execute_create_test_db(cursor, test_db_params, keepdb)\n                except Exception as e:\n                    self.log(\"Got an error recreating the test database: %s\" % e)\n                    sys.exit(2)\n        self._clone_db(source_database_name, target_database_name)\n\n    def _clone_db(self, source_database_name, target_database_name):\n        cmd_args, cmd_env = DatabaseClient.settings_to_cmd_args_env(\n            self.connection.settings_dict, []\n        )\n        dump_cmd = [\n            \"mysqldump\",\n            *cmd_args[1:-1],\n            \"--routines\",\n            \"--events\",\n            source_database_name,\n        ]\n        dump_env = load_env = {**os.environ, **cmd_env} if cmd_env else None\n        load_cmd = cmd_args\n        load_cmd[-1] = target_database_name\n\n        with subprocess.Popen(\n            dump_cmd, stdout=subprocess.PIPE, env=dump_env\n        ) as dump_proc:\n            with subprocess.Popen(\n                load_cmd,\n                stdin=dump_proc.stdout,\n                stdout=subprocess.DEVNULL,\n                env=load_env,\n            ):\n                # Allow dump_proc to receive a SIGPIPE if the load process exits.\n                dump_proc.stdout.close()"}],"ground_truth":null,"score":null},
{"question":"What are the main database backends supported by this repository?","answer":null,"relative_code_list":null,"ground_truth":"The repository supports several database backends including PostgreSQL, MySQL, SQLite3, Oracle, and a dummy backend for testing purposes.","score":null},
{"question":"How does the MySQL backend interact with other components in the repository?","answer":null,"relative_code_list":null,"ground_truth":"The MySQL backend depends on the 'backends' and 'oracle' components, indicating it may share some functionality or interfaces with these components.","score":null},
{"question":"What is the purpose of the 'Reference' class in the repository?","answer":null,"relative_code_list":null,"ground_truth":"The 'Reference' class is used to handle references to database objects like tables, columns, and indexes, and provides methods for renaming these references and string representation.","score":null},
{"question":"What functionality does the 'DatabaseFeatures' class provide?","answer":null,"relative_code_list":null,"ground_truth":"The 'DatabaseFeatures' class provides a comprehensive set of methods and attributes to check database-specific capabilities and features, such as transaction support, JSON field support, and various SQL functionalities.","score":null},
{"question":"How does the 'DatabaseOperations' class assist in database interactions?","answer":null,"relative_code_list":null,"ground_truth":"The 'DatabaseOperations' class provides methods for SQL generation and database operations, including date/time handling, type conversion, constraint management, and transaction control.","score":null},
{"question":"What is the role of the 'DatabaseSchemaEditor' class?","answer":null,"relative_code_list":null,"ground_truth":"The 'DatabaseSchemaEditor' class is responsible for modifying database schema, including creating/dropping tables and indexes, altering columns, and managing constraints through various SQL statements.","score":null},
{"question":"How does the repository handle database introspection?","answer":null,"relative_code_list":null,"ground_truth":"Database introspection is handled by the 'DatabaseIntrospection' class, which provides methods to examine database structure including table lists, field types, sequences, relations, and constraints.","score":null},
{"question":"What is the purpose of the 'CursorWrapper' class?","answer":null,"relative_code_list":null,"ground_truth":"The 'CursorWrapper' class wraps database cursors to provide additional functionality like error handling, iteration support, and execution of SQL statements with wrappers.","score":null},
{"question":"How are database connections managed in the repository?","answer":null,"relative_code_list":null,"ground_truth":"Database connections are managed by the 'DatabaseWrapper' class, which handles connection parameters, transaction management, cursor creation, and connection state initialization.","score":null},
{"question":"What testing-related functionality does the 'DatabaseCreation' class provide?","answer":null,"relative_code_list":null,"ground_truth":"The 'DatabaseCreation' class provides methods for test database management including creation, cloning, destruction, and serialization, along with handling test user and tablespace configurations.","score":null},
{"question":"How does the repository support different SQL dialects for various databases?","answer":null,"relative_code_list":null,"ground_truth":"The repository supports different SQL dialects through backend-specific implementations in classes like DatabaseOperations, DatabaseFeatures, and DatabaseSchemaEditor for each supported database system.","score":null},
{"question":"What is the purpose of the 'DummyDatabaseFeatures' class?","answer":null,"relative_code_list":null,"ground_truth":"The 'DummyDatabaseFeatures' class provides a minimal implementation of database features for testing purposes, supporting basic transaction and savepoint functionality.","score":null},
{"question":"How does the repository handle database-specific data types?","answer":null,"relative_code_list":null,"ground_truth":"Data type handling is managed through methods in DatabaseOperations and DatabaseWrapper classes, with type-specific conversion methods and data type reverse mapping in DatabaseIntrospection.","score":null},
{"question":"What transaction management capabilities are provided by the repository?","answer":null,"relative_code_list":null,"ground_truth":"Transaction management is handled by the DatabaseWrapper class, providing methods for savepoints, commit/rollback operations, and transaction state management with autocommit support.","score":null},
{"question":"How does the repository support database constraints?","answer":null,"relative_code_list":null,"ground_truth":"Constraint support is provided through DatabaseFeatures for capability checking, DatabaseSchemaEditor for constraint modification, and DatabaseWrapper for constraint checking and management.","score":null},
{"question":"What utility classes are provided for handling database intervals and timestamps?","answer":null,"relative_code_list":null,"ground_truth":"The repository provides 'IntervalToSeconds' and 'SecondsToInterval' classes for interval conversion, and 'Oracle_datetime' for Oracle-specific datetime handling.","score":null},
{"question":"How does the repository handle bulk insert operations?","answer":null,"relative_code_list":null,"ground_truth":"Bulk insert operations are supported through the 'bulk_insert_sql' method in DatabaseOperations and specialized classes like 'BulkInsertMapper' for Oracle-specific bulk operations.","score":null},
{"question":"What database validation capabilities are provided?","answer":null,"relative_code_list":null,"ground_truth":"Database validation is handled by the 'DatabaseValidation' class, which includes methods for checking field types and database-specific configurations like SQL mode in MySQL.","score":null},
{"question":"How are database-specific SQL functions implemented?","answer":null,"relative_code_list":null,"ground_truth":"Database-specific functions are implemented in backend-specific modules (like postgresql/functions.py) and through database operation methods that generate appropriate function calls.","score":null},
{"question":"What support does the repository provide for database schema comments?","answer":null,"relative_code_list":null,"ground_truth":"Schema comment support is provided through DatabaseFeatures checks and DatabaseSchemaEditor methods for altering table and column comments where supported by the database backend.","score":null}
]