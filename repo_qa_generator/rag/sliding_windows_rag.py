import sys
sys.path.append("/data3/pwh/codeqa")

import ast
import json
import os
import pickle
from typing import List, Dict, Optional, Tuple, Any
from pathlib import Path
import numpy as np
import faiss
from voyageai import Client
from repo_qa_generator.models.data_models import RepositoryStructure, QAPair, CodeNode
from repo_qa_generator.core.generator import BaseGenerator
from format.code_formatting import format_code_from_list
from dotenv import load_dotenv
import tiktoken
import concurrent.futures
import threading
from tqdm import tqdm

load_dotenv()

SYSTEM_PROMPT = "You are a professional code analysis assistant, you are good at explaining code and answering programming questions."

class SlidingWindowsRAG(BaseGenerator):
    """ä½¿ç”¨sliding windowsæ–¹æ³•å¯¹Pythonæ–‡ä»¶è¿›è¡Œchunkingï¼Œä½¿ç”¨Voyage APIè¿›è¡Œembeddingï¼Œå­˜å‚¨åˆ°FAISSä¸­çš„RAGç³»ç»Ÿ"""
    
    def __init__(self, repo_path: str, voyage_api_key: str = None, 
                 chunk_size: int = 1000, overlap: int = 100, 
                 embedding_model: str = "voyage-code-3", 
                 faiss_index_path: str = None):
        """
        åˆå§‹åŒ–SlidingWindowsRAG
        
        Args:
            repo_path: ä»“åº“è·¯å¾„
            voyage_api_key: Voyage APIå¯†é’¥ï¼Œå¦‚æœä¸ºNoneåˆ™ä»ç¯å¢ƒå˜é‡è·å–
            chunk_size: æ¯ä¸ªchunkçš„tokenæ•°é‡
            overlap: ç›¸é‚»chunkä¹‹é—´çš„é‡å tokenæ•°é‡
            embedding_model: Voyage embeddingæ¨¡å‹åç§°
            faiss_index_path: FAISSç´¢å¼•ä¿å­˜è·¯å¾„
        """
        super().__init__()
        
        self.repo_path = Path(repo_path)
        self.chunk_size = chunk_size
        self.overlap = overlap
        self.embedding_model = embedding_model
        
        load_dotenv()
        voyage_api_key = os.getenv("VOYAGE_API_KEY")
        if not voyage_api_key:
            raise ValueError("Voyage API key is required. Set VOYAGE_API_KEY environment variable or pass it as parameter.")
        
        self.voyage_client = Client(voyage_api_key)
        
        # åˆå§‹åŒ–tokenizer
        self.tokenizer = tiktoken.get_encoding("cl100k_base")
        
        # FAISSç´¢å¼•ç›¸å…³
        self.faiss_index_path = faiss_index_path or f"{self.repo_path.name}_faiss_index"
        self.metadata_path = f"{self.faiss_index_path}_metadata.pkl"
        
        # å­˜å‚¨chunkså’Œembeddings
        self.chunks = []
        self.chunk_metadata = []
        self.faiss_index = None
        self.dimension = None
        
        # å¦‚æœç´¢å¼•å·²å­˜åœ¨ï¼Œåˆ™åŠ è½½
        if self._index_exists():
            self._load_index()
        else:
            self._build_index()
    
    def _index_exists(self) -> bool:
        """æ£€æŸ¥FAISSç´¢å¼•æ˜¯å¦å­˜åœ¨"""
        return (os.path.exists(self.faiss_index_path) and 
                os.path.exists(self.metadata_path))
    
    def _load_index(self):
        """åŠ è½½å·²å­˜åœ¨çš„FAISSç´¢å¼•"""
        print(f"Loading existing FAISS index from {self.faiss_index_path}")
        
        # åŠ è½½FAISSç´¢å¼•
        self.faiss_index = faiss.read_index(self.faiss_index_path)
        self.dimension = self.faiss_index.d
        
        # åŠ è½½å…ƒæ•°æ®
        with open(self.metadata_path, 'rb') as f:
            data = pickle.load(f)
            self.chunks = data['chunks']
            self.chunk_metadata = data['metadata']
        
        print(f"Loaded {len(self.chunks)} chunks with dimension {self.dimension}")
    
    def _save_index(self):
        """ä¿å­˜FAISSç´¢å¼•å’Œå…ƒæ•°æ®"""
        print(f"Saving FAISS index to {self.faiss_index_path}")
        
        # ä¿å­˜FAISSç´¢å¼•
        faiss.write_index(self.faiss_index, self.faiss_index_path)
        
        # ä¿å­˜å…ƒæ•°æ®
        data = {
            'chunks': self.chunks,
            'metadata': self.chunk_metadata
        }
        with open(self.metadata_path, 'wb') as f:
            pickle.dump(data, f)
        
        print(f"Saved {len(self.chunks)} chunks")
    
    def _find_python_files(self) -> List[Path]:
        """æŸ¥æ‰¾ä»“åº“ä¸­æ‰€æœ‰çš„Pythonæ–‡ä»¶"""
        python_files = []
        for root, dirs, files in os.walk(self.repo_path):
            # è·³è¿‡ä¸€äº›å¸¸è§çš„éä»£ç ç›®å½•
            dirs[:] = [d for d in dirs if d not in {'.git', '__pycache__', '.pytest_cache', 'node_modules', 'venv', 'env', '.venv'}]
            
            for file in files:
                if file.endswith('.py'):
                    python_files.append(Path(root) / file)
        
        return python_files
    
    def _tokenize_text(self, text: str) -> List[str]:
        """å°†æ–‡æœ¬tokenize"""
        return self.tokenizer.encode(text)
    
    def _detokenize_text(self, tokens: List[int]) -> str:
        """å°†tokensè½¬æ¢å›æ–‡æœ¬"""
        return self.tokenizer.decode(tokens)
    
    def _create_sliding_windows(self, text: str, file_path: Path, show_progress: bool = False) -> List[Dict[str, Any]]:
        """ä½¿ç”¨sliding windowsæ–¹æ³•åˆ›å»ºchunks"""
        tokens = self._tokenize_text(text)
        chunks = []
        
        if len(tokens) <= self.chunk_size:
            # å¦‚æœæ–‡æœ¬é•¿åº¦å°äºchunk_sizeï¼Œç›´æ¥ä½œä¸ºä¸€ä¸ªchunk
            chunk_text = self._detokenize_text(tokens)
            chunks.append({
                'text': chunk_text,
                'start_token': 0,
                'end_token': len(tokens),
                'file_path': str(file_path),
                'start_line': 1,
                'end_line': len(text.split('\n'))
            })
        else:
            # ä½¿ç”¨sliding windowsåˆ›å»ºchunks
            start = 0
            chunk_count = 0
            
            # è®¡ç®—æ€»chunkæ•°é‡ç”¨äºè¿›åº¦æ¡
            total_chunks = (len(tokens) - self.chunk_size) // (self.chunk_size - self.overlap) + 1
            
            # åˆ›å»ºè¿›åº¦æ¡ï¼ˆå¦‚æœéœ€è¦ï¼‰
            if show_progress and total_chunks > 10:  # åªæœ‰chunkæ•°é‡è¾ƒå¤šæ—¶æ‰æ˜¾ç¤ºè¿›åº¦æ¡
                pbar = tqdm(total=total_chunks, desc=f"Creating chunks for {file_path.name}", leave=False)
            else:
                pbar = None
            
            while start < len(tokens):
                end = min(start + self.chunk_size, len(tokens))
                
                # æå–å½“å‰chunkçš„tokens
                chunk_tokens = tokens[start:end]
                chunk_text = self._detokenize_text(chunk_tokens)
                
                # è®¡ç®—å¯¹åº”çš„è¡Œå·èŒƒå›´ï¼ˆè¿‘ä¼¼ï¼‰
                lines = text.split('\n')
                start_line = 1
                end_line = len(lines)
                
                # å°è¯•æ›´ç²¾ç¡®åœ°è®¡ç®—è¡Œå·
                if start > 0:
                    # è®¡ç®—startä½ç½®å¯¹åº”çš„è¡Œå·
                    start_text = self._detokenize_text(tokens[:start])
                    start_line = len(start_text.split('\n'))
                
                if end < len(tokens):
                    # è®¡ç®—endä½ç½®å¯¹åº”çš„è¡Œå·
                    end_text = self._detokenize_text(tokens[:end])
                    end_line = len(end_text.split('\n'))
                
                chunks.append({
                    'text': chunk_text,
                    'start_token': start,
                    'end_token': end,
                    'file_path': str(file_path),
                    'start_line': start_line,
                    'end_line': end_line
                })
                
                chunk_count += 1
                if pbar:
                    pbar.update(1)
                
                # ç§»åŠ¨åˆ°ä¸‹ä¸€ä¸ªchunkï¼Œè€ƒè™‘é‡å 
                start = end - self.overlap
                if start >= len(tokens):
                    break
            
            if pbar:
                pbar.close()
        
        return chunks
    
    def _get_embeddings_batch(self, texts: List[str]) -> np.ndarray:
        """æ‰¹é‡è·å–embeddings"""
        try:
            embeddings = self.voyage_client.embed(texts, model=self.embedding_model)
            return np.array(embeddings.embeddings, dtype=np.float32)
        except Exception as e:
            print(f"Error getting embeddings: {e}")
            raise
    
    def _get_embeddings_parallel(self, texts: List[str], max_workers: int = 4, batch_size: int = 50) -> np.ndarray:
        """å¹¶è¡Œæ‰¹é‡è·å–embeddings"""
        if not texts:
            return np.array([])
        
        # åˆ›å»ºçº¿ç¨‹é”ï¼Œç¡®ä¿APIè°ƒç”¨çš„çº¿ç¨‹å®‰å…¨
        lock = threading.Lock()
        
        def process_batch(batch_texts):
            with lock:
                try:
                    embeddings = self.voyage_client.embed(batch_texts, model=self.embedding_model)
                    return np.array(embeddings.embeddings, dtype=np.float32)
                except Exception as e:
                    print(f"Error in batch processing: {e}")
                    # è¿”å›é›¶å‘é‡ä½œä¸ºfallback
                    return np.zeros((len(batch_texts), 1024), dtype=np.float32)
        
        # åˆ†æ‰¹å¤„ç†
        batches = [texts[i:i + batch_size] for i in range(0, len(texts), batch_size)]
        
        all_embeddings = []
        
        # ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œå¤„ç†
        with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
            # æäº¤æ‰€æœ‰æ‰¹æ¬¡çš„ä»»åŠ¡
            future_to_batch = {executor.submit(process_batch, batch): i for i, batch in enumerate(batches)}
            
            # ä½¿ç”¨tqdmæ˜¾ç¤ºè¿›åº¦
            with tqdm(total=len(batches), desc="Getting embeddings") as pbar:
                for future in concurrent.futures.as_completed(future_to_batch):
                    batch_idx = future_to_batch[future]
                    try:
                        batch_embeddings = future.result()
                        all_embeddings.append(batch_embeddings)
                    except Exception as e:
                        print(f"Batch {batch_idx} failed: {e}")
                        # æ·»åŠ é›¶å‘é‡ä½œä¸ºfallback
                        batch_size_actual = len(batches[batch_idx])
                        all_embeddings.append(np.zeros((batch_size_actual, 1024), dtype=np.float32))
                    finally:
                        pbar.update(1)
        
        if all_embeddings:
            return np.vstack(all_embeddings)
        else:
            return np.array([])
    
    def _build_index(self):
        """æ„å»ºFAISSç´¢å¼•"""
        print("Building FAISS index...")
        
        python_files = self._find_python_files()
        print(f"Found {len(python_files)} Python files")
        
        all_chunks = []
        all_metadata = []
        
        # æ·»åŠ æ–‡ä»¶å¤„ç†è¿›åº¦æ¡
        with tqdm(python_files, desc="Processing files", unit="file") as pbar:
            for file_path in pbar:
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        content = f.read()
                    
                    # åˆ›å»ºsliding windows chunks
                    # å¯¹äºå¤§å‹æ–‡ä»¶æ˜¾ç¤ºchunkåˆ›å»ºè¿›åº¦
                    show_chunk_progress = len(content) > 10000  # æ–‡ä»¶å¤§äº10KBæ—¶æ˜¾ç¤ºè¿›åº¦
                    chunks = self._create_sliding_windows(content, file_path, show_progress=show_chunk_progress)
                    all_chunks.extend(chunks)
                    
                    # æ·»åŠ å…ƒæ•°æ®
                    for chunk in chunks:
                        metadata = {
                            'file_path': chunk['file_path'],
                            'start_line': chunk['start_line'],
                            'end_line': chunk['end_line'],
                            'start_token': chunk['start_token'],
                            'end_token': chunk['end_token']
                        }
                        all_metadata.append(metadata)
                    
                    # æ›´æ–°è¿›åº¦æ¡æè¿°
                    pbar.set_postfix({
                        'chunks': len(all_chunks),
                        'current_file': file_path.name
                    })
                    
                except Exception as e:
                    print(f"Error processing {file_path}: {e}")
                    continue
        
        if not all_chunks:
            print("No chunks created!")
            return
        
        # è·å–embeddings - ä½¿ç”¨å¹¶è¡Œå¤„ç†
        print("Getting embeddings with parallel processing...")
        texts = [chunk['text'] for chunk in all_chunks]
        
        # ä½¿ç”¨å¹¶è¡Œå¤„ç†è·å–embeddings
        embeddings = self._get_embeddings_parallel(texts, max_workers=4, batch_size=50)
        
        # åˆ›å»ºFAISSç´¢å¼•
        print("Creating FAISS index...")
        self.dimension = embeddings.shape[1]
        self.faiss_index = faiss.IndexFlatIP(self.dimension)  # ä½¿ç”¨å†…ç§¯ç›¸ä¼¼åº¦
        self.faiss_index.add(embeddings)
        
        # ä¿å­˜chunkså’Œå…ƒæ•°æ®
        self.chunks = all_chunks
        self.chunk_metadata = all_metadata
        
        # ä¿å­˜ç´¢å¼•
        self._save_index()
        
        # æ˜¾ç¤ºæœ€ç»ˆç»Ÿè®¡ä¿¡æ¯
        print(f"\nâœ… Index building completed!")
        print(f"ğŸ“Š Statistics:")
        print(f"   - Total files processed: {len(python_files)}")
        print(f"   - Total chunks created: {len(all_chunks)}")
        print(f"   - Embedding dimension: {self.dimension}")
        print(f"   - Average chunks per file: {len(all_chunks) / len(python_files):.1f}")
        print(f"   - Index saved to: {self.faiss_index_path}")
    
    def search(self, query: str, top_k: int = 5) -> List[Dict[str, Any]]:
        """
        æœç´¢æœ€ç›¸å…³çš„ä»£ç chunks
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            top_k: è¿”å›çš„æœ€ç›¸å…³chunksæ•°é‡
            
        Returns:
            List of dictionaries containing chunk information and similarity scores
        """
        # è·å–æŸ¥è¯¢çš„embedding
        query_embedding = self._get_embeddings_batch([query])
        
        # æœç´¢æœ€ç›¸ä¼¼çš„chunks
        similarities, indices = self.faiss_index.search(query_embedding, top_k)
        
        results = []
        for i, (similarity, idx) in enumerate(zip(similarities[0], indices[0])):
            if idx < len(self.chunks):
                chunk = self.chunks[idx]
                metadata = self.chunk_metadata[idx]
                
                result = {
                    'text': chunk['text'],
                    'similarity': float(similarity),
                    'file_path': metadata['file_path'],
                    'start_line': metadata['start_line'],
                    'end_line': metadata['end_line'],
                    'start_token': metadata['start_token'],
                    'end_token': metadata['end_token']
                }
                results.append(result)
        
        return results
    
    def search_batch(self, queries: List[str], top_k: int = 5, max_workers: int = 4) -> List[List[Dict[str, Any]]]:
        """
        æ‰¹é‡æœç´¢å¤šä¸ªæŸ¥è¯¢ï¼Œä½¿ç”¨å¹¶è¡Œå¤„ç†
        
        Args:
            queries: æŸ¥è¯¢æ–‡æœ¬åˆ—è¡¨
            top_k: æ¯ä¸ªæŸ¥è¯¢è¿”å›çš„æœ€ç›¸å…³chunksæ•°é‡
            max_workers: å¹¶è¡Œå·¥ä½œçº¿ç¨‹æ•°
            
        Returns:
            List of search results for each query
        """
        def search_single(query):
            return self.search(query, top_k)
        
        results = []
        with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
            future_to_query = {executor.submit(search_single, query): i for i, query in enumerate(queries)}
            
            # åˆå§‹åŒ–ç»“æœåˆ—è¡¨
            results = [None] * len(queries)
            
            for future in concurrent.futures.as_completed(future_to_query):
                query_idx = future_to_query[future]
                try:
                    result = future.result()
                    results[query_idx] = result
                except Exception as e:
                    print(f"Search for query {query_idx} failed: {e}")
                    results[query_idx] = []
        
        return results
    
    def find_relevant_code(self, query: str, top_k: int = 5) -> List[CodeNode]:
        """
        æŸ¥æ‰¾ä¸æŸ¥è¯¢æœ€ç›¸å…³çš„ä»£ç å…ƒç´ ï¼Œè¿”å›CodeNodeæ ¼å¼
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            top_k: è¿”å›çš„æœ€ç›¸å…³å…ƒç´ æ•°é‡
            
        Returns:
            List of CodeNode
        """
        search_results = self.search(query, top_k)
        
        results = []
        for result in search_results:
            try:
                # è¯»å–æ–‡ä»¶å†…å®¹
                file_path = result['file_path']
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                
                # æå–ç›¸å…³ä»£ç æ®µ
                lines = content.split('\n')
                start_line = result['start_line']
                end_line = result['end_line']
                
                # ç¡®ä¿è¡Œå·åœ¨æœ‰æ•ˆèŒƒå›´å†…
                start_line = max(1, min(start_line, len(lines)))
                end_line = max(start_line, min(end_line, len(lines)))
                
                code_content = '\n'.join(lines[start_line-1:end_line])
                
                # åˆ›å»ºæ–‡ä»¶èŠ‚ç‚¹
                file_node = {
                    "file_name": os.path.basename(file_path),
                    "upper_path": os.path.dirname(file_path),
                    "module": "code_chunk",
                    "define_class": [],
                    "imports": []
                }
                
                # åˆ›å»ºä»£ç èŠ‚ç‚¹
                code_node = CodeNode(
                    start_line=start_line,
                    end_line=end_line,
                    belongs_to=file_node,
                    relative_function=[],
                    code=code_content
                )
                
                results.append(code_node)
                
            except Exception as e:
                print(f"Warning: Error reading file {file_path}: {str(e)}")
                continue
        
        return results
    
    def process_qa_pair(self, qa_pair: QAPair) -> QAPair:
        """
        å¤„ç†QA Pairï¼Œæ‰¾åˆ°ç›¸å…³ä»£ç å¹¶ä½¿ç”¨LLMå›ç­”é—®é¢˜
        
        Args:
            qa_pair: åŒ…å«é—®é¢˜å’Œå›ç­”çš„QA Pairå¯¹è±¡
        """
        if not qa_pair.relative_code_list:
            relevant_code_list = self.find_relevant_code(qa_pair.question)
        else:
            relevant_code_list = qa_pair.relative_code_list
        
        answer = self.process_answer(qa_pair.question, relevant_code_list)
        qa_pair.answer = answer
        return qa_pair
    
    def process_answer(self, question: str, relevant_code_list: List[CodeNode]) -> str:
        """
        å¤„ç†é—®é¢˜ï¼Œä½¿ç”¨ç›¸å…³ä»£ç ç”Ÿæˆå›ç­”
        
        Args:
            question: ç”¨æˆ·çš„é—®é¢˜
            relevant_code_list: ç›¸å…³ä»£ç åˆ—è¡¨
            
        Returns:
            ç”Ÿæˆçš„å›ç­”
        """
        if not relevant_code_list:
            return "No relevant code found. No sufficient information to answer the question."
        
        # æ„å»ºæäº¤ç»™LLMçš„æç¤º
        prompt = self._build_llm_prompt(question, relevant_code_list)
        
        # è°ƒç”¨LLMè·å–å›ç­”
        answer = self._call_llm(system_prompt=SYSTEM_PROMPT, user_prompt=prompt)
        print(f"LLM response answer: {answer}")
        return answer
    
    def _build_llm_prompt(self, question: str, relevant_code_list: List[CodeNode]) -> str:
        """
        æ„å»ºæäº¤ç»™LLMçš„æç¤º
        
        Args:
            question: ç”¨æˆ·çš„é—®é¢˜
            relevant_code_list: ç›¸å…³ä»£ç åˆ—è¡¨
            
        Returns:
            å®Œæ•´çš„æç¤ºæ–‡æœ¬
        """
        prompt = "You are a professional code analysis assistant. Please answer the question based on the following code snippets.\n\n"
        prompt += f"Question: {question}\n\n"
        prompt += format_code_from_list(relevant_code_list)
        
        prompt += "Please answer the question based on the above code snippets. Explain key concepts and code logic, ensuring the answer is accurate, comprehensive, and easy to understand."
        return prompt
    
    def process_qa_pairs(self, qa_pairs: List[QAPair]) -> List[QAPair]:
        """
        æ‰¹é‡å¤„ç†å¤šä¸ªQA Pair
        
        Args:
            qa_pairs: QA Pairåˆ—è¡¨
            
        Returns:
            æ›´æ–°åçš„QA Pairåˆ—è¡¨
        """
        updated_pairs = []
        for qa_pair in qa_pairs:
            updated_pair = self.process_qa_pair(qa_pair)
            updated_pairs.append(updated_pair)
        return updated_pairs
    
    def get_index_stats(self) -> Dict[str, Any]:
        """è·å–ç´¢å¼•ç»Ÿè®¡ä¿¡æ¯"""
        return {
            'total_chunks': len(self.chunks),
            'dimension': self.dimension,
            'index_path': self.faiss_index_path,
            'files_processed': len(set(chunk['file_path'] for chunk in self.chunks))
        } 

    